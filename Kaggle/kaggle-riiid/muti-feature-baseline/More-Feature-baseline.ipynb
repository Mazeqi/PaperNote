{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049311,
     "end_time": "2020-10-30T18:22:55.213053",
     "exception": false,
     "start_time": "2020-10-30T18:22:55.163742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reading Data and Importing Libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.953599,
     "end_time": "2020-10-30T18:22:56.217436",
     "exception": false,
     "start_time": "2020-10-30T18:22:55.263837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import itertools    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from sklearn.metrics import *\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import PackedSequence\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "try:\n",
    "    from tensorflow.python.keras.callbacks import CallbackList\n",
    "except ImportError:\n",
    "    from tensorflow.python.keras._impl.keras.callbacks import CallbackList\n",
    "    \n",
    "from collections import OrderedDict, namedtuple, defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# activation\n",
    "class Dice(nn.Module):\n",
    "    \"\"\"The Data Adaptive Activation Function in DIN,which can be viewed as a generalization of PReLu and can adaptively adjust the rectified point according to distribution of input data.\n",
    "\n",
    "    Input shape:\n",
    "        - 2 dims: [batch_size, embedding_size(features)]\n",
    "        - 3 dims: [batch_size, num_features, embedding_size(features)]\n",
    "\n",
    "    Output shape:\n",
    "        - Same shape as input.\n",
    "    \n",
    "    References\n",
    "        - [Zhou G, Zhu X, Song C, et al. Deep interest network for click-through rate prediction[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018: 1059-1068.](https://arxiv.org/pdf/1706.06978.pdf)\n",
    "        - https://github.com/zhougr1993/DeepInterestNetwork, https://github.com/fanoping/DIN-pytorch\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_size, dim=2, epsilon=1e-8, device='cpu'):\n",
    "        super(Dice, self).__init__()\n",
    "        assert dim == 2 or dim == 3\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(emb_size, eps=epsilon)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dim = dim\n",
    "\n",
    "        if self.dim == 2:\n",
    "            self.alpha = torch.zeros((emb_size,)).to(device)\n",
    "        else:\n",
    "            self.alpha = torch.zeros((emb_size, 1)).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.dim() == self.dim\n",
    "        if self.dim == 2:\n",
    "            x_p = self.sigmoid(self.bn(x))\n",
    "            out = self.alpha * (1 - x_p) * x + x_p * x\n",
    "        else:\n",
    "            x = torch.transpose(x, 1, 2)\n",
    "            x_p = self.sigmoid(self.bn(x))\n",
    "            out = self.alpha * (1 - x_p) * x + x_p * x\n",
    "            out = torch.transpose(out, 1, 2)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X\n",
    "\n",
    "\n",
    "def activation_layer(act_name, hidden_size=None, dice_dim=2):\n",
    "    \"\"\"Construct activation layers\n",
    "\n",
    "    Args:\n",
    "        act_name: str or nn.Module, name of activation function\n",
    "        hidden_size: int, used for Dice activation\n",
    "        dice_dim: int, used for Dice activation\n",
    "    Return:\n",
    "        act_layer: activation layer\n",
    "    \"\"\"\n",
    "    if isinstance(act_name, str):\n",
    "        if act_name.lower() == 'sigmoid':\n",
    "            act_layer = nn.Sigmoid()\n",
    "        elif act_name.lower() == 'linear':\n",
    "            act_layer = Identity()\n",
    "        elif act_name.lower() == 'relu':\n",
    "            act_layer = nn.ReLU(inplace=True)\n",
    "        elif act_name.lower() == 'dice':\n",
    "            assert dice_dim\n",
    "            act_layer = Dice(hidden_size, dice_dim)\n",
    "        elif act_name.lower() == 'prelu':\n",
    "            act_layer = nn.PReLU()\n",
    "    elif issubclass(act_name, nn.Module):\n",
    "        act_layer = act_name()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return act_layer\n",
    "\n",
    "\n",
    "class LocalActivationUnit(nn.Module):\n",
    "    \"\"\"The LocalActivationUnit used in DIN with which the representation of\n",
    "        user interests varies adaptively given different candidate items.\n",
    "\n",
    "    Input shape\n",
    "        - A list of two 3D tensor with shape:  ``(batch_size, 1, embedding_size)`` and ``(batch_size, T, embedding_size)``\n",
    "\n",
    "    Output shape\n",
    "        - 3D tensor with shape: ``(batch_size, T, 1)``.\n",
    "\n",
    "    Arguments\n",
    "        - **hidden_units**:list of positive integer, the attention net layer number and units in each layer.\n",
    "\n",
    "        - **activation**: Activation function to use in attention net.\n",
    "\n",
    "        - **l2_reg**: float between 0 and 1. L2 regularizer strength applied to the kernel weights matrix of attention net.\n",
    "\n",
    "        - **dropout_rate**: float in [0,1). Fraction of the units to dropout in attention net.\n",
    "\n",
    "        - **use_bn**: bool. Whether use BatchNormalization before activation or not in attention net.\n",
    "\n",
    "        - **seed**: A Python integer to use as random seed.\n",
    "\n",
    "    References\n",
    "        - [Zhou G, Zhu X, Song C, et al. Deep interest network for click-through rate prediction[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018: 1059-1068.](https://arxiv.org/pdf/1706.06978.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_units=(64, 32), embedding_dim=4, activation='sigmoid', dropout_rate=0, dice_dim=3,\n",
    "                 l2_reg=0, use_bn=False):\n",
    "        super(LocalActivationUnit, self).__init__()\n",
    "\n",
    "        self.dnn = DNN(inputs_dim=4 * embedding_dim,\n",
    "                       hidden_units=hidden_units,\n",
    "                       activation=activation,\n",
    "                       l2_reg=l2_reg,\n",
    "                       dropout_rate=dropout_rate,\n",
    "                       dice_dim=dice_dim,\n",
    "                       use_bn=use_bn)\n",
    "\n",
    "        self.dense = nn.Linear(hidden_units[-1], 1)\n",
    "\n",
    "    def forward(self, query, user_behavior):\n",
    "        # query ad            : size -> batch_size * 1 * embedding_size\n",
    "        # user behavior       : size -> batch_size * time_seq_len * embedding_size\n",
    "        user_behavior_len = user_behavior.size(1)\n",
    "\n",
    "        queries = query.expand(-1, user_behavior_len, -1)\n",
    "\n",
    "        attention_input = torch.cat([queries, user_behavior, queries - user_behavior, queries * user_behavior],\n",
    "                                    dim=-1)  # as the source code, subtraction simulates verctors' difference\n",
    "        attention_output = self.dnn(attention_input)\n",
    "\n",
    "        attention_score = self.dense(attention_output)  # [B, T, 1]\n",
    "\n",
    "        return attention_score\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "# core\n",
    "class DNN(nn.Module):\n",
    "    \"\"\"The Multi Layer Percetron\n",
    "\n",
    "      Input shape\n",
    "        - nD tensor with shape: ``(batch_size, ..., input_dim)``. The most common situation would be a 2D input with shape ``(batch_size, input_dim)``.\n",
    "\n",
    "      Output shape\n",
    "        - nD tensor with shape: ``(batch_size, ..., hidden_size[-1])``. For instance, for a 2D input with shape ``(batch_size, input_dim)``, the output would have shape ``(batch_size, hidden_size[-1])``.\n",
    "\n",
    "      Arguments\n",
    "        - **inputs_dim**: input feature dimension.\n",
    "\n",
    "        - **hidden_units**:list of positive integer, the layer number and units in each layer.\n",
    "\n",
    "        - **activation**: Activation function to use.\n",
    "\n",
    "        - **l2_reg**: float between 0 and 1. L2 regularizer strength applied to the kernel weights matrix.\n",
    "\n",
    "        - **dropout_rate**: float in [0,1). Fraction of the units to dropout.\n",
    "\n",
    "        - **use_bn**: bool. Whether use BatchNormalization before activation or not.\n",
    "\n",
    "        - **seed**: A Python integer to use as random seed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputs_dim, hidden_units, activation='relu', l2_reg=0, dropout_rate=0, use_bn=False,\n",
    "                 init_std=0.0001, dice_dim=3, seed=1024, device='cpu'):\n",
    "        super(DNN, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.seed = seed\n",
    "        self.l2_reg = l2_reg\n",
    "        self.use_bn = use_bn\n",
    "        if len(hidden_units) == 0:\n",
    "            raise ValueError(\"hidden_units is empty!!\")\n",
    "        hidden_units = [inputs_dim] + list(hidden_units)\n",
    "\n",
    "        self.linears = nn.ModuleList(\n",
    "            [nn.Linear(hidden_units[i], hidden_units[i + 1]) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        if self.use_bn:\n",
    "            self.bn = nn.ModuleList(\n",
    "                [nn.BatchNorm1d(hidden_units[i + 1]) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        self.activation_layers = nn.ModuleList(\n",
    "            [activation_layer(activation, hidden_units[i + 1], dice_dim) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        for name, tensor in self.linears.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(tensor, mean=0, std=init_std)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        deep_input = inputs\n",
    "\n",
    "        for i in range(len(self.linears)):\n",
    "\n",
    "            fc = self.linears[i](deep_input)\n",
    "\n",
    "            if self.use_bn:\n",
    "                fc = self.bn[i](fc)\n",
    "\n",
    "            fc = self.activation_layers[i](fc)\n",
    "\n",
    "            fc = self.dropout(fc)\n",
    "            deep_input = fc\n",
    "        return deep_input\n",
    "\n",
    "\n",
    "class PredictionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "      Arguments\n",
    "         - **task**: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n",
    "         - **use_bias**: bool.Whether add bias term or not.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, task='binary', use_bias=True, **kwargs):\n",
    "        if task not in [\"binary\", \"multiclass\", \"regression\"]:\n",
    "            raise ValueError(\"task must be binary,multiclass or regression\")\n",
    "\n",
    "        super(PredictionLayer, self).__init__()\n",
    "        self.use_bias = use_bias\n",
    "        self.task = task\n",
    "        if self.use_bias:\n",
    "            self.bias = nn.Parameter(torch.zeros((1,)))\n",
    "\n",
    "    def forward(self, X):\n",
    "        output = X\n",
    "        if self.use_bias:\n",
    "            output += self.bias\n",
    "        if self.task == \"binary\":\n",
    "            output = torch.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Conv2dSame(nn.Conv2d):\n",
    "    \"\"\" Tensorflow like 'SAME' convolution wrapper for 2D convolutions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(Conv2dSame, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, stride, 0, dilation,\n",
    "            groups, bias)\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ih, iw = x.size()[-2:]\n",
    "        kh, kw = self.weight.size()[-2:]\n",
    "        oh = math.ceil(ih / self.stride[0])\n",
    "        ow = math.ceil(iw / self.stride[1])\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2])\n",
    "        out = F.conv2d(x, self.weight, self.bias, self.stride,\n",
    "                       self.padding, self.dilation, self.groups)\n",
    "        return out\n",
    "\n",
    "    \n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "# sequence\n",
    "class SequencePoolingLayer(nn.Module):\n",
    "    \"\"\"The SequencePoolingLayer is used to apply pooling operation(sum,mean,max) on variable-length sequence feature/multi-value feature.\n",
    "\n",
    "      Input shape\n",
    "        - A list of two  tensor [seq_value,seq_len]\n",
    "\n",
    "        - seq_value is a 3D tensor with shape: ``(batch_size, T, embedding_size)``\n",
    "\n",
    "        - seq_len is a 2D tensor with shape : ``(batch_size, 1)``,indicate valid length of each sequence.\n",
    "\n",
    "      Output shape\n",
    "        - 3D tensor with shape: ``(batch_size, 1, embedding_size)``.\n",
    "\n",
    "      Arguments\n",
    "        - **mode**:str.Pooling operation to be used,can be sum,mean or max.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode='mean', supports_masking=False, device='cpu'):\n",
    "\n",
    "        super(SequencePoolingLayer, self).__init__()\n",
    "        if mode not in ['sum', 'mean', 'max']:\n",
    "            raise ValueError('parameter mode should in [sum, mean, max]')\n",
    "        self.supports_masking = supports_masking\n",
    "        self.mode = mode\n",
    "        self.device = device\n",
    "        self.eps = torch.FloatTensor([1e-8]).to(device)\n",
    "        self.to(device)\n",
    "\n",
    "    def _sequence_mask(self, lengths, maxlen=None, dtype=torch.bool):\n",
    "        # Returns a mask tensor representing the first N positions of each cell.\n",
    "        if maxlen is None:\n",
    "            maxlen = lengths.max()\n",
    "        row_vector = torch.arange(0, maxlen, 1).to(self.device)\n",
    "        matrix = torch.unsqueeze(lengths, dim=-1)\n",
    "        mask = row_vector < matrix\n",
    "\n",
    "        mask.type(dtype)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, seq_value_len_list):\n",
    "        if self.supports_masking:\n",
    "            uiseq_embed_list, mask = seq_value_len_list  # [B, T, E], [B, 1]\n",
    "            mask = mask.float()\n",
    "            user_behavior_length = torch.sum(mask, dim=-1, keepdim=True)\n",
    "            mask = mask.unsqueeze(2)\n",
    "        else:\n",
    "            uiseq_embed_list, user_behavior_length = seq_value_len_list  # [B, T, E], [B, 1]\n",
    "            mask = self._sequence_mask(user_behavior_length, maxlen=uiseq_embed_list.shape[1],\n",
    "                                       dtype=torch.float32)  # [B, 1, maxlen]\n",
    "            mask = torch.transpose(mask, 1, 2)  # [B, maxlen, 1]\n",
    "\n",
    "        embedding_size = uiseq_embed_list.shape[-1]\n",
    "\n",
    "        mask = torch.repeat_interleave(mask, embedding_size, dim=2)  # [B, maxlen, E]\n",
    "\n",
    "        if self.mode == 'max':\n",
    "            hist = uiseq_embed_list - (1 - mask) * 1e9\n",
    "            hist = torch.max(hist, dim=1, keepdim=True)[0]\n",
    "            return hist\n",
    "        hist = uiseq_embed_list * mask.float()\n",
    "        hist = torch.sum(hist, dim=1, keepdim=False)\n",
    "\n",
    "        if self.mode == 'mean':\n",
    "            hist = torch.div(hist, user_behavior_length.type(torch.float32) + self.eps)\n",
    "\n",
    "        hist = torch.unsqueeze(hist, dim=1)\n",
    "        return hist\n",
    "\n",
    "\n",
    "class AttentionSequencePoolingLayer(nn.Module):\n",
    "    \"\"\"The Attentional sequence pooling operation used in DIN & DIEN.\n",
    "\n",
    "        Arguments\n",
    "          - **att_hidden_units**:list of positive integer, the attention net layer number and units in each layer.\n",
    "\n",
    "          - **att_activation**: Activation function to use in attention net.\n",
    "\n",
    "          - **weight_normalization**: bool.Whether normalize the attention score of local activation unit.\n",
    "\n",
    "          - **supports_masking**:If True,the input need to support masking.\n",
    "\n",
    "        References\n",
    "          - [Zhou G, Zhu X, Song C, et al. Deep interest network for click-through rate prediction[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018: 1059-1068.](https://arxiv.org/pdf/1706.06978.pdf)\n",
    "      \"\"\"\n",
    "\n",
    "    def __init__(self, att_hidden_units=(80, 40), att_activation='sigmoid', weight_normalization=False,\n",
    "                 return_score=False, supports_masking=False, embedding_dim=4, **kwargs):\n",
    "        super(AttentionSequencePoolingLayer, self).__init__()\n",
    "        self.return_score = return_score\n",
    "        self.weight_normalization = weight_normalization\n",
    "        self.supports_masking = supports_masking\n",
    "        self.local_att = LocalActivationUnit(hidden_units=att_hidden_units, embedding_dim=embedding_dim,\n",
    "                                             activation=att_activation,\n",
    "                                             dropout_rate=0, use_bn=False)\n",
    "\n",
    "    def forward(self, query, keys, keys_length, mask=None):\n",
    "        \"\"\"\n",
    "        Input shape\n",
    "          - A list of three tensor: [query,keys,keys_length]\n",
    "\n",
    "          - query is a 3D tensor with shape:  ``(batch_size, 1, embedding_size)``\n",
    "\n",
    "          - keys is a 3D tensor with shape:   ``(batch_size, T, embedding_size)``\n",
    "\n",
    "          - keys_length is a 2D tensor with shape: ``(batch_size, 1)``\n",
    "\n",
    "        Output shape\n",
    "          - 3D tensor with shape: ``(batch_size, 1, embedding_size)``.\n",
    "        \"\"\"\n",
    "        batch_size, max_length, dim = keys.size()\n",
    "\n",
    "        # Mask\n",
    "        if self.supports_masking:\n",
    "            if mask is None:\n",
    "                raise ValueError(\"When supports_masking=True,input must support masking\")\n",
    "            keys_masks = mask.unsqueeze(1)\n",
    "        else:\n",
    "            keys_masks = torch.arange(max_length, device=keys_length.device, dtype=keys_length.dtype).repeat(batch_size,\n",
    "                                                                                                             1)  # [B, T]\n",
    "            keys_masks = keys_masks < keys_length.view(-1, 1)  # 0, 1 mask\n",
    "            keys_masks = keys_masks.unsqueeze(1)  # [B, 1, T]\n",
    "\n",
    "        attention_score = self.local_att(query, keys)  # [B, T, 1]\n",
    "\n",
    "        outputs = torch.transpose(attention_score, 1, 2)  # [B, 1, T]\n",
    "\n",
    "        if self.weight_normalization:\n",
    "            paddings = torch.ones_like(outputs) * (-2 ** 32 + 1)\n",
    "        else:\n",
    "            paddings = torch.zeros_like(outputs)\n",
    "\n",
    "        outputs = torch.where(keys_masks, outputs, paddings)  # [B, 1, T]\n",
    "\n",
    "        # Scale\n",
    "        # outputs = outputs / (keys.shape[-1] ** 0.05)\n",
    "\n",
    "        if self.weight_normalization:\n",
    "            outputs = F.softmax(outputs, dim=-1)  # [B, 1, T]\n",
    "\n",
    "        if not self.return_score:\n",
    "            # Weighted sum\n",
    "            outputs = torch.matmul(outputs, keys)  # [B, 1, E]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class KMaxPooling(nn.Module):\n",
    "    \"\"\"K Max pooling that selects the k biggest value along the specific axis.\n",
    "\n",
    "      Input shape\n",
    "        -  nD tensor with shape: ``(batch_size, ..., input_dim)``.\n",
    "\n",
    "      Output shape\n",
    "        - nD tensor with shape: ``(batch_size, ..., output_dim)``.\n",
    "\n",
    "      Arguments\n",
    "        - **k**: positive integer, number of top elements to look for along the ``axis`` dimension.\n",
    "\n",
    "        - **axis**: positive integer, the dimension to look for elements.\n",
    "\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, k, axis, device='cpu'):\n",
    "        super(KMaxPooling, self).__init__()\n",
    "        self.k = k\n",
    "        self.axis = axis\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.axis < 0 or self.axis >= len(input.shape):\n",
    "            raise ValueError(\"axis must be 0~%d,now is %d\" %\n",
    "                             (len(input.shape) - 1, self.axis))\n",
    "\n",
    "        if self.k < 1 or self.k > input.shape[self.axis]:\n",
    "            raise ValueError(\"k must be in 1 ~ %d,now k is %d\" %\n",
    "                             (input.shape[self.axis], self.k))\n",
    "\n",
    "        out = torch.topk(input, k=self.k, dim=self.axis, sorted=True)[0]\n",
    "        return out\n",
    "\n",
    "\n",
    "class AGRUCell(nn.Module):\n",
    "    \"\"\" Attention based GRU (AGRU)\n",
    "\n",
    "        Reference:\n",
    "        -  Deep Interest Evolution Network for Click-Through Rate Prediction[J]. arXiv preprint arXiv:1809.03672, 2018.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(AGRUCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        # (W_ir|W_iz|W_ih)\n",
    "        self.weight_ih = nn.Parameter(torch.Tensor(3 * hidden_size, input_size))\n",
    "        self.register_parameter('weight_ih', self.weight_ih)\n",
    "        # (W_hr|W_hz|W_hh)\n",
    "        self.weight_hh = nn.Parameter(torch.Tensor(3 * hidden_size, hidden_size))\n",
    "        self.register_parameter('weight_hh', self.weight_hh)\n",
    "        if bias:\n",
    "            # (b_ir|b_iz|b_ih)\n",
    "            self.bias_ih = nn.Parameter(torch.Tensor(3 * hidden_size))\n",
    "            self.register_parameter('bias_ih', self.bias_ih)\n",
    "            # (b_hr|b_hz|b_hh)\n",
    "            self.bias_hh = nn.Parameter(torch.Tensor(3 * hidden_size))\n",
    "            self.register_parameter('bias_hh', self.bias_hh)\n",
    "            for tensor in [self.bias_ih, self.bias_hh]:\n",
    "                nn.init.zeros_(tensor, )\n",
    "        else:\n",
    "            self.register_parameter('bias_ih', None)\n",
    "            self.register_parameter('bias_hh', None)\n",
    "\n",
    "    def forward(self, input, hx, att_score):\n",
    "        gi = F.linear(input, self.weight_ih, self.bias_ih)\n",
    "        gh = F.linear(hx, self.weight_hh, self.bias_hh)\n",
    "        i_r, i_z, i_n = gi.chunk(3, 1)\n",
    "        h_r, h_z, h_n = gh.chunk(3, 1)\n",
    "\n",
    "        reset_gate = torch.sigmoid(i_r + h_r)\n",
    "        # update_gate = torch.sigmoid(i_z + h_z)\n",
    "        new_state = torch.tanh(i_n + reset_gate * h_n)\n",
    "\n",
    "        att_score = att_score.view(-1, 1)\n",
    "        hy = (1. - att_score) * hx + att_score * new_state\n",
    "        return hy\n",
    "\n",
    "\n",
    "class AUGRUCell(nn.Module):\n",
    "    \"\"\" Effect of GRU with attentional update gate (AUGRU)\n",
    "\n",
    "        Reference:\n",
    "        -  Deep Interest Evolution Network for Click-Through Rate Prediction[J]. arXiv preprint arXiv:1809.03672, 2018.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(AUGRUCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        # (W_ir|W_iz|W_ih)\n",
    "        self.weight_ih = nn.Parameter(torch.Tensor(3 * hidden_size, input_size))\n",
    "        self.register_parameter('weight_ih', self.weight_ih)\n",
    "        # (W_hr|W_hz|W_hh)\n",
    "        self.weight_hh = nn.Parameter(torch.Tensor(3 * hidden_size, hidden_size))\n",
    "        self.register_parameter('weight_hh', self.weight_hh)\n",
    "        if bias:\n",
    "            # (b_ir|b_iz|b_ih)\n",
    "            self.bias_ih = nn.Parameter(torch.Tensor(3 * hidden_size))\n",
    "            self.register_parameter('bias_ih', self.bias_ih)\n",
    "            # (b_hr|b_hz|b_hh)\n",
    "            self.bias_hh = nn.Parameter(torch.Tensor(3 * hidden_size))\n",
    "            self.register_parameter('bias_ih', self.bias_hh)\n",
    "            for tensor in [self.bias_ih, self.bias_hh]:\n",
    "                nn.init.zeros_(tensor, )\n",
    "        else:\n",
    "            self.register_parameter('bias_ih', None)\n",
    "            self.register_parameter('bias_hh', None)\n",
    "\n",
    "    def forward(self, input, hx, att_score):\n",
    "        gi = F.linear(input, self.weight_ih, self.bias_ih)\n",
    "        gh = F.linear(hx, self.weight_hh, self.bias_hh)\n",
    "        i_r, i_z, i_n = gi.chunk(3, 1)\n",
    "        h_r, h_z, h_n = gh.chunk(3, 1)\n",
    "\n",
    "        reset_gate = torch.sigmoid(i_r + h_r)\n",
    "        update_gate = torch.sigmoid(i_z + h_z)\n",
    "        new_state = torch.tanh(i_n + reset_gate * h_n)\n",
    "\n",
    "        att_score = att_score.view(-1, 1)\n",
    "        update_gate = att_score * update_gate\n",
    "        hy = (1. - update_gate) * hx + update_gate * new_state\n",
    "        return hy\n",
    "\n",
    "\n",
    "class DynamicGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True, gru_type='AGRU'):\n",
    "        super(DynamicGRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if gru_type == 'AGRU':\n",
    "            self.rnn = AGRUCell(input_size, hidden_size, bias)\n",
    "        elif gru_type == 'AUGRU':\n",
    "            self.rnn = AUGRUCell(input_size, hidden_size, bias)\n",
    "\n",
    "    def forward(self, input, att_scores=None, hx=None):\n",
    "        if not isinstance(input, PackedSequence) or not isinstance(att_scores, PackedSequence):\n",
    "            raise NotImplementedError(\"DynamicGRU only supports packed input and att_scores\")\n",
    "\n",
    "        input, batch_sizes, sorted_indices, unsorted_indices = input\n",
    "        att_scores, _, _, _ = att_scores\n",
    "\n",
    "        max_batch_size = int(batch_sizes[0])\n",
    "        if hx is None:\n",
    "            hx = torch.zeros(max_batch_size, self.hidden_size,\n",
    "                             dtype=input.dtype, device=input.device)\n",
    "\n",
    "        outputs = torch.zeros(input.size(0), self.hidden_size,\n",
    "                              dtype=input.dtype, device=input.device)\n",
    "\n",
    "        begin = 0\n",
    "        for batch in batch_sizes:\n",
    "            new_hx = self.rnn(\n",
    "                input[begin:begin + batch],\n",
    "                hx[0:batch],\n",
    "                att_scores[begin:begin + batch])\n",
    "            outputs[begin:begin + batch] = new_hx\n",
    "            hx = new_hx\n",
    "            begin += batch\n",
    "        return PackedSequence(outputs, batch_sizes, sorted_indices, unsorted_indices)\n",
    "\n",
    "# utils\n",
    "#-------------------------------------------------------------------------------------\n",
    "def concat_fun(inputs, axis=-1):\n",
    "    if len(inputs) == 1:\n",
    "        return inputs[0]\n",
    "    else:\n",
    "        return torch.cat(inputs, dim=axis)\n",
    "\n",
    "\n",
    "def slice_arrays(arrays, start=None, stop=None):\n",
    "    \"\"\"Slice an array or list of arrays.\n",
    "\n",
    "    This takes an array-like, or a list of\n",
    "    array-likes, and outputs:\n",
    "        - arrays[start:stop] if `arrays` is an array-like\n",
    "        - [x[start:stop] for x in arrays] if `arrays` is a list\n",
    "\n",
    "    Can also work on list/array of indices: `slice_arrays(x, indices)`\n",
    "\n",
    "    Arguments:\n",
    "        arrays: Single array or list of arrays.\n",
    "        start: can be an integer index (start index)\n",
    "            or a list/array of indices\n",
    "        stop: integer (stop index); should be None if\n",
    "            `start` was a list.\n",
    "\n",
    "    Returns:\n",
    "        A slice of the array(s).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the value of start is a list and stop is not None.\n",
    "    \"\"\"\n",
    "\n",
    "    if arrays is None:\n",
    "        return [None]\n",
    "\n",
    "    if isinstance(arrays, np.ndarray):\n",
    "        arrays = [arrays]\n",
    "\n",
    "    if isinstance(start, list) and stop is not None:\n",
    "        raise ValueError('The stop argument has to be None if the value of start '\n",
    "                         'is a list.')\n",
    "    elif isinstance(arrays, list):\n",
    "        if hasattr(start, '__len__'):\n",
    "            # hdf5 datasets only support list objects as indices\n",
    "            if hasattr(start, 'shape'):\n",
    "                start = start.tolist()\n",
    "            return [None if x is None else x[start] for x in arrays]\n",
    "        else:\n",
    "            if len(arrays) == 1:\n",
    "                return arrays[0][start:stop]\n",
    "            return [None if x is None else x[start:stop] for x in arrays]\n",
    "    else:\n",
    "        if hasattr(start, '__len__'):\n",
    "            if hasattr(start, 'shape'):\n",
    "                start = start.tolist()\n",
    "            return arrays[start]\n",
    "        elif hasattr(start, '__getitem__'):\n",
    "            return arrays[start:stop]\n",
    "        else:\n",
    "            return [None]\n",
    "\n",
    "#inputs\n",
    "#-------------------------------------------------------------------------------------\n",
    "DEFAULT_GROUP_NAME = \"default_group\"\n",
    "\n",
    "\n",
    "class SparseFeat(namedtuple('SparseFeat',\n",
    "                            ['name', 'vocabulary_size', 'embedding_dim', 'use_hash', 'dtype', 'embedding_name',\n",
    "                             'group_name'])):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, name, vocabulary_size, embedding_dim=4, use_hash=False, dtype=\"int32\", embedding_name=None,\n",
    "                group_name=DEFAULT_GROUP_NAME):\n",
    "        if embedding_name is None:\n",
    "            embedding_name = name\n",
    "        if embedding_dim == \"auto\":\n",
    "            embedding_dim = 6 * int(pow(vocabulary_size, 0.25))\n",
    "        if use_hash:\n",
    "            print(\n",
    "                \"Notice! Feature Hashing on the fly currently is not supported in torch version,you can use tensorflow version!\")\n",
    "        return super(SparseFeat, cls).__new__(cls, name, vocabulary_size, embedding_dim, use_hash, dtype,\n",
    "                                              embedding_name, group_name)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.name.__hash__()\n",
    "\n",
    "\n",
    "class VarLenSparseFeat(namedtuple('VarLenSparseFeat',\n",
    "                                  ['sparsefeat', 'maxlen', 'combiner', 'length_name'])):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, sparsefeat, maxlen, combiner=\"mean\", length_name=None):\n",
    "        return super(VarLenSparseFeat, cls).__new__(cls, sparsefeat, maxlen, combiner, length_name)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.sparsefeat.name\n",
    "\n",
    "    @property\n",
    "    def vocabulary_size(self):\n",
    "        return self.sparsefeat.vocabulary_size\n",
    "\n",
    "    @property\n",
    "    def embedding_dim(self):\n",
    "        return self.sparsefeat.embedding_dim\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.sparsefeat.dtype\n",
    "\n",
    "    @property\n",
    "    def embedding_name(self):\n",
    "        return self.sparsefeat.embedding_name\n",
    "\n",
    "    @property\n",
    "    def group_name(self):\n",
    "        return self.sparsefeat.group_name\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.name.__hash__()\n",
    "\n",
    "\n",
    "class DenseFeat(namedtuple('DenseFeat', ['name', 'dimension', 'dtype'])):\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, name, dimension=1, dtype=\"float32\"):\n",
    "        return super(DenseFeat, cls).__new__(cls, name, dimension, dtype)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.name.__hash__()\n",
    "\n",
    "\n",
    "def get_feature_names(feature_columns):\n",
    "    features = build_input_features(feature_columns)\n",
    "    return list(features.keys())\n",
    "\n",
    "\n",
    "# def get_inputs_list(inputs):\n",
    "#     return list(chain(*list(map(lambda x: x.values(), filter(lambda x: x is not None, inputs)))))\n",
    "\n",
    "\n",
    "def build_input_features(feature_columns):\n",
    "    # Return OrderedDict: {feature_name:(start, start+dimension)}\n",
    "\n",
    "    features = OrderedDict()\n",
    "\n",
    "    start = 0\n",
    "    for feat in feature_columns:\n",
    "        feat_name = feat.name\n",
    "        if feat_name in features:\n",
    "            continue\n",
    "        if isinstance(feat, SparseFeat):\n",
    "            features[feat_name] = (start, start + 1)\n",
    "            start += 1\n",
    "        elif isinstance(feat, DenseFeat):\n",
    "            features[feat_name] = (start, start + feat.dimension)\n",
    "            start += feat.dimension\n",
    "        elif isinstance(feat, VarLenSparseFeat):\n",
    "            features[feat_name] = (start, start + feat.maxlen)\n",
    "            start += feat.maxlen\n",
    "            if feat.length_name is not None and feat.length_name not in features:\n",
    "                features[feat.length_name] = (start, start + 1)\n",
    "                start += 1\n",
    "        else:\n",
    "            raise TypeError(\"Invalid feature column type,got\", type(feat))\n",
    "    return features\n",
    "\n",
    "\n",
    "def combined_dnn_input(sparse_embedding_list, dense_value_list):\n",
    "    if len(sparse_embedding_list) > 0 and len(dense_value_list) > 0:\n",
    "        sparse_dnn_input = torch.flatten(\n",
    "            torch.cat(sparse_embedding_list, dim=-1), start_dim=1)\n",
    "        dense_dnn_input = torch.flatten(\n",
    "            torch.cat(dense_value_list, dim=-1), start_dim=1)\n",
    "        return concat_fun([sparse_dnn_input, dense_dnn_input])\n",
    "    elif len(sparse_embedding_list) > 0:\n",
    "        return torch.flatten(torch.cat(sparse_embedding_list, dim=-1), start_dim=1)\n",
    "    elif len(dense_value_list) > 0:\n",
    "        return torch.flatten(torch.cat(dense_value_list, dim=-1), start_dim=1)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def get_varlen_pooling_list(embedding_dict, features, feature_index, varlen_sparse_feature_columns, device):\n",
    "    varlen_sparse_embedding_list = []\n",
    "\n",
    "    for feat in varlen_sparse_feature_columns:\n",
    "        seq_emb = embedding_dict[feat.embedding_name](\n",
    "            features[:, feature_index[feat.name][0]:feature_index[feat.name][1]].long())\n",
    "        if feat.length_name is None:\n",
    "            seq_mask = features[:, feature_index[feat.name][0]:feature_index[feat.name][1]].long() != 0\n",
    "\n",
    "            emb = SequencePoolingLayer(mode=feat.combiner, supports_masking=True, device=device)(\n",
    "                [seq_emb, seq_mask])\n",
    "        else:\n",
    "            seq_length = features[:,\n",
    "                         feature_index[feat.length_name][0]:feature_index[feat.length_name][1]].long()\n",
    "            emb = SequencePoolingLayer(mode=feat.combiner, supports_masking=False, device=device)(\n",
    "                [seq_emb, seq_length])\n",
    "        varlen_sparse_embedding_list.append(emb)\n",
    "    return varlen_sparse_embedding_list\n",
    "\n",
    "\n",
    "def create_embedding_matrix(feature_columns, init_std=0.0001, linear=False, sparse=False, device='cpu'):\n",
    "    # Return nn.ModuleDict: for sparse features, {embedding_name: nn.Embedding}\n",
    "    # for varlen sparse features, {embedding_name: nn.EmbeddingBag}\n",
    "    sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "    varlen_sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "    embedding_dict = nn.ModuleDict(\n",
    "        {feat.embedding_name: nn.Embedding(feat.vocabulary_size, feat.embedding_dim if not linear else 1, sparse=sparse)\n",
    "         for feat in\n",
    "         sparse_feature_columns + varlen_sparse_feature_columns}\n",
    "    )\n",
    "\n",
    "    # for feat in varlen_sparse_feature_columns:\n",
    "    #     embedding_dict[feat.embedding_name] = nn.EmbeddingBag(\n",
    "    #         feat.dimension, embedding_size, sparse=sparse, mode=feat.combiner)\n",
    "\n",
    "    for tensor in embedding_dict.values():\n",
    "        nn.init.normal_(tensor.weight, mean=0, std=init_std)\n",
    "\n",
    "    return embedding_dict.to(device)\n",
    "\n",
    "\n",
    "def input_from_feature_columns(self, X, feature_columns, embedding_dict, support_dense=True):\n",
    "    sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "    dense_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, DenseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "    varlen_sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
    "\n",
    "    if not support_dense and len(dense_feature_columns) > 0:\n",
    "        raise ValueError(\n",
    "            \"DenseFeat is not supported in dnn_feature_columns\")\n",
    "\n",
    "    sparse_embedding_list = [embedding_dict[feat.embedding_name](\n",
    "        X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]].long()) for\n",
    "        feat in sparse_feature_columns]\n",
    "\n",
    "    varlen_sparse_embedding_list = get_varlen_pooling_list(self.embedding_dict, X, self.feature_index,\n",
    "                                                           varlen_sparse_feature_columns, self.device)\n",
    "\n",
    "    dense_value_list = [X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]] for feat in\n",
    "                        dense_feature_columns]\n",
    "\n",
    "    return sparse_embedding_list + varlen_sparse_embedding_list, dense_value_list\n",
    "\n",
    "\n",
    "\n",
    "def embedding_lookup(X, sparse_embedding_dict, sparse_input_dict, sparse_feature_columns, return_feat_list=(),\n",
    "                     mask_feat_list=(), to_list=False):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            X: input Tensor [batch_size x hidden_dim]\n",
    "            sparse_embedding_dict: nn.ModuleDict, {embedding_name: nn.Embedding}\n",
    "            sparse_input_dict: OrderedDict, {feature_name:(start, start+dimension)}\n",
    "            sparse_feature_columns: list, sparse features\n",
    "            return_feat_list: list, names of feature to be returned, defualt () -> return all features\n",
    "            mask_feat_list, list, names of feature to be masked in hash transform\n",
    "        Return:\n",
    "            group_embedding_dict: defaultdict(list)\n",
    "    \"\"\"\n",
    "    group_embedding_dict = defaultdict(list)\n",
    "    for fc in sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        embedding_name = fc.embedding_name\n",
    "        if (len(return_feat_list) == 0 or feature_name in return_feat_list):\n",
    "            # TODO: add hash function\n",
    "            # if fc.use_hash:\n",
    "            #     raise NotImplementedError(\"hash function is not implemented in this version!\")\n",
    "            lookup_idx = np.array(sparse_input_dict[feature_name])\n",
    "            input_tensor = X[:, lookup_idx[0]:lookup_idx[1]].long()\n",
    "            emb = sparse_embedding_dict[embedding_name](input_tensor)\n",
    "            group_embedding_dict[fc.group_name].append(emb)\n",
    "    if to_list:\n",
    "        return list(chain.from_iterable(group_embedding_dict.values()))\n",
    "    return group_embedding_dict\n",
    "\n",
    "\n",
    "def varlen_embedding_lookup(X, embedding_dict, sequence_input_dict, varlen_sparse_feature_columns):\n",
    "    varlen_embedding_vec_dict = {}\n",
    "    for fc in varlen_sparse_feature_columns:\n",
    "        feature_name = fc.name\n",
    "        embedding_name = fc.embedding_name\n",
    "        if fc.use_hash:\n",
    "            # lookup_idx = Hash(fc.vocabulary_size, mask_zero=True)(sequence_input_dict[feature_name])\n",
    "            # TODO: add hash function\n",
    "            lookup_idx = sequence_input_dict[feature_name]\n",
    "        else:\n",
    "            lookup_idx = sequence_input_dict[feature_name]\n",
    "        varlen_embedding_vec_dict[feature_name] = embedding_dict[embedding_name](\n",
    "            X[:, lookup_idx[0]:lookup_idx[1]].long())  # (lookup_idx)\n",
    "\n",
    "    return varlen_embedding_vec_dict\n",
    "\n",
    "\n",
    "def get_dense_input(X, features, feature_columns):\n",
    "    dense_feature_columns = list(filter(lambda x: isinstance(\n",
    "        x, DenseFeat), feature_columns)) if feature_columns else []\n",
    "    dense_input_list = []\n",
    "    for fc in dense_feature_columns:\n",
    "        lookup_idx = np.array(features[fc.name])\n",
    "        input_tensor = X[:, lookup_idx[0]:lookup_idx[1]].float()\n",
    "        dense_input_list.append(input_tensor)\n",
    "    return dense_input_list\n",
    "\n",
    "\n",
    "def maxlen_lookup(X, sparse_input_dict, maxlen_column):\n",
    "    if maxlen_column is None or len(maxlen_column)==0:\n",
    "        raise ValueError('please add max length column for VarLenSparseFeat of DIEN input')\n",
    "    lookup_idx = np.array(sparse_input_dict[maxlen_column[0]])\n",
    "    return X[:, lookup_idx[0]:lookup_idx[1]].long()\n",
    "\n",
    "\n",
    "# basemodel\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, feature_columns, feature_index, init_std=0.0001, device='cpu'):\n",
    "        super(Linear, self).__init__()\n",
    "        self.feature_index = feature_index\n",
    "        self.device = device\n",
    "        self.sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "        self.dense_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, DenseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "        self.varlen_sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "        self.embedding_dict = create_embedding_matrix(feature_columns, init_std, linear=True, sparse=False,\n",
    "                                                      device=device)\n",
    "\n",
    "        #         nn.ModuleDict(\n",
    "        #             {feat.embedding_name: nn.Embedding(feat.dimension, 1, sparse=True) for feat in\n",
    "        #              self.sparse_feature_columns}\n",
    "        #         )\n",
    "        # .to(\"cuda:1\")\n",
    "        for tensor in self.embedding_dict.values():\n",
    "            nn.init.normal_(tensor.weight, mean=0, std=init_std)\n",
    "\n",
    "        if len(self.dense_feature_columns) > 0:\n",
    "            self.weight = nn.Parameter(torch.Tensor(sum(fc.dimension for fc in self.dense_feature_columns), 1)).to(\n",
    "                device)\n",
    "            torch.nn.init.normal_(self.weight, mean=0, std=init_std)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        sparse_embedding_list = [self.embedding_dict[feat.embedding_name](\n",
    "            X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]].long()) for\n",
    "            feat in self.sparse_feature_columns]\n",
    "\n",
    "        dense_value_list = [X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]] for feat in\n",
    "                            self.dense_feature_columns]\n",
    "\n",
    "        varlen_embedding_list = get_varlen_pooling_list(self.embedding_dict, X, self.feature_index,\n",
    "                                                        self.varlen_sparse_feature_columns, self.device)\n",
    "\n",
    "        sparse_embedding_list += varlen_embedding_list\n",
    "\n",
    "        if len(sparse_embedding_list) > 0 and len(dense_value_list) > 0:\n",
    "            linear_sparse_logit = torch.sum(\n",
    "                torch.cat(sparse_embedding_list, dim=-1), dim=-1, keepdim=False)\n",
    "            linear_dense_logit = torch.cat(\n",
    "                dense_value_list, dim=-1).matmul(self.weight)\n",
    "            linear_logit = linear_sparse_logit + linear_dense_logit\n",
    "        elif len(sparse_embedding_list) > 0:\n",
    "            linear_logit = torch.sum(\n",
    "                torch.cat(sparse_embedding_list, dim=-1), dim=-1, keepdim=False)\n",
    "        elif len(dense_value_list) > 0:\n",
    "            linear_logit = torch.cat(\n",
    "                dense_value_list, dim=-1).matmul(self.weight)\n",
    "        else:\n",
    "            linear_logit = torch.zeros([X.shape[0], 1])\n",
    "        return linear_logit\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, linear_feature_columns, dnn_feature_columns, l2_reg_linear=1e-5, l2_reg_embedding=1e-5,\n",
    "                 init_std=0.0001, seed=1024, task='binary', device='cpu'):\n",
    "\n",
    "        super(BaseModel, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.dnn_feature_columns = dnn_feature_columns\n",
    "\n",
    "        self.reg_loss = torch.zeros((1,), device=device)\n",
    "        self.aux_loss = torch.zeros((1,), device=device)\n",
    "        self.device = device  # device\n",
    "\n",
    "        self.feature_index = build_input_features(\n",
    "            linear_feature_columns + dnn_feature_columns)\n",
    "        self.dnn_feature_columns = dnn_feature_columns\n",
    "\n",
    "        self.embedding_dict = create_embedding_matrix(dnn_feature_columns, init_std, sparse=False, device=device)\n",
    "        #         nn.ModuleDict(\n",
    "        #             {feat.embedding_name: nn.Embedding(feat.dimension, embedding_size, sparse=True) for feat in\n",
    "        #              self.dnn_feature_columns}\n",
    "        #         )\n",
    "\n",
    "        self.linear_model = Linear(\n",
    "            linear_feature_columns, self.feature_index, device=device)\n",
    "\n",
    "        self.regularization_weight = []\n",
    "\n",
    "        self.add_regularization_weight(\n",
    "            self.embedding_dict.parameters(), l2_reg_embedding)\n",
    "        self.add_regularization_weight(\n",
    "            self.linear_model.parameters(), l2_reg_linear)\n",
    "\n",
    "        self.out = PredictionLayer(task, )\n",
    "        self.to(device)\n",
    "        self._is_graph_network = True  # used for callbacks\n",
    "\n",
    "    def fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, initial_epoch=0, validation_split=0.,\n",
    "            validation_data=None, shuffle=True, callbacks=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: Numpy array of training data (if the model has a single input), or list of Numpy arrays (if the model has multiple inputs).If input layers in the model are named, you can also pass a\n",
    "            dictionary mapping input names to Numpy arrays.\n",
    "        :param y: Numpy array of target (label) data (if the model has a single output), or list of Numpy arrays (if the model has multiple outputs).\n",
    "        :param batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 256.\n",
    "        :param epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached.\n",
    "        :param verbose: Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "        :param initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run).\n",
    "        :param validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling.\n",
    "        :param validation_data: tuple `(x_val, y_val)` or tuple `(x_val, y_val, val_sample_weights)` on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`.\n",
    "        :param shuffle: Boolean. Whether to shuffle the order of the batches at the beginning of each epoch.\n",
    "        :param callbacks: List of `deepctr_torch.callbacks.Callback` instances. List of callbacks to apply during training and validation (if ). See [callbacks](https://tensorflow.google.cn/api_docs/python/tf/keras/callbacks). Now available: `EarlyStopping` , `ModelCheckpoint`\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(x, dict):\n",
    "            x = [x[feature] for feature in self.feature_index]\n",
    "\n",
    "        do_validation = False\n",
    "        if validation_data:\n",
    "            do_validation = True\n",
    "            if len(validation_data) == 2:\n",
    "                val_x, val_y = validation_data\n",
    "                val_sample_weight = None\n",
    "            elif len(validation_data) == 3:\n",
    "                val_x, val_y, val_sample_weight = validation_data  # pylint: disable=unpacking-non-sequence\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    'When passing a `validation_data` argument, '\n",
    "                    'it must contain either 2 items (x_val, y_val), '\n",
    "                    'or 3 items (x_val, y_val, val_sample_weights), '\n",
    "                    'or alternatively it could be a dataset or a '\n",
    "                    'dataset or a dataset iterator. '\n",
    "                    'However we received `validation_data=%s`' % validation_data)\n",
    "            if isinstance(val_x, dict):\n",
    "                val_x = [val_x[feature] for feature in self.feature_index]\n",
    "\n",
    "        elif validation_split and 0. < validation_split < 1.:\n",
    "            do_validation = True\n",
    "            if hasattr(x[0], 'shape'):\n",
    "                split_at = int(x[0].shape[0] * (1. - validation_split))\n",
    "            else:\n",
    "                split_at = int(len(x[0]) * (1. - validation_split))\n",
    "            x, val_x = (slice_arrays(x, 0, split_at),\n",
    "                        slice_arrays(x, split_at))\n",
    "            y, val_y = (slice_arrays(y, 0, split_at),\n",
    "                        slice_arrays(y, split_at))\n",
    "\n",
    "        else:\n",
    "            val_x = []\n",
    "            val_y = []\n",
    "        for i in range(len(x)):\n",
    "            if len(x[i].shape) == 1:\n",
    "                x[i] = np.expand_dims(x[i], axis=1)\n",
    "\n",
    "        train_tensor_data = Data.TensorDataset(\n",
    "            torch.from_numpy(\n",
    "                np.concatenate(x, axis=-1)),\n",
    "            torch.from_numpy(y))\n",
    "        if batch_size is None:\n",
    "            batch_size = 256\n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_tensor_data, shuffle=shuffle, batch_size=batch_size)\n",
    "\n",
    "        print(self.device, end=\"\\n\")\n",
    "        model = self.train()\n",
    "        loss_func = self.loss_func\n",
    "        optim = self.optim\n",
    "        \n",
    "        # ------------------------------------------------------------\n",
    "        reduceLR      = lr_scheduler.ReduceLROnPlateau(optim, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "        \n",
    "        sample_num = len(train_tensor_data)\n",
    "        steps_per_epoch = (sample_num - 1) // batch_size + 1\n",
    "\n",
    "        callbacks = CallbackList(callbacks)\n",
    "        callbacks.set_model(self)\n",
    "        callbacks.on_train_begin()\n",
    "        self.stop_training = False  # used for early stopping\n",
    "\n",
    "        # Train\n",
    "        print(\"Train on {0} samples, validate on {1} samples, {2} steps per epoch\".format(\n",
    "            len(train_tensor_data), len(val_y), steps_per_epoch))\n",
    "        for epoch in range(initial_epoch, epochs):\n",
    "            callbacks.on_epoch_begin(epoch)\n",
    "            epoch_logs = {}\n",
    "            start_time = time.time()\n",
    "            loss_epoch = 0\n",
    "            total_loss_epoch = 0\n",
    "            train_result = {}\n",
    "            try:\n",
    "                with tqdm(enumerate(train_loader), disable=verbose != 1) as t:\n",
    "                    for index, (x_train, y_train) in t:\n",
    "                        x = x_train.to(self.device).float()\n",
    "                        y = y_train.to(self.device).float()\n",
    "\n",
    "                        y_pred = model(x).squeeze()\n",
    "\n",
    "                        optim.zero_grad()\n",
    "                        loss = loss_func(y_pred, y.squeeze(), reduction='sum')\n",
    "                        reg_loss = self.get_regularization_loss()\n",
    "\n",
    "                        total_loss = loss + reg_loss + self.aux_loss\n",
    "\n",
    "                        loss_epoch += loss.item()\n",
    "                        total_loss_epoch += total_loss.item()\n",
    "                        total_loss.backward(retain_graph=True)\n",
    "                        optim.step()\n",
    "\n",
    "                        if verbose > 0:\n",
    "                            for name, metric_fun in self.metrics.items():\n",
    "                                if name not in train_result:\n",
    "                                    train_result[name] = []\n",
    "                                train_result[name].append(metric_fun(\n",
    "                                    y.cpu().data.numpy(), y_pred.cpu().data.numpy().astype(\"float64\")))\n",
    "\n",
    "             \n",
    "            except KeyboardInterrupt:\n",
    "                t.close()\n",
    "                raise\n",
    "            t.close()\n",
    "           \n",
    "            \n",
    "            # Add epoch_logs\n",
    "            epoch_logs[\"loss\"] = total_loss_epoch / sample_num\n",
    "            \n",
    "           \n",
    "               \n",
    "                \n",
    "            for name, result in train_result.items():\n",
    "                epoch_logs[name] = np.sum(result) / steps_per_epoch\n",
    "\n",
    "            if do_validation:\n",
    "                eval_result = self.evaluate(val_x, val_y, batch_size)\n",
    "                for name, result in eval_result.items():\n",
    "                    epoch_logs[\"val_\" + name] = result\n",
    "            # verbose\n",
    "            if verbose > 0:\n",
    "                epoch_time = int(time.time() - start_time)\n",
    "                print('Epoch {0}/{1}'.format(epoch + 1, epochs))\n",
    "\n",
    "                eval_str = \"{0}s - loss: {1: .4f}\".format(\n",
    "                    epoch_time, epoch_logs[\"loss\"])\n",
    "\n",
    "                for name in self.metrics:\n",
    "                    eval_str += \" - \" + name + \\\n",
    "                                \": {0: .4f}\".format(epoch_logs[name])\n",
    "\n",
    "                if do_validation:\n",
    "                    i_tag = 1\n",
    "                    for name in self.metrics:\n",
    "                        eval_str += \" - \" + \"val_\" + name + \\\n",
    "                                    \": {0: .4f}\".format(epoch_logs[\"val_\" + name])\n",
    "                        \n",
    "                        # -----------------------------------------------------------\n",
    "                        if i_tag == 0:\n",
    "                            reduceLR.step(epoch_logs[\"val_\" + name])\n",
    "                            i_tag = 1\n",
    "                    \n",
    "                print(eval_str)\n",
    "            callbacks.on_epoch_end(epoch, epoch_logs)\n",
    "            if self.stop_training:\n",
    "                break\n",
    "\n",
    "        callbacks.on_train_end()\n",
    "\n",
    "    def evaluate(self, x, y, batch_size=256):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: Numpy array of test data (if the model has a single input), or list of Numpy arrays (if the model has multiple inputs).\n",
    "        :param y: Numpy array of target (label) data (if the model has a single output), or list of Numpy arrays (if the model has multiple outputs).\n",
    "        :param batch_size: Integer or `None`. Number of samples per evaluation step. If unspecified, `batch_size` will default to 256.\n",
    "        :return: Dict contains metric names and metric values.\n",
    "        \"\"\"\n",
    "        pred_ans = self.predict(x, batch_size)\n",
    "        eval_result = {}\n",
    "        for name, metric_fun in self.metrics.items():\n",
    "            eval_result[name] = metric_fun(y, pred_ans)\n",
    "        return eval_result\n",
    "\n",
    "    def predict(self, x, batch_size=256):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs).\n",
    "        :param batch_size: Integer. If unspecified, it will default to 256.\n",
    "        :return: Numpy array(s) of predictions.\n",
    "        \"\"\"\n",
    "        model = self.eval()\n",
    "        if isinstance(x, dict):\n",
    "            x = [x[feature] for feature in self.feature_index]\n",
    "        for i in range(len(x)):\n",
    "            if len(x[i].shape) == 1:\n",
    "                x[i] = np.expand_dims(x[i], axis=1)\n",
    "\n",
    "        tensor_data = Data.TensorDataset(\n",
    "            torch.from_numpy(np.concatenate(x, axis=-1)))\n",
    "        test_loader = DataLoader(\n",
    "            dataset=tensor_data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "        pred_ans = []\n",
    "        with torch.no_grad():\n",
    "            for index, x_test in enumerate(test_loader):\n",
    "                x = x_test[0].to(self.device).float()\n",
    "\n",
    "                y_pred = model(x).cpu().data.numpy()  # .squeeze()\n",
    "                pred_ans.append(y_pred)\n",
    "\n",
    "        return np.concatenate(pred_ans).astype(\"float64\")\n",
    "\n",
    "    def input_from_feature_columns(self, X, feature_columns, embedding_dict, support_dense=True):\n",
    "\n",
    "        sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "        dense_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, DenseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "        varlen_sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if feature_columns else []\n",
    "\n",
    "        if not support_dense and len(dense_feature_columns) > 0:\n",
    "            raise ValueError(\n",
    "                \"DenseFeat is not supported in dnn_feature_columns\")\n",
    "\n",
    "        sparse_embedding_list = [embedding_dict[feat.embedding_name](\n",
    "            X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]].long()) for\n",
    "            feat in sparse_feature_columns]\n",
    "\n",
    "        varlen_sparse_embedding_list = get_varlen_pooling_list(self.embedding_dict, X, self.feature_index,\n",
    "                                                               varlen_sparse_feature_columns, self.device)\n",
    "\n",
    "        dense_value_list = [X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]] for feat in\n",
    "                            dense_feature_columns]\n",
    "\n",
    "        return sparse_embedding_list + varlen_sparse_embedding_list, dense_value_list\n",
    "\n",
    "    def compute_input_dim(self, feature_columns, include_sparse=True, include_dense=True, feature_group=False):\n",
    "        sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, (SparseFeat, VarLenSparseFeat)), feature_columns)) if len(\n",
    "            feature_columns) else []\n",
    "        dense_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, DenseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "        dense_input_dim = sum(\n",
    "            map(lambda x: x.dimension, dense_feature_columns))\n",
    "        if feature_group:\n",
    "            sparse_input_dim = len(sparse_feature_columns)\n",
    "        else:\n",
    "            sparse_input_dim = sum(feat.embedding_dim for feat in sparse_feature_columns)\n",
    "        input_dim = 0\n",
    "        if include_sparse:\n",
    "            input_dim += sparse_input_dim\n",
    "        if include_dense:\n",
    "            input_dim += dense_input_dim\n",
    "        return input_dim\n",
    "\n",
    "    def add_regularization_weight(self, weight_list, weight_decay, p=2):\n",
    "        self.regularization_weight.append((list(weight_list), weight_decay, p))\n",
    "\n",
    "    def get_regularization_loss(self, ):\n",
    "        total_reg_loss = torch.zeros((1,), device=self.device)\n",
    "        for weight_list, weight_decay, p in self.regularization_weight:\n",
    "            weight_reg_loss = torch.zeros((1,), device=self.device)\n",
    "            for w in weight_list:\n",
    "                if isinstance(w, tuple):\n",
    "                    l2_reg = torch.norm(w[1], p=p, )\n",
    "                else:\n",
    "                    l2_reg = torch.norm(w, p=p, )\n",
    "                weight_reg_loss = weight_reg_loss + l2_reg\n",
    "            reg_loss = weight_decay * weight_reg_loss\n",
    "            total_reg_loss += reg_loss\n",
    "        return total_reg_loss\n",
    "\n",
    "    def add_auxiliary_loss(self, aux_loss, alpha):\n",
    "        self.aux_loss = aux_loss * alpha\n",
    "\n",
    "    def compile(self, optimizer,\n",
    "                loss=None,\n",
    "                metrics=None,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        :param optimizer: String (name of optimizer) or optimizer instance. See [optimizers](https://pytorch.org/docs/stable/optim.html).\n",
    "        :param loss: String (name of objective function) or objective function. See [losses](https://pytorch.org/docs/stable/nn.functional.html#loss-functions).\n",
    "        :param metrics: List of metrics to be evaluated by the model during training and testing. Typically you will use `metrics=['accuracy']`.\n",
    "        \"\"\"\n",
    "        self.metrics_names = [\"loss\"]\n",
    "        self.optim = self._get_optim(optimizer)\n",
    "        self.loss_func = self._get_loss_func(loss)\n",
    "        self.metrics = self._get_metrics(metrics)\n",
    "\n",
    "    def _get_optim(self, optimizer):\n",
    "        if isinstance(optimizer, str):\n",
    "            if optimizer == \"sgd\":\n",
    "                optim = torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "            elif optimizer == \"adam\":\n",
    "                optim = torch.optim.Adam(self.parameters())  # 0.001\n",
    "            elif optimizer == \"adagrad\":\n",
    "                optim = torch.optim.Adagrad(self.parameters())  # 0.01\n",
    "            elif optimizer == \"rmsprop\":\n",
    "                optim = torch.optim.RMSprop(self.parameters())\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            optim = optimizer\n",
    "        return optim\n",
    "\n",
    "    def _get_loss_func(self, loss):\n",
    "        if isinstance(loss, str):\n",
    "            if loss == \"binary_crossentropy\":\n",
    "                loss_func = F.binary_cross_entropy\n",
    "            elif loss == \"mse\":\n",
    "                loss_func = F.mse_loss\n",
    "            elif loss == \"mae\":\n",
    "                loss_func = F.l1_loss\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            loss_func = loss\n",
    "        return loss_func\n",
    "\n",
    "    def _log_loss(self, y_true, y_pred, eps=1e-7, normalize=True, sample_weight=None, labels=None):\n",
    "        # change eps to improve calculation accuracy\n",
    "        return log_loss(y_true,\n",
    "                        y_pred,\n",
    "                        eps,\n",
    "                        normalize,\n",
    "                        sample_weight,\n",
    "                        labels)\n",
    "\n",
    "    def _get_metrics(self, metrics, set_eps=False):\n",
    "        metrics_ = {}\n",
    "        if metrics:\n",
    "            for metric in metrics:\n",
    "                if metric == \"binary_crossentropy\" or metric == \"logloss\":\n",
    "                    if set_eps:\n",
    "                        metrics_[metric] = self._log_loss\n",
    "                    else:\n",
    "                        metrics_[metric] = log_loss\n",
    "                if metric == \"auc\":\n",
    "                    metrics_[metric] = roc_auc_score\n",
    "                if metric == \"mse\":\n",
    "                    metrics_[metric] = mean_squared_error\n",
    "                if metric == \"accuracy\" or metric == \"acc\":\n",
    "                    metrics_[metric] = lambda y_true, y_pred: accuracy_score(\n",
    "                        y_true, np.where(y_pred > 0.5, 1, 0))\n",
    "                self.metrics_names.append(metric)\n",
    "        return metrics_\n",
    "\n",
    "    @property\n",
    "    def embedding_size(self, ):\n",
    "        feature_columns = self.dnn_feature_columns\n",
    "        sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, (SparseFeat, VarLenSparseFeat)), feature_columns)) if len(\n",
    "            feature_columns) else []\n",
    "        embedding_size_set = set([feat.embedding_dim for feat in sparse_feature_columns])\n",
    "        if len(embedding_size_set) > 1:\n",
    "            raise ValueError(\"embedding_dim of SparseFeat and VarlenSparseFeat must be same in this model!\")\n",
    "        return list(embedding_size_set)[0]\n",
    "\n",
    "# interaction\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "class FM(nn.Module):\n",
    "    \"\"\"Factorization Machine models pairwise (order-2) feature interactions\n",
    "     without linear term and bias.\n",
    "      Input shape\n",
    "        - 3D tensor with shape: ``(batch_size,field_size,embedding_size)``.\n",
    "      Output shape\n",
    "        - 2D tensor with shape: ``(batch_size, 1)``.\n",
    "      References\n",
    "        - [Factorization Machines](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FM, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        fm_input = inputs\n",
    "\n",
    "        square_of_sum = torch.pow(torch.sum(fm_input, dim=1, keepdim=True), 2)\n",
    "        sum_of_square = torch.sum(fm_input * fm_input, dim=1, keepdim=True)\n",
    "        cross_term = square_of_sum - sum_of_square\n",
    "        cross_term = 0.5 * torch.sum(cross_term, dim=2, keepdim=False)\n",
    "\n",
    "        return cross_term\n",
    "\n",
    "\n",
    "class BiInteractionPooling(nn.Module):\n",
    "    \"\"\"Bi-Interaction Layer used in Neural FM,compress the\n",
    "     pairwise element-wise product of features into one single vector.\n",
    "\n",
    "      Input shape\n",
    "        - A 3D tensor with shape:``(batch_size,field_size,embedding_size)``.\n",
    "\n",
    "      Output shape\n",
    "        - 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n",
    "\n",
    "      References\n",
    "        - [He X, Chua T S. Neural factorization machines for sparse predictive analytics[C]//Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 2017: 355-364.](http://arxiv.org/abs/1708.05027)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BiInteractionPooling, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        concated_embeds_value = inputs\n",
    "        square_of_sum = torch.pow(\n",
    "            torch.sum(concated_embeds_value, dim=1, keepdim=True), 2)\n",
    "        sum_of_square = torch.sum(\n",
    "            concated_embeds_value * concated_embeds_value, dim=1, keepdim=True)\n",
    "        cross_term = 0.5 * (square_of_sum - sum_of_square)\n",
    "        return cross_term\n",
    "\n",
    "\n",
    "class SENETLayer(nn.Module):\n",
    "    \"\"\"SENETLayer used in FiBiNET.\n",
    "      Input shape\n",
    "        - A list of 3D tensor with shape: ``(batch_size,filed_size,embedding_size)``.\n",
    "      Output shape\n",
    "        - A list of 3D tensor with shape: ``(batch_size,filed_size,embedding_size)``.\n",
    "      Arguments\n",
    "        - **filed_size** : Positive integer, number of feature groups.\n",
    "        - **reduction_ratio** : Positive integer, dimensionality of the\n",
    "         attention network output space.\n",
    "        - **seed** : A Python integer to use as random seed.\n",
    "      References\n",
    "        - [FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction\n",
    "Tongwen](https://arxiv.org/pdf/1905.09433.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filed_size, reduction_ratio=3, seed=1024, device='cpu'):\n",
    "        super(SENETLayer, self).__init__()\n",
    "        self.seed = seed\n",
    "        self.filed_size = filed_size\n",
    "        self.reduction_size = max(1, filed_size // reduction_ratio)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(self.filed_size, self.reduction_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.reduction_size, self.filed_size, bias=False),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if len(inputs.shape) != 3:\n",
    "            raise ValueError(\n",
    "                \"Unexpected inputs dimensions %d, expect to be 3 dimensions\" % (len(inputs.shape)))\n",
    "        Z = torch.mean(inputs, dim=-1, out=None)\n",
    "        A = self.excitation(Z)\n",
    "        V = torch.mul(inputs, torch.unsqueeze(A, dim=2))\n",
    "\n",
    "        return V\n",
    "\n",
    "\n",
    "class BilinearInteraction(nn.Module):\n",
    "    \"\"\"BilinearInteraction Layer used in FiBiNET.\n",
    "      Input shape\n",
    "        - A list of 3D tensor with shape: ``(batch_size,filed_size, embedding_size)``.\n",
    "      Output shape\n",
    "        - 3D tensor with shape: ``(batch_size,filed_size, embedding_size)``.\n",
    "      Arguments\n",
    "        - **filed_size** : Positive integer, number of feature groups.\n",
    "        - **str** : String, types of bilinear functions used in this layer.\n",
    "        - **seed** : A Python integer to use as random seed.\n",
    "      References\n",
    "        - [FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction\n",
    "Tongwen](https://arxiv.org/pdf/1905.09433.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filed_size, embedding_size, bilinear_type=\"interaction\", seed=1024, device='cpu'):\n",
    "        super(BilinearInteraction, self).__init__()\n",
    "        self.bilinear_type = bilinear_type\n",
    "        self.seed = seed\n",
    "        self.bilinear = nn.ModuleList()\n",
    "        if self.bilinear_type == \"all\":\n",
    "            self.bilinear = nn.Linear(\n",
    "                embedding_size, embedding_size, bias=False)\n",
    "        elif self.bilinear_type == \"each\":\n",
    "            for i in range(filed_size):\n",
    "                self.bilinear.append(\n",
    "                    nn.Linear(embedding_size, embedding_size, bias=False))\n",
    "        elif self.bilinear_type == \"interaction\":\n",
    "            for i, j in itertools.combinations(range(filed_size), 2):\n",
    "                self.bilinear.append(\n",
    "                    nn.Linear(embedding_size, embedding_size, bias=False))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if len(inputs.shape) != 3:\n",
    "            raise ValueError(\n",
    "                \"Unexpected inputs dimensions %d, expect to be 3 dimensions\" % (len(inputs.shape)))\n",
    "        inputs = torch.split(inputs, 1, dim=1)\n",
    "        if self.bilinear_type == \"all\":\n",
    "            p = [torch.mul(self.bilinear(v_i), v_j)\n",
    "                 for v_i, v_j in itertools.combinations(inputs, 2)]\n",
    "        elif self.bilinear_type == \"each\":\n",
    "            p = [torch.mul(self.bilinear[i](inputs[i]), inputs[j])\n",
    "                 for i, j in itertools.combinations(range(len(inputs)), 2)]\n",
    "        elif self.bilinear_type == \"interaction\":\n",
    "            p = [torch.mul(bilinear(v[0]), v[1])\n",
    "                 for v, bilinear in zip(itertools.combinations(inputs, 2), self.bilinear)]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return torch.cat(p, dim=1)\n",
    "\n",
    "\n",
    "class CIN(nn.Module):\n",
    "    \"\"\"Compressed Interaction Network used in xDeepFM.\n",
    "      Input shape\n",
    "        - 3D tensor with shape: ``(batch_size,field_size,embedding_size)``.\n",
    "      Output shape\n",
    "        - 2D tensor with shape: ``(batch_size, featuremap_num)`` ``featuremap_num =  sum(self.layer_size[:-1]) // 2 + self.layer_size[-1]`` if ``split_half=True``,else  ``sum(layer_size)`` .\n",
    "      Arguments\n",
    "        - **filed_size** : Positive integer, number of feature groups.\n",
    "        - **layer_size** : list of int.Feature maps in each layer.\n",
    "        - **activation** : activation function name used on feature maps.\n",
    "        - **split_half** : bool.if set to False, half of the feature maps in each hidden will connect to output unit.\n",
    "        - **seed** : A Python integer to use as random seed.\n",
    "      References\n",
    "        - [Lian J, Zhou X, Zhang F, et al. xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems[J]. arXiv preprint arXiv:1803.05170, 2018.] (https://arxiv.org/pdf/1803.05170.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field_size, layer_size=(128, 128), activation='relu', split_half=True, l2_reg=1e-5, seed=1024,\n",
    "                 device='cpu'):\n",
    "        super(CIN, self).__init__()\n",
    "        if len(layer_size) == 0:\n",
    "            raise ValueError(\n",
    "                \"layer_size must be a list(tuple) of length greater than 1\")\n",
    "\n",
    "        self.layer_size = layer_size\n",
    "        self.field_nums = [field_size]\n",
    "        self.split_half = split_half\n",
    "        self.activation = activation_layer(activation)\n",
    "        self.l2_reg = l2_reg\n",
    "        self.seed = seed\n",
    "\n",
    "        self.conv1ds = nn.ModuleList()\n",
    "        for i, size in enumerate(self.layer_size):\n",
    "            self.conv1ds.append(\n",
    "                nn.Conv1d(self.field_nums[-1] * self.field_nums[0], size, 1))\n",
    "\n",
    "            if self.split_half:\n",
    "                if i != len(self.layer_size) - 1 and size % 2 > 0:\n",
    "                    raise ValueError(\n",
    "                        \"layer_size must be even number except for the last layer when split_half=True\")\n",
    "\n",
    "                self.field_nums.append(size // 2)\n",
    "            else:\n",
    "                self.field_nums.append(size)\n",
    "\n",
    "        #         for tensor in self.conv1ds:\n",
    "        #             nn.init.normal_(tensor.weight, mean=0, std=init_std)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if len(inputs.shape) != 3:\n",
    "            raise ValueError(\n",
    "                \"Unexpected inputs dimensions %d, expect to be 3 dimensions\" % (len(inputs.shape)))\n",
    "        batch_size = inputs.shape[0]\n",
    "        dim = inputs.shape[-1]\n",
    "        hidden_nn_layers = [inputs]\n",
    "        final_result = []\n",
    "\n",
    "        for i, size in enumerate(self.layer_size):\n",
    "            # x^(k-1) * x^0\n",
    "            x = torch.einsum(\n",
    "                'bhd,bmd->bhmd', hidden_nn_layers[-1], hidden_nn_layers[0])\n",
    "            # x.shape = (batch_size , hi * m, dim)\n",
    "            x = x.reshape(\n",
    "                batch_size, hidden_nn_layers[-1].shape[1] * hidden_nn_layers[0].shape[1], dim)\n",
    "            # x.shape = (batch_size , hi, dim)\n",
    "            x = self.conv1ds[i](x)\n",
    "\n",
    "            if self.activation is None or self.activation == 'linear':\n",
    "                curr_out = x\n",
    "            else:\n",
    "                curr_out = self.activation(x)\n",
    "\n",
    "            if self.split_half:\n",
    "                if i != len(self.layer_size) - 1:\n",
    "                    next_hidden, direct_connect = torch.split(\n",
    "                        curr_out, 2 * [size // 2], 1)\n",
    "                else:\n",
    "                    direct_connect = curr_out\n",
    "                    next_hidden = 0\n",
    "            else:\n",
    "                direct_connect = curr_out\n",
    "                next_hidden = curr_out\n",
    "\n",
    "            final_result.append(direct_connect)\n",
    "            hidden_nn_layers.append(next_hidden)\n",
    "\n",
    "        result = torch.cat(final_result, dim=1)\n",
    "        result = torch.sum(result, -1)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class AFMLayer(nn.Module):\n",
    "    \"\"\"Attentonal Factorization Machine models pairwise (order-2) feature\n",
    "    interactions without linear term and bias.\n",
    "      Input shape\n",
    "        - A list of 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n",
    "      Output shape\n",
    "        - 2D tensor with shape: ``(batch_size, 1)``.\n",
    "      Arguments\n",
    "        - **in_features** : Positive integer, dimensionality of input features.\n",
    "        - **attention_factor** : Positive integer, dimensionality of the\n",
    "         attention network output space.\n",
    "        - **l2_reg_w** : float between 0 and 1. L2 regularizer strength\n",
    "         applied to attention network.\n",
    "        - **dropout_rate** : float between in [0,1). Fraction of the attention net output units to dropout.\n",
    "        - **seed** : A Python integer to use as random seed.\n",
    "      References\n",
    "        - [Attentional Factorization Machines : Learning the Weight of Feature\n",
    "        Interactions via Attention Networks](https://arxiv.org/pdf/1708.04617.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, attention_factor=4, l2_reg_w=0, dropout_rate=0, seed=1024, device='cpu'):\n",
    "        super(AFMLayer, self).__init__()\n",
    "        self.attention_factor = attention_factor\n",
    "        self.l2_reg_w = l2_reg_w\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.seed = seed\n",
    "        embedding_size = in_features\n",
    "\n",
    "        self.attention_W = nn.Parameter(torch.Tensor(\n",
    "            embedding_size, self.attention_factor))\n",
    "\n",
    "        self.attention_b = nn.Parameter(torch.Tensor(self.attention_factor))\n",
    "\n",
    "        self.projection_h = nn.Parameter(\n",
    "            torch.Tensor(self.attention_factor, 1))\n",
    "\n",
    "        self.projection_p = nn.Parameter(torch.Tensor(embedding_size, 1))\n",
    "\n",
    "        for tensor in [self.attention_W, self.projection_h, self.projection_p]:\n",
    "            nn.init.xavier_normal_(tensor, )\n",
    "\n",
    "        for tensor in [self.attention_b]:\n",
    "            nn.init.zeros_(tensor, )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds_vec_list = inputs\n",
    "        row = []\n",
    "        col = []\n",
    "\n",
    "        for r, c in itertools.combinations(embeds_vec_list, 2):\n",
    "            row.append(r)\n",
    "            col.append(c)\n",
    "\n",
    "        p = torch.cat(row, dim=1)\n",
    "        q = torch.cat(col, dim=1)\n",
    "        inner_product = p * q\n",
    "\n",
    "        bi_interaction = inner_product\n",
    "        attention_temp = F.relu(torch.tensordot(\n",
    "            bi_interaction, self.attention_W, dims=([-1], [0])) + self.attention_b)\n",
    "\n",
    "        self.normalized_att_score = F.softmax(torch.tensordot(\n",
    "            attention_temp, self.projection_h, dims=([-1], [0])), dim=1)\n",
    "        attention_output = torch.sum(\n",
    "            self.normalized_att_score * bi_interaction, dim=1)\n",
    "\n",
    "        attention_output = self.dropout(attention_output)  # training\n",
    "\n",
    "        afm_out = torch.tensordot(\n",
    "            attention_output, self.projection_p, dims=([-1], [0]))\n",
    "        return afm_out\n",
    "\n",
    "\n",
    "class InteractingLayer(nn.Module):\n",
    "    \"\"\"A Layer used in AutoInt that model the correlations between different feature fields by multi-head self-attention mechanism.\n",
    "      Input shape\n",
    "            - A 3D tensor with shape: ``(batch_size,field_size,embedding_size)``.\n",
    "      Output shape\n",
    "            - 3D tensor with shape:``(batch_size,field_size,att_embedding_size * head_num)``.\n",
    "      Arguments\n",
    "            - **in_features** : Positive integer, dimensionality of input features.\n",
    "            - **att_embedding_size**: int.The embedding size in multi-head self-attention network.\n",
    "            - **head_num**: int.The head number in multi-head  self-attention network.\n",
    "            - **use_res**: bool.Whether or not use standard residual connections before output.\n",
    "            - **seed**: A Python integer to use as random seed.\n",
    "      References\n",
    "            - [Song W, Shi C, Xiao Z, et al. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks[J]. arXiv preprint arXiv:1810.11921, 2018.](https://arxiv.org/abs/1810.11921)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, att_embedding_size=8, head_num=2, use_res=True, seed=1024, device='cpu'):\n",
    "        super(InteractingLayer, self).__init__()\n",
    "        if head_num <= 0:\n",
    "            raise ValueError('head_num must be a int > 0')\n",
    "        self.att_embedding_size = att_embedding_size\n",
    "        self.head_num = head_num\n",
    "        self.use_res = use_res\n",
    "        self.seed = seed\n",
    "\n",
    "        embedding_size = in_features\n",
    "\n",
    "        self.W_Query = nn.Parameter(torch.Tensor(\n",
    "            embedding_size, self.att_embedding_size * self.head_num))\n",
    "\n",
    "        self.W_key = nn.Parameter(torch.Tensor(\n",
    "            embedding_size, self.att_embedding_size * self.head_num))\n",
    "\n",
    "        self.W_Value = nn.Parameter(torch.Tensor(\n",
    "            embedding_size, self.att_embedding_size * self.head_num))\n",
    "\n",
    "        if self.use_res:\n",
    "            self.W_Res = nn.Parameter(torch.Tensor(\n",
    "                embedding_size, self.att_embedding_size * self.head_num))\n",
    "        for tensor in self.parameters():\n",
    "            nn.init.normal_(tensor, mean=0.0, std=0.05)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        if len(inputs.shape) != 3:\n",
    "            raise ValueError(\n",
    "                \"Unexpected inputs dimensions %d, expect to be 3 dimensions\" % (len(inputs.shape)))\n",
    "\n",
    "        querys = torch.tensordot(inputs, self.W_Query,\n",
    "                                 dims=([-1], [0]))  # None F D*head_num\n",
    "        keys = torch.tensordot(inputs, self.W_key, dims=([-1], [0]))\n",
    "        values = torch.tensordot(inputs, self.W_Value, dims=([-1], [0]))\n",
    "\n",
    "        # head_num None F D\n",
    "\n",
    "        querys = torch.stack(torch.split(\n",
    "            querys, self.att_embedding_size, dim=2))\n",
    "        keys = torch.stack(torch.split(keys, self.att_embedding_size, dim=2))\n",
    "        values = torch.stack(torch.split(\n",
    "            values, self.att_embedding_size, dim=2))\n",
    "        inner_product = torch.einsum(\n",
    "            'bnik,bnjk->bnij', querys, keys)  # head_num None F F\n",
    "\n",
    "        self.normalized_att_scores = F.softmax(\n",
    "            inner_product, dim=-1)  # head_num None F F\n",
    "        result = torch.matmul(self.normalized_att_scores,\n",
    "                              values)  # head_num None F D\n",
    "\n",
    "        result = torch.cat(torch.split(result, 1, ), dim=-1)\n",
    "        result = torch.squeeze(result, dim=0)  # None F D*head_num\n",
    "        if self.use_res:\n",
    "            result += torch.tensordot(inputs, self.W_Res, dims=([-1], [0]))\n",
    "        result = F.relu(result)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class CrossNet(nn.Module):\n",
    "    \"\"\"The Cross Network part of Deep&Cross Network model,\n",
    "    which leans both low and high degree cross feature.\n",
    "      Input shape\n",
    "        - 2D tensor with shape: ``(batch_size, units)``.\n",
    "      Output shape\n",
    "        - 2D tensor with shape: ``(batch_size, units)``.\n",
    "      Arguments\n",
    "        - **in_features** : Positive integer, dimensionality of input features.\n",
    "        - **input_feature_num**: Positive integer, shape(Input tensor)[-1]\n",
    "        - **layer_num**: Positive integer, the cross layer number\n",
    "        - **parameterization**: string, ``\"vector\"``  or ``\"matrix\"`` ,  way to parameterize the cross network.\n",
    "        - **l2_reg**: float between 0 and 1. L2 regularizer strength applied to the kernel weights matrix\n",
    "        - **seed**: A Python integer to use as random seed.\n",
    "      References\n",
    "        - [Wang R, Fu B, Fu G, et al. Deep & cross network for ad click predictions[C]//Proceedings of the ADKDD'17. ACM, 2017: 12.](https://arxiv.org/abs/1708.05123)\n",
    "        - [Wang R, Shivanna R, Cheng D Z, et al. DCN-M: Improved Deep & Cross Network for Feature Cross Learning in Web-scale Learning to Rank Systems[J]. 2020.](https://arxiv.org/abs/2008.13535)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, layer_num=2, parameterization='vector', seed=1024, device='cpu'):\n",
    "        super(CrossNet, self).__init__()\n",
    "        self.layer_num = layer_num\n",
    "        self.parameterization = parameterization\n",
    "        if self.parameterization == 'vector':\n",
    "            # weight in DCN.  (in_features, 1)\n",
    "            self.kernels = torch.nn.ParameterList(\n",
    "                [nn.Parameter(nn.init.xavier_normal_(torch.empty(in_features, 1))) for i in range(self.layer_num)])\n",
    "        elif self.parameterization == 'matrix':\n",
    "            # weight matrix in DCN-M.  (in_features, in_features)\n",
    "            self.kernels = torch.nn.ParameterList([nn.Parameter(nn.init.xavier_normal_(\n",
    "                torch.empty(in_features, in_features))) for i in range(self.layer_num)])\n",
    "        else:  # error\n",
    "            raise ValueError(\"parameterization should be 'vector' or 'matrix'\")\n",
    "\n",
    "        self.bias = torch.nn.ParameterList(\n",
    "            [nn.Parameter(nn.init.zeros_(torch.empty(in_features, 1))) for i in range(self.layer_num)])\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_0 = inputs.unsqueeze(2)\n",
    "        x_l = x_0\n",
    "        for i in range(self.layer_num):\n",
    "            if self.parameterization == 'vector':\n",
    "                xl_w = torch.tensordot(x_l, self.kernels[i], dims=([1], [0]))\n",
    "                dot_ = torch.matmul(x_0, xl_w)\n",
    "                x_l = dot_ + self.bias[i]\n",
    "            elif self.parameterization == 'matrix':\n",
    "                dot_ = torch.matmul(self.kernels[i], x_l)  # W * xi  (bs, in_features, 1)\n",
    "                dot_ = dot_ + self.bias[i]  # W * xi + b\n",
    "                dot_ = x_0 * dot_  # x0 · (W * xi + b)  Hadamard-product\n",
    "            else:  # error\n",
    "                print(\"parameterization should be 'vector' or 'matrix'\")\n",
    "                pass\n",
    "            x_l = dot_ + x_l\n",
    "        x_l = torch.squeeze(x_l, dim=2)\n",
    "        return x_l\n",
    "\n",
    "\n",
    "class CrossNetMix(nn.Module):\n",
    "    \"\"\"The Cross Network part of DCN-Mix model, which improves DCN-M by:\n",
    "      1 add MOE to learn feature interactions in different subspaces\n",
    "      2 add nonlinear transformations in low-dimensional space\n",
    "      Input shape\n",
    "        - 2D tensor with shape: ``(batch_size, units)``.\n",
    "      Output shape\n",
    "        - 2D tensor with shape: ``(batch_size, units)``.\n",
    "      Arguments\n",
    "        - **in_features** : Positive integer, dimensionality of input features.\n",
    "        - **low_rank** : Positive integer, dimensionality of low-rank sapce.\n",
    "        - **num_experts** : Positive integer, number of experts.\n",
    "        - **layer_num**: Positive integer, the cross layer number\n",
    "        - **device**: str, e.g. ``\"cpu\"`` or ``\"cuda:0\"``\n",
    "      References\n",
    "        - [Wang R, Shivanna R, Cheng D Z, et al. DCN-M: Improved Deep & Cross Network for Feature Cross Learning in Web-scale Learning to Rank Systems[J]. 2020.](https://arxiv.org/abs/2008.13535)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, low_rank=32, num_experts=4, layer_num=2, device='cpu'):\n",
    "        super(CrossNetMix, self).__init__()\n",
    "        self.layer_num = layer_num\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        # U: (in_features, low_rank)\n",
    "        self.U_list = torch.nn.ParameterList([nn.Parameter(nn.init.xavier_normal_(\n",
    "            torch.empty(num_experts, in_features, low_rank))) for i in range(self.layer_num)])\n",
    "        # V: (in_features, low_rank)\n",
    "        self.V_list = torch.nn.ParameterList([nn.Parameter(nn.init.xavier_normal_(\n",
    "            torch.empty(num_experts, in_features, low_rank))) for i in range(self.layer_num)])\n",
    "        # C: (low_rank, low_rank)\n",
    "        self.C_list = torch.nn.ParameterList([nn.Parameter(nn.init.xavier_normal_(\n",
    "            torch.empty(num_experts, low_rank, low_rank))) for i in range(self.layer_num)])\n",
    "        self.gating = nn.ModuleList([nn.Linear(in_features, 1, bias=False) for i in range(self.num_experts)])\n",
    "\n",
    "        self.bias = torch.nn.ParameterList([nn.Parameter(nn.init.zeros_(\n",
    "            torch.empty(in_features, 1))) for i in range(self.layer_num)])\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x_0 = inputs.unsqueeze(2)  # (bs, in_features, 1)\n",
    "        x_l = x_0\n",
    "        for i in range(self.layer_num):\n",
    "            output_of_experts = []\n",
    "            gating_score_of_experts = []\n",
    "            for expert_id in range(self.num_experts):\n",
    "                # (1) G(x_l)\n",
    "                # compute the gating score by x_l\n",
    "                gating_score_of_experts.append(self.gating[expert_id](x_l.squeeze(2)))\n",
    "\n",
    "                # (2) E(x_l)\n",
    "                # project the input x_l to $\\mathbb{R}^{r}$\n",
    "                v_x = torch.matmul(self.V_list[i][expert_id].T, x_l)  # (bs, low_rank, 1)\n",
    "\n",
    "                # nonlinear activation in low rank space\n",
    "                v_x = torch.tanh(v_x)\n",
    "                v_x = torch.matmul(self.C_list[i][expert_id], v_x)\n",
    "                v_x = torch.tanh(v_x)\n",
    "\n",
    "                # project back to $\\mathbb{R}^{d}$\n",
    "                uv_x = torch.matmul(self.U_list[i][expert_id], v_x)  # (bs, in_features, 1)\n",
    "\n",
    "                dot_ = uv_x + self.bias[i]\n",
    "                dot_ = x_0 * dot_  # Hadamard-product\n",
    "\n",
    "                output_of_experts.append(dot_.squeeze(2))\n",
    "\n",
    "            # (3) mixture of low-rank experts\n",
    "            output_of_experts = torch.stack(output_of_experts, 2)  # (bs, in_features, num_experts)\n",
    "            gating_score_of_experts = torch.stack(gating_score_of_experts, 1)  # (bs, num_experts, 1)\n",
    "            moe_out = torch.matmul(output_of_experts, gating_score_of_experts.softmax(1))\n",
    "            x_l = moe_out + x_l  # (bs, in_features, 1)\n",
    "\n",
    "        x_l = x_l.squeeze()  # (bs, in_features)\n",
    "        return x_l\n",
    "\n",
    "\n",
    "class InnerProductLayer(nn.Module):\n",
    "    \"\"\"InnerProduct Layer used in PNN that compute the element-wise\n",
    "    product or inner product between feature vectors.\n",
    "      Input shape\n",
    "        - a list of 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n",
    "      Output shape\n",
    "        - 3D tensor with shape: ``(batch_size, N*(N-1)/2 ,1)`` if use reduce_sum. or 3D tensor with shape:\n",
    "        ``(batch_size, N*(N-1)/2, embedding_size )`` if not use reduce_sum.\n",
    "      Arguments\n",
    "        - **reduce_sum**: bool. Whether return inner product or element-wise product\n",
    "      References\n",
    "            - [Qu Y, Cai H, Ren K, et al. Product-based neural networks for user response prediction[C]//\n",
    "            Data Mining (ICDM), 2016 IEEE 16th International Conference on. IEEE, 2016: 1149-1154.]\n",
    "            (https://arxiv.org/pdf/1611.00144.pdf)\"\"\"\n",
    "\n",
    "    def __init__(self, reduce_sum=True, device='cpu'):\n",
    "        super(InnerProductLayer, self).__init__()\n",
    "        self.reduce_sum = reduce_sum\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        embed_list = inputs\n",
    "        row = []\n",
    "        col = []\n",
    "        num_inputs = len(embed_list)\n",
    "\n",
    "        for i in range(num_inputs - 1):\n",
    "            for j in range(i + 1, num_inputs):\n",
    "                row.append(i)\n",
    "                col.append(j)\n",
    "        p = torch.cat([embed_list[idx]\n",
    "                       for idx in row], dim=1)  # batch num_pairs k\n",
    "        q = torch.cat([embed_list[idx]\n",
    "                       for idx in col], dim=1)\n",
    "\n",
    "        inner_product = p * q\n",
    "        if self.reduce_sum:\n",
    "            inner_product = torch.sum(\n",
    "                inner_product, dim=2, keepdim=True)\n",
    "        return inner_product\n",
    "\n",
    "\n",
    "class OutterProductLayer(nn.Module):\n",
    "    \"\"\"OutterProduct Layer used in PNN.This implemention is\n",
    "    adapted from code that the author of the paper published on https://github.com/Atomu2014/product-nets.\n",
    "      Input shape\n",
    "            - A list of N 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n",
    "      Output shape\n",
    "            - 2D tensor with shape:``(batch_size,N*(N-1)/2 )``.\n",
    "      Arguments\n",
    "            - **filed_size** : Positive integer, number of feature groups.\n",
    "            - **kernel_type**: str. The kernel weight matrix type to use,can be mat,vec or num\n",
    "            - **seed**: A Python integer to use as random seed.\n",
    "      References\n",
    "            - [Qu Y, Cai H, Ren K, et al. Product-based neural networks for user response prediction[C]//Data Mining (ICDM), 2016 IEEE 16th International Conference on. IEEE, 2016: 1149-1154.](https://arxiv.org/pdf/1611.00144.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field_size, embedding_size, kernel_type='mat', seed=1024, device='cpu'):\n",
    "        super(OutterProductLayer, self).__init__()\n",
    "        self.kernel_type = kernel_type\n",
    "\n",
    "        num_inputs = field_size\n",
    "        num_pairs = int(num_inputs * (num_inputs - 1) / 2)\n",
    "        embed_size = embedding_size\n",
    "        if self.kernel_type == 'mat':\n",
    "\n",
    "            self.kernel = nn.Parameter(torch.Tensor(\n",
    "                embed_size, num_pairs, embed_size))\n",
    "\n",
    "        elif self.kernel_type == 'vec':\n",
    "            self.kernel = nn.Parameter(torch.Tensor(num_pairs, embed_size))\n",
    "\n",
    "        elif self.kernel_type == 'num':\n",
    "            self.kernel = nn.Parameter(torch.Tensor(num_pairs, 1))\n",
    "        nn.init.xavier_uniform_(self.kernel)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embed_list = inputs\n",
    "        row = []\n",
    "        col = []\n",
    "        num_inputs = len(embed_list)\n",
    "        for i in range(num_inputs - 1):\n",
    "            for j in range(i + 1, num_inputs):\n",
    "                row.append(i)\n",
    "                col.append(j)\n",
    "        p = torch.cat([embed_list[idx]\n",
    "                       for idx in row], dim=1)  # batch num_pairs k\n",
    "        q = torch.cat([embed_list[idx] for idx in col], dim=1)\n",
    "\n",
    "        # -------------------------\n",
    "        if self.kernel_type == 'mat':\n",
    "            p.unsqueeze_(dim=1)\n",
    "            # k     k* pair* k\n",
    "            # batch * pair\n",
    "            kp = torch.sum(\n",
    "\n",
    "                # batch * pair * k\n",
    "\n",
    "                torch.mul(\n",
    "\n",
    "                    # batch * pair * k\n",
    "\n",
    "                    torch.transpose(\n",
    "\n",
    "                        # batch * k * pair\n",
    "\n",
    "                        torch.sum(\n",
    "\n",
    "                            # batch * k * pair * k\n",
    "\n",
    "                            torch.mul(\n",
    "\n",
    "                                p, self.kernel),\n",
    "\n",
    "                            dim=-1),\n",
    "\n",
    "                        2, 1),\n",
    "\n",
    "                    q),\n",
    "\n",
    "                dim=-1)\n",
    "        else:\n",
    "            # 1 * pair * (k or 1)\n",
    "\n",
    "            k = torch.unsqueeze(self.kernel, 0)\n",
    "\n",
    "            # batch * pair\n",
    "\n",
    "            kp = torch.sum(p * q * k, dim=-1)\n",
    "\n",
    "            # p q # b * p * k\n",
    "\n",
    "        return kp\n",
    "\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    \"\"\"Conv Layer used in CCPM.\n",
    "\n",
    "      Input shape\n",
    "            - A list of N 3D tensor with shape: ``(batch_size,1,filed_size,embedding_size)``.\n",
    "      Output shape\n",
    "            - A list of N 3D tensor with shape: ``(batch_size,last_filters,pooling_size,embedding_size)``.\n",
    "      Arguments\n",
    "            - **filed_size** : Positive integer, number of feature groups.\n",
    "            - **conv_kernel_width**: list. list of positive integer or empty list,the width of filter in each conv layer.\n",
    "            - **conv_filters**: list. list of positive integer or empty list,the number of filters in each conv layer.\n",
    "      Reference:\n",
    "            - Liu Q, Yu F, Wu S, et al. A convolutional click prediction model[C]//Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, 2015: 1743-1746.(http://ir.ia.ac.cn/bitstream/173211/12337/1/A%20Convolutional%20Click%20Prediction%20Model.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field_size, conv_kernel_width, conv_filters, device='cpu'):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.device = device\n",
    "        module_list = []\n",
    "        n = int(field_size)\n",
    "        l = len(conv_filters)\n",
    "        filed_shape = n\n",
    "        for i in range(1, l + 1):\n",
    "            if i == 1:\n",
    "                in_channels = 1\n",
    "            else:\n",
    "                in_channels = conv_filters[i - 2]\n",
    "            out_channels = conv_filters[i - 1]\n",
    "            width = conv_kernel_width[i - 1]\n",
    "            k = max(1, int((1 - pow(i / l, l - i)) * n)) if i < l else 3\n",
    "            module_list.append(Conv2dSame(in_channels=in_channels, out_channels=out_channels, kernel_size=(width, 1),\n",
    "                                          stride=1).to(self.device))\n",
    "            module_list.append(torch.nn.Tanh().to(self.device))\n",
    "\n",
    "            # KMaxPooling, extract top_k, returns tensors values\n",
    "            module_list.append(KMaxPooling(k=min(k, filed_shape), axis=2, device=self.device).to(self.device))\n",
    "            filed_shape = min(k, filed_shape)\n",
    "        self.conv_layer = nn.Sequential(*module_list)\n",
    "        self.to(device)\n",
    "        self.filed_shape = filed_shape\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.conv_layer(inputs)\n",
    "\n",
    "# DeepFM\n",
    "# -----------------------------------------------------------------------------------------------------------------\n",
    "class DeepFM(BaseModel):\n",
    "    \"\"\"Instantiates the DeepFM Network architecture.\n",
    "\n",
    "    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.\n",
    "    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n",
    "    :param use_fm: bool,use FM part or not\n",
    "    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN\n",
    "    :param l2_reg_linear: float. L2 regularizer strength applied to linear part\n",
    "    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n",
    "    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n",
    "    :param init_std: float,to use as the initialize std of embedding vector\n",
    "    :param seed: integer ,to use as random seed.\n",
    "    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n",
    "    :param dnn_activation: Activation function to use in DNN\n",
    "    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN\n",
    "    :param task: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n",
    "    :param device: str, ``\"cpu\"`` or ``\"cuda:0\"``\n",
    "    :return: A PyTorch model instance.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 linear_feature_columns, dnn_feature_columns, use_fm=True,\n",
    "                 dnn_hidden_units=(256, 128),\n",
    "                 l2_reg_linear=0.00001, l2_reg_embedding=0.00001, l2_reg_dnn=0, init_std=0.0001, seed=1024,\n",
    "                 dnn_dropout=0,\n",
    "                 dnn_activation='relu', dnn_use_bn=False, task='binary', device='cpu'):\n",
    "\n",
    "        super(DeepFM, self).__init__(linear_feature_columns, dnn_feature_columns, l2_reg_linear=l2_reg_linear,\n",
    "                                     l2_reg_embedding=l2_reg_embedding, init_std=init_std, seed=seed, task=task,\n",
    "                                     device=device)\n",
    "\n",
    "        self.use_fm = use_fm\n",
    "        self.use_dnn = len(dnn_feature_columns) > 0 and len(\n",
    "            dnn_hidden_units) > 0\n",
    "        if use_fm:\n",
    "            self.fm = FM()\n",
    "\n",
    "        if self.use_dnn:\n",
    "            self.dnn = DNN(self.compute_input_dim(dnn_feature_columns), dnn_hidden_units,\n",
    "                           activation=dnn_activation, l2_reg=l2_reg_dnn, dropout_rate=dnn_dropout, use_bn=dnn_use_bn,\n",
    "                           init_std=init_std, device=device)\n",
    "            self.dnn_linear = nn.Linear(\n",
    "                dnn_hidden_units[-1], 1, bias=False).to(device)\n",
    "\n",
    "            self.add_regularization_weight(\n",
    "                filter(lambda x: 'weight' in x[0] and 'bn' not in x[0], self.dnn.named_parameters()), l2_reg_dnn)\n",
    "            self.add_regularization_weight(self.dnn_linear.weight, l2_reg_dnn)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        sparse_embedding_list, dense_value_list = self.input_from_feature_columns(X, self.dnn_feature_columns,\n",
    "                                                                                  self.embedding_dict)\n",
    "        logit = self.linear_model(X)\n",
    "\n",
    "        if self.use_fm and len(sparse_embedding_list) > 0:\n",
    "            fm_input = torch.cat(sparse_embedding_list, dim=1)\n",
    "            logit += self.fm(fm_input)\n",
    "\n",
    "        if self.use_dnn:\n",
    "            dnn_input = combined_dnn_input(\n",
    "                sparse_embedding_list, dense_value_list)\n",
    "            dnn_output = self.dnn(dnn_input)\n",
    "            dnn_logit = self.dnn_linear(dnn_output)\n",
    "            logit += dnn_logit\n",
    "\n",
    "        y_pred = self.out(logit)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# xDeepFm\n",
    "class xDeepFM(BaseModel):\n",
    "    \"\"\"Instantiates the xDeepFM architecture.\n",
    "\n",
    "    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.\n",
    "    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n",
    "    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net\n",
    "    :param cin_layer_size: list,list of positive integer or empty list, the feature maps  in each hidden layer of Compressed Interaction Network\n",
    "    :param cin_split_half: bool.if set to True, half of the feature maps in each hidden will connect to output unit\n",
    "    :param cin_activation: activation function used on feature maps\n",
    "    :param l2_reg_linear: float. L2 regularizer strength applied to linear part\n",
    "    :param l2_reg_embedding: L2 regularizer strength applied to embedding vector\n",
    "    :param l2_reg_dnn: L2 regularizer strength applied to deep net\n",
    "    :param l2_reg_cin: L2 regularizer strength applied to CIN.\n",
    "    :param init_std: float,to use as the initialize std of embedding vector\n",
    "    :param seed: integer ,to use as random seed.\n",
    "    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n",
    "    :param dnn_activation: Activation function to use in DNN\n",
    "    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN\n",
    "    :param task: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n",
    "    :param device: str, ``\"cpu\"`` or ``\"cuda:0\"``\n",
    "    :return: A PyTorch model instance.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(256, 256),\n",
    "                 cin_layer_size=(256, 128,), cin_split_half=True, cin_activation='relu', l2_reg_linear=0.00001,\n",
    "                 l2_reg_embedding=0.00001, l2_reg_dnn=0, l2_reg_cin=0, init_std=0.0001, seed=1024, dnn_dropout=0,\n",
    "                 dnn_activation='relu', dnn_use_bn=False, task='binary', device='cpu'):\n",
    "\n",
    "        super(xDeepFM, self).__init__(linear_feature_columns, dnn_feature_columns, l2_reg_linear=l2_reg_linear,\n",
    "                                      l2_reg_embedding=l2_reg_embedding, init_std=init_std, seed=seed, task=task,\n",
    "                                      device=device)\n",
    "        self.dnn_hidden_units = dnn_hidden_units\n",
    "        self.use_dnn = len(dnn_feature_columns) > 0 and len(dnn_hidden_units) > 0\n",
    "        if self.use_dnn:\n",
    "            self.dnn = DNN(self.compute_input_dim(dnn_feature_columns), dnn_hidden_units,\n",
    "                           activation=dnn_activation, l2_reg=l2_reg_dnn, dropout_rate=dnn_dropout, use_bn=dnn_use_bn,\n",
    "                           init_std=init_std, device=device)\n",
    "            self.dnn_linear = nn.Linear(dnn_hidden_units[-1], 1, bias=False).to(device)\n",
    "            self.add_regularization_weight(\n",
    "                filter(lambda x: 'weight' in x[0] and 'bn' not in x[0], self.dnn.named_parameters()), l2_reg_dnn)\n",
    "\n",
    "            self.add_regularization_weight(self.dnn_linear.weight, l2_reg_dnn)\n",
    "\n",
    "        self.cin_layer_size = cin_layer_size\n",
    "        self.use_cin = len(self.cin_layer_size) > 0 and len(dnn_feature_columns) > 0\n",
    "        if self.use_cin:\n",
    "            field_num = len(self.embedding_dict)\n",
    "            if cin_split_half == True:\n",
    "                self.featuremap_num = sum(\n",
    "                    cin_layer_size[:-1]) // 2 + cin_layer_size[-1]\n",
    "            else:\n",
    "                self.featuremap_num = sum(cin_layer_size)\n",
    "            self.cin = CIN(field_num, cin_layer_size,\n",
    "                           cin_activation, cin_split_half, l2_reg_cin, seed, device=device)\n",
    "            self.cin_linear = nn.Linear(self.featuremap_num, 1, bias=False).to(device)\n",
    "            self.add_regularization_weight(\n",
    "                filter(lambda x: 'weight' in x[0], self.cin.named_parameters()), l2_reg_cin)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        sparse_embedding_list, dense_value_list = self.input_from_feature_columns(X, self.dnn_feature_columns,\n",
    "                                                                                  self.embedding_dict)\n",
    "\n",
    "        linear_logit = self.linear_model(X)\n",
    "        if self.use_cin:\n",
    "            cin_input = torch.cat(sparse_embedding_list, dim=1)\n",
    "            cin_output = self.cin(cin_input)\n",
    "            cin_logit = self.cin_linear(cin_output)\n",
    "        if self.use_dnn:\n",
    "            dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)\n",
    "            dnn_output = self.dnn(dnn_input)\n",
    "            dnn_logit = self.dnn_linear(dnn_output)\n",
    "\n",
    "        if len(self.dnn_hidden_units) == 0 and len(self.cin_layer_size) == 0:  # only linear\n",
    "            final_logit = linear_logit\n",
    "        elif len(self.dnn_hidden_units) == 0 and len(self.cin_layer_size) > 0:  # linear + CIN\n",
    "            final_logit = linear_logit + cin_logit\n",
    "        elif len(self.dnn_hidden_units) > 0 and len(self.cin_layer_size) == 0:  # linear +　Deep\n",
    "            final_logit = linear_logit + dnn_logit\n",
    "        elif len(self.dnn_hidden_units) > 0 and len(self.cin_layer_size) > 0:  # linear + CIN + Deep\n",
    "            final_logit = linear_logit + dnn_logit + cin_logit\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        y_pred = self.out(final_logit)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "#DCN-M\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "class DCNMix(BaseModel):\n",
    "    \"\"\"Instantiates the DCN-Mix model.\n",
    "\n",
    "    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.\n",
    "    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n",
    "    :param cross_num: positive integet,cross layer number\n",
    "    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN\n",
    "    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n",
    "    :param l2_reg_cross: float. L2 regularizer strength applied to cross net\n",
    "    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n",
    "    :param init_std: float,to use as the initialize std of embedding vector\n",
    "    :param seed: integer ,to use as random seed.\n",
    "    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n",
    "    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not DNN\n",
    "    :param dnn_activation: Activation function to use in DNN\n",
    "    :param low_rank: Positive integer, dimensionality of low-rank sapce.\n",
    "    :param num_experts: Positive integer, number of experts.\n",
    "    :param task: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n",
    "    :param device: str, ``\"cpu\"`` or ``\"cuda:0\"``\n",
    "    :return: A PyTorch model instance.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, linear_feature_columns,\n",
    "                 dnn_feature_columns, cross_num=2,\n",
    "                 dnn_hidden_units=(128, 128), l2_reg_linear=0.0001,\n",
    "                 l2_reg_embedding=0.0001, l2_reg_cross=0.0001, l2_reg_dnn=0.001, init_std=0.0001, seed=1024,\n",
    "                 dnn_dropout=0.5, low_rank=32, num_experts=4,\n",
    "                 dnn_activation='relu', dnn_use_bn=True, task='binary', device='cpu'):\n",
    "\n",
    "        super(DCNMix, self).__init__(linear_feature_columns=linear_feature_columns,\n",
    "                                     dnn_feature_columns=dnn_feature_columns, l2_reg_embedding=l2_reg_embedding,\n",
    "                                     init_std=init_std, seed=seed, task=task, device=device)\n",
    "        self.dnn_hidden_units = dnn_hidden_units\n",
    "        self.cross_num = cross_num\n",
    "        self.dnn = DNN(self.compute_input_dim(dnn_feature_columns), dnn_hidden_units,\n",
    "                       activation=dnn_activation, use_bn=dnn_use_bn, l2_reg=l2_reg_dnn, dropout_rate=dnn_dropout,\n",
    "                       init_std=init_std, device=device)\n",
    "        if len(self.dnn_hidden_units) > 0 and self.cross_num > 0:\n",
    "            dnn_linear_in_feature = self.compute_input_dim(dnn_feature_columns) + dnn_hidden_units[-1]\n",
    "        elif len(self.dnn_hidden_units) > 0:\n",
    "            dnn_linear_in_feature = dnn_hidden_units[-1]\n",
    "        elif self.cross_num > 0:\n",
    "            dnn_linear_in_feature = self.compute_input_dim(dnn_feature_columns)\n",
    "\n",
    "        self.dnn_linear = nn.Linear(dnn_linear_in_feature, 1, bias=False).to(\n",
    "            device)\n",
    "        self.crossnet = CrossNetMix(in_features=self.compute_input_dim(dnn_feature_columns),\n",
    "                                    low_rank=low_rank, num_experts=num_experts,\n",
    "                                    layer_num=cross_num, device=device)\n",
    "        self.add_regularization_weight(\n",
    "            filter(lambda x: 'weight' in x[0] and 'bn' not in x[0], self.dnn.named_parameters()), l2_reg_dnn)\n",
    "        self.add_regularization_weight(self.dnn_linear.weight, l2_reg_linear)\n",
    "        self.add_regularization_weight(self.crossnet.U_list, l2_reg_cross)\n",
    "        self.add_regularization_weight(self.crossnet.V_list, l2_reg_cross)\n",
    "        self.add_regularization_weight(self.crossnet.C_list, l2_reg_cross)\n",
    "        \n",
    "        #-----\n",
    "        #afm\n",
    "        self.fm2 = AFMLayer(self.embedding_size, attention_factor=8, l2_reg_w=1e-5, dropout_rate=0.5,\n",
    "                               seed=1024, device=device)\n",
    "        self.add_regularization_weight(self.fm2.attention_W, weight_decay=1e-5)\n",
    "        \n",
    "        #---------\n",
    "        field_num = len(self.embedding_dict)\n",
    "        att_layer_num=3\n",
    "        att_embedding_size=8\n",
    "        att_head_num=2\n",
    "        att_res=True\n",
    "        dnn_linear_in_feature2 = dnn_hidden_units[-1] + \\\n",
    "                                    field_num * att_embedding_size * att_head_num\n",
    "        self.dnn_linear2 = nn.Linear(dnn_linear_in_feature2, 1, bias=False).to(device)\n",
    "        self.int_layers = nn.ModuleList(\n",
    "            [InteractingLayer(self.embedding_size if i == 0 else att_embedding_size * att_head_num,\n",
    "                              att_embedding_size, att_head_num, att_res, device=device) for i in range(att_layer_num)])\n",
    "\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        logit = self.linear_model(X)\n",
    "        sparse_embedding_list, dense_value_list = self.input_from_feature_columns(X, self.dnn_feature_columns,\n",
    "                                                                                  self.embedding_dict)\n",
    "\n",
    "        dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)\n",
    "        \n",
    "        att_input = concat_fun(sparse_embedding_list, axis=1)\n",
    "        for layer in self.int_layers:\n",
    "            att_input = layer(att_input)\n",
    "        att_output = torch.flatten(att_input, start_dim=1)\n",
    "    \n",
    "        \n",
    "        if len(sparse_embedding_list) > 0:\n",
    "            afm_output = self.fm2(sparse_embedding_list)\n",
    "\n",
    "        if len(self.dnn_hidden_units) > 0 and self.cross_num > 0:  # Deep & Cross\n",
    "            \n",
    "            #deep_out = self.dnn(dnn_input)\n",
    "            #cross_out = self.crossnet(dnn_input)\n",
    "            #stack_out = torch.cat((cross_out, deep_out), dim=-1)\n",
    "            #logit += self.dnn_linear(stack_out)\n",
    "            \n",
    "            #stack_out = concat_fun([att_output, deep_out])\n",
    "            #logit += self.dnn_linear2(stack_out)\n",
    "            logit += afm_output\n",
    "        elif len(self.dnn_hidden_units) > 0:  # Only Deep\n",
    "            deep_out = self.dnn(dnn_input)\n",
    "            logit += self.dnn_linear(deep_out)\n",
    "        elif self.cross_num > 0:  # Only Cross\n",
    "            cross_out = self.crossnet(dnn_input)\n",
    "            logit += self.dnn_linear(cross_out)\n",
    "        else:  # Error\n",
    "            pass\n",
    "        y_pred = self.out(logit)\n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'riiid_data/'\n",
    "file_train = 'train.csv'\n",
    "file_questions = 'questions.csv'\n",
    "file_lecture = 'lectures.csv'\n",
    "nrows =  200*100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 265.168936,
     "end_time": "2020-10-30T18:27:21.467302",
     "exception": false,
     "start_time": "2020-10-30T18:22:56.298366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv( dir_path + file_train,\n",
    "                   usecols=[1, 2, 3, 4, 5, 7, 8, 9],\n",
    "                    nrows =  200*100000,\n",
    "                   dtype={'timestamp': 'int64',\n",
    "                          'user_id': 'int32',\n",
    "                          'content_id': 'int16',\n",
    "                          'content_type_id': 'int8',\n",
    "                          'task_container_id': 'int16',\n",
    "                          'answered_correctly':'int8',\n",
    "                          'prior_question_elapsed_time': 'float32',\n",
    "                          'prior_question_had_explanation': 'boolean'}\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.063739,
     "end_time": "2020-10-30T18:27:21.579272",
     "exception": false,
     "start_time": "2020-10-30T18:27:21.515533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reading in question df\n",
    "questions_df = pd.read_csv( dir_path + file_questions,  \n",
    "                            usecols=[0, 3],\n",
    "                            nrows =  200*100000,\n",
    "                            dtype={'question_id': 'int16',\n",
    "                              'part': 'int8'}\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.059379,
     "end_time": "2020-10-30T18:27:21.686115",
     "exception": false,
     "start_time": "2020-10-30T18:27:21.626736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reading in lecture df\n",
    "lectures_df = pd.read_csv(dir_path + file_lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.078297,
     "end_time": "2020-10-30T18:27:21.812238",
     "exception": false,
     "start_time": "2020-10-30T18:27:21.733941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lectures_df['type_of'] = lectures_df['type_of'].replace('solving question', 'solving_question')\n",
    "\n",
    "lectures_df = pd.get_dummies(lectures_df, columns=['part', 'type_of'])\n",
    "\n",
    "part_lectures_columns = [column for column in lectures_df.columns if column.startswith('part')]\n",
    "\n",
    "types_of_lectures_columns = [column for column in lectures_df.columns if column.startswith('type_of_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 0.06796,
     "end_time": "2020-10-30T18:27:21.927852",
     "exception": false,
     "start_time": "2020-10-30T18:27:21.859892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lecture_id</th>\n",
       "      <th>tag</th>\n",
       "      <th>part_1</th>\n",
       "      <th>part_2</th>\n",
       "      <th>part_3</th>\n",
       "      <th>part_4</th>\n",
       "      <th>part_5</th>\n",
       "      <th>part_6</th>\n",
       "      <th>part_7</th>\n",
       "      <th>type_of_concept</th>\n",
       "      <th>type_of_intention</th>\n",
       "      <th>type_of_solving_question</th>\n",
       "      <th>type_of_starter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>185</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>317</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lecture_id  tag  part_1  part_2  part_3  part_4  part_5  part_6  part_7  \\\n",
       "0          89  159       0       0       0       0       1       0       0   \n",
       "1         100   70       1       0       0       0       0       0       0   \n",
       "2         185   45       0       0       0       0       0       1       0   \n",
       "3         192   79       0       0       0       0       1       0       0   \n",
       "4         317  156       0       0       0       0       1       0       0   \n",
       "\n",
       "   type_of_concept  type_of_intention  type_of_solving_question  \\\n",
       "0                1                  0                         0   \n",
       "1                1                  0                         0   \n",
       "2                1                  0                         0   \n",
       "3                0                  0                         1   \n",
       "4                0                  0                         1   \n",
       "\n",
       "   type_of_starter  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lectures_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 0.831637,
     "end_time": "2020-10-30T18:27:22.808805",
     "exception": false,
     "start_time": "2020-10-30T18:27:21.977168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge lecture features to train dataset\n",
    "train_lectures = train[train.content_type_id == True].merge(lectures_df, left_on='content_id', right_on='lecture_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 0.079568,
     "end_time": "2020-10-30T18:27:22.937772",
     "exception": false,
     "start_time": "2020-10-30T18:27:22.858204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>content_type_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "      <th>lecture_id</th>\n",
       "      <th>tag</th>\n",
       "      <th>...</th>\n",
       "      <th>part_2</th>\n",
       "      <th>part_3</th>\n",
       "      <th>part_4</th>\n",
       "      <th>part_5</th>\n",
       "      <th>part_6</th>\n",
       "      <th>part_7</th>\n",
       "      <th>type_of_concept</th>\n",
       "      <th>type_of_intention</th>\n",
       "      <th>type_of_solving_question</th>\n",
       "      <th>type_of_starter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>653762</td>\n",
       "      <td>2746</td>\n",
       "      <td>6808</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>6808</td>\n",
       "      <td>129</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10183847</td>\n",
       "      <td>5382</td>\n",
       "      <td>16736</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>16736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1424348597</td>\n",
       "      <td>5382</td>\n",
       "      <td>30207</td>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>30207</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1425557777</td>\n",
       "      <td>5382</td>\n",
       "      <td>18545</td>\n",
       "      <td>1</td>\n",
       "      <td>121</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>18545</td>\n",
       "      <td>58</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>405813029</td>\n",
       "      <td>8623</td>\n",
       "      <td>10540</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>10540</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  user_id  content_id  content_type_id  task_container_id  \\\n",
       "0      653762     2746        6808                1                 14   \n",
       "1    10183847     5382       16736                1                 21   \n",
       "2  1424348597     5382       30207                1                104   \n",
       "3  1425557777     5382       18545                1                121   \n",
       "4   405813029     8623       10540                1                 59   \n",
       "\n",
       "   answered_correctly  prior_question_elapsed_time  \\\n",
       "0                  -1                          NaN   \n",
       "1                  -1                          NaN   \n",
       "2                  -1                          NaN   \n",
       "3                  -1                          NaN   \n",
       "4                  -1                          NaN   \n",
       "\n",
       "   prior_question_had_explanation  lecture_id  tag  ...  part_2  part_3  \\\n",
       "0                           False        6808  129  ...       1       0   \n",
       "1                           False       16736   40  ...       0       0   \n",
       "2                           False       30207   43  ...       0       0   \n",
       "3                           False       18545   58  ...       0       0   \n",
       "4                           False       10540   99  ...       0       0   \n",
       "\n",
       "   part_4  part_5  part_6  part_7  type_of_concept  type_of_intention  \\\n",
       "0       0       0       0       0                0                  1   \n",
       "1       0       0       0       0                1                  0   \n",
       "2       0       1       0       0                1                  0   \n",
       "3       0       1       0       0                1                  0   \n",
       "4       0       0       0       0                1                  0   \n",
       "\n",
       "   type_of_solving_question  type_of_starter  \n",
       "0                         0                0  \n",
       "1                         0                0  \n",
       "2                         0                0  \n",
       "3                         0                0  \n",
       "4                         0                0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lectures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 0.262435,
     "end_time": "2020-10-30T18:27:23.253304",
     "exception": false,
     "start_time": "2020-10-30T18:27:22.990869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# collect per user stats\n",
    "user_lecture_stats_part = train_lectures.groupby('user_id')[part_lectures_columns + types_of_lectures_columns].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "papermill": {
     "duration": 0.065649,
     "end_time": "2020-10-30T18:27:23.370185",
     "exception": false,
     "start_time": "2020-10-30T18:27:23.304536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>part_1</th>\n",
       "      <th>part_2</th>\n",
       "      <th>part_3</th>\n",
       "      <th>part_4</th>\n",
       "      <th>part_5</th>\n",
       "      <th>part_6</th>\n",
       "      <th>part_7</th>\n",
       "      <th>type_of_concept</th>\n",
       "      <th>type_of_intention</th>\n",
       "      <th>type_of_solving_question</th>\n",
       "      <th>type_of_starter</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5382</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8623</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12741</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13134</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         part_1  part_2  part_3  part_4  part_5  part_6  part_7  \\\n",
       "user_id                                                           \n",
       "2746          0       1       0       0       0       0       0   \n",
       "5382          1       0       0       0       2       0       0   \n",
       "8623          2       1       0       0       0       0       0   \n",
       "12741         0       0       0       3       0       1       2   \n",
       "13134         1       3       0       0       3       0       0   \n",
       "\n",
       "         type_of_concept  type_of_intention  type_of_solving_question  \\\n",
       "user_id                                                                 \n",
       "2746                   0                  1                         0   \n",
       "5382                   3                  0                         0   \n",
       "8623                   3                  0                         0   \n",
       "12741                  4                  0                         2   \n",
       "13134                  6                  1                         0   \n",
       "\n",
       "         type_of_starter  \n",
       "user_id                   \n",
       "2746                   0  \n",
       "5382                   0  \n",
       "8623                   0  \n",
       "12741                  0  \n",
       "13134                  0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_lecture_stats_part.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 0.073265,
     "end_time": "2020-10-30T18:27:23.494840",
     "exception": false,
     "start_time": "2020-10-30T18:27:23.421575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add boolean features\n",
    "for column in user_lecture_stats_part.columns:\n",
    "    bool_column = column + '_boolean'\n",
    "    user_lecture_stats_part[bool_column] = (user_lecture_stats_part[column] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 0.074029,
     "end_time": "2020-10-30T18:27:23.619515",
     "exception": false,
     "start_time": "2020-10-30T18:27:23.545486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>part_1</th>\n",
       "      <th>part_2</th>\n",
       "      <th>part_3</th>\n",
       "      <th>part_4</th>\n",
       "      <th>part_5</th>\n",
       "      <th>part_6</th>\n",
       "      <th>part_7</th>\n",
       "      <th>type_of_concept</th>\n",
       "      <th>type_of_intention</th>\n",
       "      <th>type_of_solving_question</th>\n",
       "      <th>...</th>\n",
       "      <th>part_2_boolean</th>\n",
       "      <th>part_3_boolean</th>\n",
       "      <th>part_4_boolean</th>\n",
       "      <th>part_5_boolean</th>\n",
       "      <th>part_6_boolean</th>\n",
       "      <th>part_7_boolean</th>\n",
       "      <th>type_of_concept_boolean</th>\n",
       "      <th>type_of_intention_boolean</th>\n",
       "      <th>type_of_solving_question_boolean</th>\n",
       "      <th>type_of_starter_boolean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5382</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8623</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12741</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13134</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         part_1  part_2  part_3  part_4  part_5  part_6  part_7  \\\n",
       "user_id                                                           \n",
       "2746          0       1       0       0       0       0       0   \n",
       "5382          1       0       0       0       2       0       0   \n",
       "8623          2       1       0       0       0       0       0   \n",
       "12741         0       0       0       3       0       1       2   \n",
       "13134         1       3       0       0       3       0       0   \n",
       "\n",
       "         type_of_concept  type_of_intention  type_of_solving_question  ...  \\\n",
       "user_id                                                                ...   \n",
       "2746                   0                  1                         0  ...   \n",
       "5382                   3                  0                         0  ...   \n",
       "8623                   3                  0                         0  ...   \n",
       "12741                  4                  0                         2  ...   \n",
       "13134                  6                  1                         0  ...   \n",
       "\n",
       "         part_2_boolean  part_3_boolean  part_4_boolean  part_5_boolean  \\\n",
       "user_id                                                                   \n",
       "2746                  1               0               0               0   \n",
       "5382                  0               0               0               1   \n",
       "8623                  1               0               0               0   \n",
       "12741                 0               0               1               0   \n",
       "13134                 1               0               0               1   \n",
       "\n",
       "         part_6_boolean  part_7_boolean  type_of_concept_boolean  \\\n",
       "user_id                                                            \n",
       "2746                  0               0                        0   \n",
       "5382                  0               0                        1   \n",
       "8623                  0               0                        1   \n",
       "12741                 1               1                        1   \n",
       "13134                 0               0                        1   \n",
       "\n",
       "         type_of_intention_boolean  type_of_solving_question_boolean  \\\n",
       "user_id                                                                \n",
       "2746                             1                                 0   \n",
       "5382                             0                                 0   \n",
       "8623                             0                                 0   \n",
       "12741                            0                                 1   \n",
       "13134                            1                                 0   \n",
       "\n",
       "         type_of_starter_boolean  \n",
       "user_id                           \n",
       "2746                           0  \n",
       "5382                           0  \n",
       "8623                           0  \n",
       "12741                          0  \n",
       "13134                          0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_lecture_stats_part.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": 0.05761,
     "end_time": "2020-10-30T18:27:23.728836",
     "exception": false,
     "start_time": "2020-10-30T18:27:23.671226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#clearing memory\n",
    "del(train_lectures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.050874,
     "end_time": "2020-10-30T18:27:23.830884",
     "exception": false,
     "start_time": "2020-10-30T18:27:23.780010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Affirmatives (True) for content_type_id are only for those with a different type of content (lectures). These are not real questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "papermill": {
     "duration": 31.315981,
     "end_time": "2020-10-30T18:27:55.198084",
     "exception": false,
     "start_time": "2020-10-30T18:27:23.882103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#removing True or 1 for content_type_id\n",
    "\n",
    "train = train[train.content_type_id == False].sort_values('timestamp').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "papermill": {
     "duration": 0.255707,
     "end_time": "2020-10-30T18:27:55.506963",
     "exception": false,
     "start_time": "2020-10-30T18:27:55.251256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>content_type_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19072048</th>\n",
       "      <td>40777237106</td>\n",
       "      <td>248574621</td>\n",
       "      <td>12046</td>\n",
       "      <td>0</td>\n",
       "      <td>9999</td>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19247225</th>\n",
       "      <td>46403605560</td>\n",
       "      <td>362401676</td>\n",
       "      <td>3665</td>\n",
       "      <td>0</td>\n",
       "      <td>9999</td>\n",
       "      <td>1</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19337970</th>\n",
       "      <td>50520797214</td>\n",
       "      <td>112794624</td>\n",
       "      <td>554</td>\n",
       "      <td>0</td>\n",
       "      <td>9999</td>\n",
       "      <td>0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19451831</th>\n",
       "      <td>57418138099</td>\n",
       "      <td>221811246</td>\n",
       "      <td>10601</td>\n",
       "      <td>0</td>\n",
       "      <td>9999</td>\n",
       "      <td>1</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19551537</th>\n",
       "      <td>65805246899</td>\n",
       "      <td>270648383</td>\n",
       "      <td>6317</td>\n",
       "      <td>0</td>\n",
       "      <td>9999</td>\n",
       "      <td>1</td>\n",
       "      <td>37000.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp    user_id  content_id  content_type_id  \\\n",
       "19072048  40777237106  248574621       12046                0   \n",
       "19247225  46403605560  362401676        3665                0   \n",
       "19337970  50520797214  112794624         554                0   \n",
       "19451831  57418138099  221811246       10601                0   \n",
       "19551537  65805246899  270648383        6317                0   \n",
       "\n",
       "          task_container_id  answered_correctly  prior_question_elapsed_time  \\\n",
       "19072048               9999                   1                      20000.0   \n",
       "19247225               9999                   1                      15000.0   \n",
       "19337970               9999                   0                      17000.0   \n",
       "19451831               9999                   1                      15000.0   \n",
       "19551537               9999                   1                      37000.0   \n",
       "\n",
       "          prior_question_had_explanation  \n",
       "19072048                            True  \n",
       "19247225                            True  \n",
       "19337970                            True  \n",
       "19451831                            True  \n",
       "19551537                            True  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[(train.task_container_id == 9999)].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "papermill": {
     "duration": 5.37707,
     "end_time": "2020-10-30T18:28:00.936721",
     "exception": false,
     "start_time": "2020-10-30T18:27:55.559651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[(train.content_type_id == False)].task_container_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "papermill": {
     "duration": 0.199683,
     "end_time": "2020-10-30T18:28:01.191474",
     "exception": false,
     "start_time": "2020-10-30T18:28:00.991791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#saving value to fillna\n",
    "elapsed_mean = train.prior_question_elapsed_time.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "papermill": {
     "duration": 75.482206,
     "end_time": "2020-10-30T18:29:16.727348",
     "exception": false,
     "start_time": "2020-10-30T18:28:01.245142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "group1 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['count'])\n",
    "group1.columns = ['avg_questions']\n",
    "group2 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['nunique'])\n",
    "group2.columns = ['avg_questions']\n",
    "group3 = group1 / group2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "papermill": {
     "duration": 0.067807,
     "end_time": "2020-10-30T18:29:16.871165",
     "exception": false,
     "start_time": "2020-10-30T18:29:16.803358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "group3['avg_questions_seen'] = group3.avg_questions.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "papermill": {
     "duration": 0.09889,
     "end_time": "2020-10-30T18:29:17.040119",
     "exception": false,
     "start_time": "2020-10-30T18:29:16.941229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0062925257072697"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group3.iloc[0].avg_questions_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "papermill": {
     "duration": 34.77285,
     "end_time": "2020-10-30T18:29:51.888845",
     "exception": false,
     "start_time": "2020-10-30T18:29:17.115995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_u_final = train.loc[train.content_type_id == False, ['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\n",
    "results_u_final.columns = ['answered_correctly_user']\n",
    "\n",
    "results_u2_final = train.loc[train.content_type_id == False, ['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\n",
    "results_u2_final.columns = ['explanation_mean_user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "papermill": {
     "duration": 0.117058,
     "end_time": "2020-10-30T18:29:52.088413",
     "exception": false,
     "start_time": "2020-10-30T18:29:51.971355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    78251.000000\n",
       "mean         0.566713\n",
       "std          0.360619\n",
       "min          0.000000\n",
       "25%          0.230769\n",
       "50%          0.655172\n",
       "75%          0.904762\n",
       "max          1.000000\n",
       "Name: explanation_mean_user, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_u2_final.explanation_mean_user.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "papermill": {
     "duration": 18.230328,
     "end_time": "2020-10-30T18:30:10.373990",
     "exception": false,
     "start_time": "2020-10-30T18:29:52.143662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "papermill": {
     "duration": 17.478391,
     "end_time": "2020-10-30T18:30:27.907861",
     "exception": false,
     "start_time": "2020-10-30T18:30:10.429470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_q_final = train.loc[train.content_type_id == False, ['question_id','answered_correctly']].groupby(['question_id']).agg(['mean'])\n",
    "results_q_final.columns = ['quest_pct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "papermill": {
     "duration": 15.787128,
     "end_time": "2020-10-30T18:30:43.757431",
     "exception": false,
     "start_time": "2020-10-30T18:30:27.970303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_q2_final = train.loc[train.content_type_id == False, ['question_id','part']].groupby(['question_id']).agg(['count'])\n",
    "results_q2_final.columns = ['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "papermill": {
     "duration": 0.104144,
     "end_time": "2020-10-30T18:30:43.920829",
     "exception": false,
     "start_time": "2020-10-30T18:30:43.816685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "question2 = pd.merge(questions_df, results_q_final, left_on = 'question_id', right_on = 'question_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "papermill": {
     "duration": 0.103089,
     "end_time": "2020-10-30T18:30:44.131450",
     "exception": false,
     "start_time": "2020-10-30T18:30:44.028361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "question2 = pd.merge(question2, results_q2_final, left_on = 'question_id', right_on = 'question_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "papermill": {
     "duration": 0.094251,
     "end_time": "2020-10-30T18:30:44.316358",
     "exception": false,
     "start_time": "2020-10-30T18:30:44.222107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "question2.quest_pct = round(question2.quest_pct,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "papermill": {
     "duration": 0.134043,
     "end_time": "2020-10-30T18:30:44.533042",
     "exception": false,
     "start_time": "2020-10-30T18:30:44.398999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>part</th>\n",
       "      <th>quest_pct</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90818</td>\n",
       "      <td>1394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.89434</td>\n",
       "      <td>1467.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.55624</td>\n",
       "      <td>8899.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.77569</td>\n",
       "      <td>4525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.62075</td>\n",
       "      <td>6294.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id  part  quest_pct   count\n",
       "0            0     1    0.90818  1394.0\n",
       "1            1     1    0.89434  1467.0\n",
       "2            2     1    0.55624  8899.0\n",
       "3            3     1    0.77569  4525.0\n",
       "4            4     1    0.62075  6294.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>part</th>\n",
       "      <th>quest_pct</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13518</th>\n",
       "      <td>13518</td>\n",
       "      <td>5</td>\n",
       "      <td>0.82022</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13519</th>\n",
       "      <td>13519</td>\n",
       "      <td>5</td>\n",
       "      <td>0.59043</td>\n",
       "      <td>188.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13520</th>\n",
       "      <td>13520</td>\n",
       "      <td>5</td>\n",
       "      <td>0.71429</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13521</th>\n",
       "      <td>13521</td>\n",
       "      <td>5</td>\n",
       "      <td>0.85882</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13522</th>\n",
       "      <td>13522</td>\n",
       "      <td>5</td>\n",
       "      <td>0.89535</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       question_id  part  quest_pct  count\n",
       "13518        13518     5    0.82022  178.0\n",
       "13519        13519     5    0.59043  188.0\n",
       "13520        13520     5    0.71429  154.0\n",
       "13521        13521     5    0.85882  170.0\n",
       "13522        13522     5    0.89535  172.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(question2.head(), question2.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "papermill": {
     "duration": 0.096861,
     "end_time": "2020-10-30T18:30:44.704696",
     "exception": false,
     "start_time": "2020-10-30T18:30:44.607835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>content_type_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "      <th>question_id</th>\n",
       "      <th>part</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>5692</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5692</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>62362253</td>\n",
       "      <td>4981</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>4981</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>345907239</td>\n",
       "      <td>5196</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5196</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>345892616</td>\n",
       "      <td>7900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>7900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>345891142</td>\n",
       "      <td>7900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>7900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp    user_id  content_id  content_type_id  task_container_id  \\\n",
       "0          0        115        5692                0                  1   \n",
       "1          0   62362253        4981                0                  0   \n",
       "2          0  345907239        5196                0                  0   \n",
       "3          0  345892616        7900                0                  0   \n",
       "4          0  345891142        7900                0                  0   \n",
       "\n",
       "   answered_correctly  prior_question_elapsed_time  \\\n",
       "0                   1                          NaN   \n",
       "1                   0                          NaN   \n",
       "2                   1                          NaN   \n",
       "3                   1                          NaN   \n",
       "4                   1                          NaN   \n",
       "\n",
       "   prior_question_had_explanation  question_id  part  \n",
       "0                            <NA>         5692     5  \n",
       "1                            <NA>         4981     5  \n",
       "2                            <NA>         5196     5  \n",
       "3                            <NA>         7900     1  \n",
       "4                            <NA>         7900     1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "papermill": {
     "duration": 0.092536,
     "end_time": "2020-10-30T18:30:44.870352",
     "exception": false,
     "start_time": "2020-10-30T18:30:44.777816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19610123"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.092857,
     "end_time": "2020-10-30T18:30:45.040685",
     "exception": false,
     "start_time": "2020-10-30T18:30:44.947828",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Exploration ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "papermill": {
     "duration": 0.067389,
     "end_time": "2020-10-30T18:30:45.172623",
     "exception": false,
     "start_time": "2020-10-30T18:30:45.105234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19610123"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "papermill": {
     "duration": 0.134113,
     "end_time": "2020-10-30T18:30:45.403968",
     "exception": false,
     "start_time": "2020-10-30T18:30:45.269855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6576615557179321"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.answered_correctly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "papermill": {
     "duration": 0.065947,
     "end_time": "2020-10-30T18:30:45.528816",
     "exception": false,
     "start_time": "2020-10-30T18:30:45.462869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prior_mean_user = results_u2_final.explanation_mean_user.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "papermill": {
     "duration": 0.197254,
     "end_time": "2020-10-30T18:30:45.785069",
     "exception": false,
     "start_time": "2020-10-30T18:30:45.587815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6796069669425684"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[(train.timestamp == 0)].answered_correctly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "papermill": {
     "duration": 4.394188,
     "end_time": "2020-10-30T18:30:50.238758",
     "exception": false,
     "start_time": "2020-10-30T18:30:45.844570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6575730475582565"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[(train.timestamp != 0)].answered_correctly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "papermill": {
     "duration": 0.863208,
     "end_time": "2020-10-30T18:30:51.162901",
     "exception": false,
     "start_time": "2020-10-30T18:30:50.299693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.drop(['timestamp', 'content_type_id', 'question_id', 'part'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "papermill": {
     "duration": 0.071352,
     "end_time": "2020-10-30T18:30:51.296928",
     "exception": false,
     "start_time": "2020-10-30T18:30:51.225576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19610123"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.059855,
     "end_time": "2020-10-30T18:30:51.419973",
     "exception": false,
     "start_time": "2020-10-30T18:30:51.360118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating Validation Set (Most Recent Answers by User) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "papermill": {
     "duration": 22.95362,
     "end_time": "2020-10-30T18:31:14.559520",
     "exception": false,
     "start_time": "2020-10-30T18:30:51.605900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19610123"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = train.groupby('user_id').tail(5)\n",
    "train = train[~train.index.isin(validation.index)]\n",
    "len(train) + len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "papermill": {
     "duration": 0.072255,
     "end_time": "2020-10-30T18:31:14.693765",
     "exception": false,
     "start_time": "2020-10-30T18:31:14.621510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.540148000460105"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.answered_correctly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "papermill": {
     "duration": 0.135666,
     "end_time": "2020-10-30T18:31:14.891218",
     "exception": false,
     "start_time": "2020-10-30T18:31:14.755552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6600536305184457"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.answered_correctly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "papermill": {
     "duration": 17.675045,
     "end_time": "2020-10-30T18:31:32.628820",
     "exception": false,
     "start_time": "2020-10-30T18:31:14.953775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_u_val = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\n",
    "results_u_val.columns = ['answered_correctly_user']\n",
    "\n",
    "results_u2_val = train[['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\n",
    "results_u2_val.columns = ['explanation_mean_user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.062246,
     "end_time": "2020-10-30T18:31:32.753560",
     "exception": false,
     "start_time": "2020-10-30T18:31:32.691314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Does it make sense to use last questions as validation? Why is the rate of correct answers so low?\n",
    "I am convinced there is a better way to match the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.062267,
     "end_time": "2020-10-30T18:31:32.878153",
     "exception": false,
     "start_time": "2020-10-30T18:31:32.815886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Extracting Training Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "papermill": {
     "duration": 24.129633,
     "end_time": "2020-10-30T18:31:57.069659",
     "exception": false,
     "start_time": "2020-10-30T18:31:32.940026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19610123"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train.groupby('user_id').tail(18)\n",
    "train = train[~train.index.isin(X.index)]\n",
    "len(X) + len(train) + len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "papermill": {
     "duration": 0.077582,
     "end_time": "2020-10-30T18:31:57.210279",
     "exception": false,
     "start_time": "2020-10-30T18:31:57.132697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.551874572610847"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.answered_correctly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "papermill": {
     "duration": 0.134154,
     "end_time": "2020-10-30T18:31:57.408393",
     "exception": false,
     "start_time": "2020-10-30T18:31:57.274239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6679874935928232"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.answered_correctly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "papermill": {
     "duration": 16.135509,
     "end_time": "2020-10-30T18:32:13.609055",
     "exception": false,
     "start_time": "2020-10-30T18:31:57.473546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_u_X = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\n",
    "results_u_X.columns = ['answered_correctly_user']\n",
    "\n",
    "results_u2_X = train[['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\n",
    "results_u2_X.columns = ['explanation_mean_user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.066644,
     "end_time": "2020-10-30T18:32:13.743591",
     "exception": false,
     "start_time": "2020-10-30T18:32:13.676947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Merging Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "papermill": {
     "duration": 0.182485,
     "end_time": "2020-10-30T18:32:13.993486",
     "exception": false,
     "start_time": "2020-10-30T18:32:13.811001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#clearing memory\n",
    "del(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "papermill": {
     "duration": 4.357752,
     "end_time": "2020-10-30T18:32:18.419534",
     "exception": false,
     "start_time": "2020-10-30T18:32:14.061782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = pd.merge(X, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\n",
    "X = pd.merge(X, results_u_X, on=['user_id'], how=\"left\")\n",
    "X = pd.merge(X, results_u2_X, on=['user_id'], how=\"left\")\n",
    "\n",
    "X = pd.merge(X, user_lecture_stats_part, on=['user_id'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "papermill": {
     "duration": 1.357739,
     "end_time": "2020-10-30T18:32:19.841886",
     "exception": false,
     "start_time": "2020-10-30T18:32:18.484147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation = pd.merge(validation, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\n",
    "validation = pd.merge(validation, results_u_val, on=['user_id'], how=\"left\")\n",
    "validation = pd.merge(validation, results_u2_val, on=['user_id'], how=\"left\")\n",
    "\n",
    "validation = pd.merge(validation, user_lecture_stats_part, on=['user_id'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "papermill": {
     "duration": 2.891219,
     "end_time": "2020-10-30T18:32:22.798219",
     "exception": false,
     "start_time": "2020-10-30T18:32:19.907000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "\n",
    "X.prior_question_had_explanation.fillna(False, inplace = True)\n",
    "validation.prior_question_had_explanation.fillna(False, inplace = True)\n",
    "\n",
    "validation[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(validation[\"prior_question_had_explanation\"])\n",
    "X[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(X[\"prior_question_had_explanation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "papermill": {
     "duration": 0.117061,
     "end_time": "2020-10-30T18:32:23.019768",
     "exception": false,
     "start_time": "2020-10-30T18:32:22.902707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reading in question df\n",
    "#question2 = pd.read_csv('/kaggle/input/question2/question2.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "papermill": {
     "duration": 0.107573,
     "end_time": "2020-10-30T18:32:23.223490",
     "exception": false,
     "start_time": "2020-10-30T18:32:23.115917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7098044677073315"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_mean = question2.quest_pct.mean()\n",
    "\n",
    "question2.quest_pct.mean()\n",
    "#there are a lot of high percentage questions, should use median instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "papermill": {
     "duration": 0.133191,
     "end_time": "2020-10-30T18:32:23.450587",
     "exception": false,
     "start_time": "2020-10-30T18:32:23.317396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#filling questions with no info with a new value\n",
    "question2.quest_pct = question2.quest_pct.mask((question2['count'] < 3), .65)\n",
    "\n",
    "\n",
    "#filling very hard new questions with a more reasonable value\n",
    "question2.quest_pct = question2.quest_pct.mask((question2.quest_pct < .2) & (question2['count'] < 21), .2)\n",
    "\n",
    "#filling very easy new questions with a more reasonable value\n",
    "question2.quest_pct = question2.quest_pct.mask((question2.quest_pct > .95) & (question2['count'] < 21), .95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "papermill": {
     "duration": 2.658176,
     "end_time": "2020-10-30T18:32:26.206787",
     "exception": false,
     "start_time": "2020-10-30T18:32:23.548611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = pd.merge(X, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "validation = pd.merge(validation, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "X.part = X.part - 1\n",
    "validation.part = validation.part - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "papermill": {
     "duration": 0.099169,
     "end_time": "2020-10-30T18:32:26.372357",
     "exception": false,
     "start_time": "2020-10-30T18:32:26.273188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "      <th>avg_questions</th>\n",
       "      <th>avg_questions_seen</th>\n",
       "      <th>answered_correctly_user</th>\n",
       "      <th>explanation_mean_user</th>\n",
       "      <th>...</th>\n",
       "      <th>part_7_boolean</th>\n",
       "      <th>type_of_concept_boolean</th>\n",
       "      <th>type_of_intention_boolean</th>\n",
       "      <th>type_of_solving_question_boolean</th>\n",
       "      <th>type_of_starter_boolean</th>\n",
       "      <th>prior_question_had_explanation_enc</th>\n",
       "      <th>question_id</th>\n",
       "      <th>part</th>\n",
       "      <th>quest_pct</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62379023</td>\n",
       "      <td>5543</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1.006293</td>\n",
       "      <td>1.006293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5543</td>\n",
       "      <td>4</td>\n",
       "      <td>0.77876</td>\n",
       "      <td>791.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62354036</td>\n",
       "      <td>4348</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1.006293</td>\n",
       "      <td>1.006293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4348</td>\n",
       "      <td>4</td>\n",
       "      <td>0.86364</td>\n",
       "      <td>572.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>345926160</td>\n",
       "      <td>6169</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1.006293</td>\n",
       "      <td>1.006293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6169</td>\n",
       "      <td>4</td>\n",
       "      <td>0.75128</td>\n",
       "      <td>587.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>345962922</td>\n",
       "      <td>5693</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1.006293</td>\n",
       "      <td>1.006293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5693</td>\n",
       "      <td>4</td>\n",
       "      <td>0.92828</td>\n",
       "      <td>739.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345773304</td>\n",
       "      <td>4164</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1.006293</td>\n",
       "      <td>1.006293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4164</td>\n",
       "      <td>4</td>\n",
       "      <td>0.75388</td>\n",
       "      <td>516.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  content_id  task_container_id  answered_correctly  \\\n",
       "0   62379023        5543                  0                   1   \n",
       "1   62354036        4348                  0                   1   \n",
       "2  345926160        6169                  0                   0   \n",
       "3  345962922        5693                  0                   1   \n",
       "4  345773304        4164                  0                   1   \n",
       "\n",
       "   prior_question_elapsed_time  prior_question_had_explanation  avg_questions  \\\n",
       "0                          NaN                           False       1.006293   \n",
       "1                          NaN                           False       1.006293   \n",
       "2                          NaN                           False       1.006293   \n",
       "3                          NaN                           False       1.006293   \n",
       "4                          NaN                           False       1.006293   \n",
       "\n",
       "   avg_questions_seen  answered_correctly_user  explanation_mean_user  ...  \\\n",
       "0            1.006293                      NaN                    NaN  ...   \n",
       "1            1.006293                      NaN                    NaN  ...   \n",
       "2            1.006293                      NaN                    NaN  ...   \n",
       "3            1.006293                      NaN                    NaN  ...   \n",
       "4            1.006293                      NaN                    NaN  ...   \n",
       "\n",
       "   part_7_boolean  type_of_concept_boolean  type_of_intention_boolean  \\\n",
       "0             NaN                      NaN                        NaN   \n",
       "1             NaN                      NaN                        NaN   \n",
       "2             NaN                      NaN                        NaN   \n",
       "3             NaN                      NaN                        NaN   \n",
       "4             NaN                      NaN                        NaN   \n",
       "\n",
       "   type_of_solving_question_boolean  type_of_starter_boolean  \\\n",
       "0                               NaN                      NaN   \n",
       "1                               NaN                      NaN   \n",
       "2                               NaN                      NaN   \n",
       "3                               NaN                      NaN   \n",
       "4                               NaN                      NaN   \n",
       "\n",
       "   prior_question_had_explanation_enc  question_id  part  quest_pct  count  \n",
       "0                                   0         5543     4    0.77876  791.0  \n",
       "1                                   0         4348     4    0.86364  572.0  \n",
       "2                                   0         6169     4    0.75128  587.0  \n",
       "3                                   0         5693     4    0.92828  739.0  \n",
       "4                                   0         4164     4    0.75388  516.0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "papermill": {
     "duration": 2.030403,
     "end_time": "2020-10-30T18:32:28.475400",
     "exception": false,
     "start_time": "2020-10-30T18:32:26.444997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = X['answered_correctly']\n",
    "X = X.drop(['answered_correctly'], axis=1)\n",
    "X.head()\n",
    "\n",
    "y_val = validation['answered_correctly']\n",
    "X_val = validation.drop(['answered_correctly'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "papermill": {
     "duration": 0.702908,
     "end_time": "2020-10-30T18:32:29.246464",
     "exception": false,
     "start_time": "2020-10-30T18:32:28.543556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = X[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n",
    "       'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part',\n",
    "       'part_1', 'part_2', 'part_3', 'part_4', 'part_5', 'part_6', 'part_7',\n",
    "       'type_of_concept', 'type_of_intention', 'type_of_solving_question', 'type_of_starter',\n",
    "       'part_1_boolean', 'part_2_boolean', 'part_3_boolean', 'part_4_boolean', 'part_5_boolean', 'part_6_boolean', 'part_7_boolean',\n",
    "       'type_of_concept_boolean', 'type_of_intention_boolean', 'type_of_solving_question_boolean', 'type_of_starter_boolean']]\n",
    "X_val = X_val[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n",
    "               'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part',\n",
    "               'part_1', 'part_2', 'part_3', 'part_4', 'part_5', 'part_6', 'part_7',\n",
    "               'type_of_concept', 'type_of_intention', 'type_of_solving_question', 'type_of_starter',\n",
    "               'part_1_boolean', 'part_2_boolean', 'part_3_boolean', 'part_4_boolean', 'part_5_boolean', 'part_6_boolean', 'part_7_boolean',\n",
    "               'type_of_concept_boolean', 'type_of_intention_boolean', 'type_of_solving_question_boolean', 'type_of_starter_boolean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "papermill": {
     "duration": 0.628645,
     "end_time": "2020-10-30T18:32:29.943345",
     "exception": false,
     "start_time": "2020-10-30T18:32:29.314700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Filling with 0.5 for simplicity; there could likely be a better value\n",
    "X['answered_correctly_user'].fillna(0.65,  inplace=True)\n",
    "X['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\n",
    "X['quest_pct'].fillna(content_mean, inplace=True)\n",
    "\n",
    "X['part'].fillna(4, inplace = True)\n",
    "X['avg_questions_seen'].fillna(1, inplace = True)\n",
    "X['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\n",
    "X['prior_question_had_explanation_enc'].fillna(0, inplace = True)\n",
    "\n",
    "X['part_1'].fillna(0, inplace = True)\n",
    "X['part_2'].fillna(0, inplace = True)\n",
    "X['part_3'].fillna(0, inplace = True)\n",
    "X['part_4'].fillna(0, inplace = True)\n",
    "X['part_5'].fillna(0, inplace = True)\n",
    "X['part_6'].fillna(0, inplace = True)\n",
    "X['part_7'].fillna(0, inplace = True)\n",
    "X['type_of_concept'].fillna(0, inplace = True)\n",
    "X['type_of_intention'].fillna(0, inplace = True)\n",
    "X['type_of_solving_question'].fillna(0, inplace = True)\n",
    "X['type_of_starter'].fillna(0, inplace = True)\n",
    "X['part_1_boolean'].fillna(0, inplace = True)\n",
    "X['part_2_boolean'].fillna(0, inplace = True)\n",
    "X['part_3_boolean'].fillna(0, inplace = True)\n",
    "X['part_4_boolean'].fillna(0, inplace = True)\n",
    "X['part_5_boolean'].fillna(0, inplace = True)\n",
    "X['part_6_boolean'].fillna(0, inplace = True)\n",
    "X['part_7_boolean'].fillna(0, inplace = True)\n",
    "X['type_of_concept_boolean'].fillna(0, inplace = True)\n",
    "X['type_of_intention_boolean'].fillna(0, inplace = True)\n",
    "X['type_of_solving_question_boolean'].fillna(0, inplace = True)\n",
    "X['type_of_starter_boolean'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "papermill": {
     "duration": 0.25411,
     "end_time": "2020-10-30T18:32:30.266506",
     "exception": false,
     "start_time": "2020-10-30T18:32:30.012396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_val['answered_correctly_user'].fillna(0.65,  inplace=True)\n",
    "X_val['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\n",
    "X_val['quest_pct'].fillna(content_mean,  inplace=True)\n",
    "\n",
    "X_val['part'].fillna(4, inplace = True)\n",
    "X_val['avg_questions_seen'].fillna(1, inplace = True)\n",
    "X_val['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\n",
    "X_val['prior_question_had_explanation_enc'].fillna(0, inplace = True)\n",
    "\n",
    "X_val['part_1'].fillna(0, inplace = True)\n",
    "X_val['part_2'].fillna(0, inplace = True)\n",
    "X_val['part_3'].fillna(0, inplace = True)\n",
    "X_val['part_4'].fillna(0, inplace = True)\n",
    "X_val['part_5'].fillna(0, inplace = True)\n",
    "X_val['part_6'].fillna(0, inplace = True)\n",
    "X_val['part_7'].fillna(0, inplace = True)\n",
    "X_val['type_of_concept'].fillna(0, inplace = True)\n",
    "X_val['type_of_intention'].fillna(0, inplace = True)\n",
    "X_val['type_of_solving_question'].fillna(0, inplace = True)\n",
    "X_val['type_of_starter'].fillna(0, inplace = True)\n",
    "X_val['part_1_boolean'].fillna(0, inplace = True)\n",
    "X_val['part_2_boolean'].fillna(0, inplace = True)\n",
    "X_val['part_3_boolean'].fillna(0, inplace = True)\n",
    "X_val['part_4_boolean'].fillna(0, inplace = True)\n",
    "X_val['part_5_boolean'].fillna(0, inplace = True)\n",
    "X_val['part_6_boolean'].fillna(0, inplace = True)\n",
    "X_val['part_7_boolean'].fillna(0, inplace = True)\n",
    "X_val['type_of_concept_boolean'].fillna(0, inplace = True)\n",
    "X_val['type_of_intention_boolean'].fillna(0, inplace = True)\n",
    "X_val['type_of_solving_question_boolean'].fillna(0, inplace = True)\n",
    "X_val['type_of_starter_boolean'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.067624,
     "end_time": "2020-10-30T18:32:30.402330",
     "exception": false,
     "start_time": "2020-10-30T18:32:30.334706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Modeling ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "papermill": {
     "duration": 3.577685,
     "end_time": "2020-10-30T18:32:34.047978",
     "exception": false,
     "start_time": "2020-10-30T18:32:30.470293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cont_columns = ['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n",
    "               'prior_question_elapsed_time']\n",
    "\n",
    "cat_columns = ['prior_question_had_explanation_enc', 'part',\n",
    "               'part_1', 'part_2', 'part_3', 'part_4', 'part_5', 'part_6', 'part_7',\n",
    "               'type_of_concept', 'type_of_intention', 'type_of_solving_question', 'type_of_starter',\n",
    "               'part_1_boolean', 'part_2_boolean', 'part_3_boolean', 'part_4_boolean', 'part_5_boolean', 'part_6_boolean', 'part_7_boolean',\n",
    "               'type_of_concept_boolean', 'type_of_intention_boolean', 'type_of_solving_question_boolean', 'type_of_starter_boolean']\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "for col in cat_columns+cont_columns:\n",
    "    X[col] = scaler.fit_transform(X[col].values.reshape(-1,1))\n",
    "    X_val[col] = scaler.fit_transform(X_val[col].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "papermill": {
     "duration": 1.122917,
     "end_time": "2020-10-30T18:59:52.203494",
     "exception": false,
     "start_time": "2020-10-30T18:59:51.080577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda ready...\n",
      "cuda:0\n",
      "Train on 1050564 samples, validate on 262642 samples, 33 steps per epoch\n",
      "Epoch 1/50\n",
      "11s - loss:  0.6883 - binary_crossentropy:  0.6881 - auc:  0.6273 - val_binary_crossentropy:  0.6676 - val_auc:  0.5282\n",
      "Epoch 2/50\n",
      "11s - loss:  0.6830 - binary_crossentropy:  0.6830 - auc:  0.6413 - val_binary_crossentropy:  0.6625 - val_auc:  0.5284\n",
      "Epoch 3/50\n",
      "11s - loss:  0.6802 - binary_crossentropy:  0.6803 - auc:  0.6415 - val_binary_crossentropy:  0.6618 - val_auc:  0.5291\n",
      "Epoch 4/50\n",
      "11s - loss:  0.6782 - binary_crossentropy:  0.6781 - auc:  0.6431 - val_binary_crossentropy:  0.6610 - val_auc:  0.5298\n",
      "Epoch 5/50\n",
      "11s - loss:  0.6764 - binary_crossentropy:  0.6763 - auc:  0.6442 - val_binary_crossentropy:  0.6601 - val_auc:  0.5307\n",
      "Epoch 6/50\n",
      "11s - loss:  0.6746 - binary_crossentropy:  0.6746 - auc:  0.6442 - val_binary_crossentropy:  0.6589 - val_auc:  0.5319\n",
      "Epoch 7/50\n",
      "11s - loss:  0.6721 - binary_crossentropy:  0.6722 - auc:  0.6445 - val_binary_crossentropy:  0.6574 - val_auc:  0.5318\n",
      "Epoch 8/50\n",
      "11s - loss:  0.6680 - binary_crossentropy:  0.6680 - auc:  0.6475 - val_binary_crossentropy:  0.6556 - val_auc:  0.5394\n",
      "Epoch 9/50\n",
      "11s - loss:  0.6663 - binary_crossentropy:  0.6661 - auc:  0.6559 - val_binary_crossentropy:  0.6561 - val_auc:  0.5441\n",
      "Epoch 10/50\n",
      "11s - loss:  0.6662 - binary_crossentropy:  0.6662 - auc:  0.6560 - val_binary_crossentropy:  0.6560 - val_auc:  0.5438\n",
      "Epoch 11/50\n",
      "11s - loss:  0.6662 - binary_crossentropy:  0.6664 - auc:  0.6551 - val_binary_crossentropy:  0.6559 - val_auc:  0.5436\n",
      "Epoch 12/50\n",
      "11s - loss:  0.6661 - binary_crossentropy:  0.6660 - auc:  0.6561 - val_binary_crossentropy:  0.6556 - val_auc:  0.5440\n",
      "Epoch 13/50\n",
      "11s - loss:  0.6661 - binary_crossentropy:  0.6662 - auc:  0.6559 - val_binary_crossentropy:  0.6555 - val_auc:  0.5443\n",
      "Epoch 14/50\n",
      "11s - loss:  0.6661 - binary_crossentropy:  0.6661 - auc:  0.6561 - val_binary_crossentropy:  0.6559 - val_auc:  0.5443\n",
      "Epoch 15/50\n",
      "11s - loss:  0.6661 - binary_crossentropy:  0.6663 - auc:  0.6555 - val_binary_crossentropy:  0.6558 - val_auc:  0.5445\n",
      "Epoch 16/50\n",
      "11s - loss:  0.6661 - binary_crossentropy:  0.6662 - auc:  0.6556 - val_binary_crossentropy:  0.6557 - val_auc:  0.5449\n",
      "Epoch 17/50\n",
      "11s - loss:  0.6661 - binary_crossentropy:  0.6661 - auc:  0.6562 - val_binary_crossentropy:  0.6557 - val_auc:  0.5453\n",
      "Epoch 18/50\n",
      "11s - loss:  0.6661 - binary_crossentropy:  0.6661 - auc:  0.6560 - val_binary_crossentropy:  0.6555 - val_auc:  0.5454\n",
      "Epoch 19/50\n",
      "11s - loss:  0.6661 - binary_crossentropy:  0.6662 - auc:  0.6559 - val_binary_crossentropy:  0.6557 - val_auc:  0.5454\n",
      "Epoch 20/50\n",
      "11s - loss:  0.6661 - binary_crossentropy:  0.6660 - auc:  0.6563 - val_binary_crossentropy:  0.6556 - val_auc:  0.5454\n",
      "Epoch 21/50\n",
      "11s - loss:  0.6661 - binary_crossentropy:  0.6660 - auc:  0.6559 - val_binary_crossentropy:  0.6555 - val_auc:  0.5455\n",
      "Epoch 22/50\n",
      "11s - loss:  0.6661 - binary_crossentropy:  0.6660 - auc:  0.6564 - val_binary_crossentropy:  0.6554 - val_auc:  0.5456\n",
      "Epoch 23/50\n",
      "11s - loss:  0.6661 - binary_crossentropy:  0.6659 - auc:  0.6566 - val_binary_crossentropy:  0.6557 - val_auc:  0.5449\n",
      "Epoch 24/50\n",
      "11s - loss:  0.6661 - binary_crossentropy:  0.6663 - auc:  0.6556 - val_binary_crossentropy:  0.6557 - val_auc:  0.5452\n",
      "Epoch 25/50\n",
      "11s - loss:  0.6661 - binary_crossentropy:  0.6662 - auc:  0.6561 - val_binary_crossentropy:  0.6556 - val_auc:  0.5457\n",
      "Epoch 26/50\n",
      "11s - loss:  0.6661 - binary_crossentropy:  0.6663 - auc:  0.6555 - val_binary_crossentropy:  0.6554 - val_auc:  0.5453\n",
      "Epoch 27/50\n",
      "11s - loss:  0.6661 - binary_crossentropy:  0.6662 - auc:  0.6560 - val_binary_crossentropy:  0.6554 - val_auc:  0.5454\n",
      "Epoch 28/50\n",
      "11s - loss:  0.6661 - binary_crossentropy:  0.6662 - auc:  0.6559 - val_binary_crossentropy:  0.6554 - val_auc:  0.5457\n",
      "Epoch 29/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6659 - auc:  0.6564 - val_binary_crossentropy:  0.6554 - val_auc:  0.5469\n",
      "Epoch 30/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6661 - auc:  0.6564 - val_binary_crossentropy:  0.6556 - val_auc:  0.5468\n",
      "Epoch 31/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6661 - auc:  0.6564 - val_binary_crossentropy:  0.6556 - val_auc:  0.5464\n",
      "Epoch 32/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6659 - auc:  0.6566 - val_binary_crossentropy:  0.6553 - val_auc:  0.5466\n",
      "Epoch 33/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6661 - auc:  0.6565 - val_binary_crossentropy:  0.6555 - val_auc:  0.5469\n",
      "Epoch 34/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6659 - auc:  0.6567 - val_binary_crossentropy:  0.6556 - val_auc:  0.5466\n",
      "Epoch 35/50\n",
      "11s - loss:  0.6661 - binary_crossentropy:  0.6660 - auc:  0.6565 - val_binary_crossentropy:  0.6558 - val_auc:  0.5465\n",
      "Epoch 36/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6660 - auc:  0.6568 - val_binary_crossentropy:  0.6555 - val_auc:  0.5469\n",
      "Epoch 37/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6662 - auc:  0.6560 - val_binary_crossentropy:  0.6555 - val_auc:  0.5471\n",
      "Epoch 38/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6663 - auc:  0.6558 - val_binary_crossentropy:  0.6553 - val_auc:  0.5470\n",
      "Epoch 39/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6661 - auc:  0.6561 - val_binary_crossentropy:  0.6554 - val_auc:  0.5470\n",
      "Epoch 40/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6661 - auc:  0.6564 - val_binary_crossentropy:  0.6555 - val_auc:  0.5466\n",
      "Epoch 41/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6663 - auc:  0.6560 - val_binary_crossentropy:  0.6553 - val_auc:  0.5472\n",
      "Epoch 42/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6662 - auc:  0.6561 - val_binary_crossentropy:  0.6554 - val_auc:  0.5472\n",
      "Epoch 43/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6660 - auc:  0.6565 - val_binary_crossentropy:  0.6558 - val_auc:  0.5467\n",
      "Epoch 44/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6660 - auc:  0.6565 - val_binary_crossentropy:  0.6554 - val_auc:  0.5471\n",
      "Epoch 45/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6658 - auc:  0.6571 - val_binary_crossentropy:  0.6554 - val_auc:  0.5471\n",
      "Epoch 46/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6659 - auc:  0.6564 - val_binary_crossentropy:  0.6555 - val_auc:  0.5468\n",
      "Epoch 47/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6659 - auc:  0.6567 - val_binary_crossentropy:  0.6553 - val_auc:  0.5473\n",
      "Epoch 48/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6658 - auc:  0.6567 - val_binary_crossentropy:  0.6555 - val_auc:  0.5470\n",
      "Epoch 49/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6663 - auc:  0.6559 - val_binary_crossentropy:  0.6556 - val_auc:  0.5470\n",
      "Epoch 50/50\n",
      "11s - loss:  0.6660 - binary_crossentropy:  0.6660 - auc:  0.6563 - val_binary_crossentropy:  0.6554 - val_auc:  0.5475\n",
      "\n",
      "test LogLoss 0.6827\n",
      "test AUC 0.6015\n"
     ]
    }
   ],
   "source": [
    "fixlen_feature_columns = [SparseFeat(feat, X[feat].nunique())\n",
    "                          for feat in cat_columns] + [DenseFeat(feat, 1, )\n",
    "                                                      for feat in cont_columns]\n",
    "\n",
    "feature_names = get_feature_names(fixlen_feature_columns)\n",
    "\n",
    "# 3.generate input data for model\n",
    "\n",
    "train_model_input = {name: X[name] for name in feature_names}\n",
    "test_model_input = {name: X_val[name] for name in feature_names}\n",
    "\n",
    "device = 'cpu'\n",
    "use_cuda = True\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    print('cuda ready...')\n",
    "    device = 'cuda:0'\n",
    "\n",
    "'''\n",
    "model = DeepFM(linear_feature_columns=fixlen_feature_columns, dnn_feature_columns=fixlen_feature_columns,\n",
    "               task='binary',dnn_dropout=0.5, dnn_use_bn=True, \n",
    "               l2_reg_embedding=1e-5, device=device)\n",
    "'''\n",
    "\n",
    "\n",
    "model = DCNMix(linear_feature_columns=fixlen_feature_columns, dnn_feature_columns=fixlen_feature_columns,dnn_use_bn=True,\n",
    "            cross_num=5, dnn_hidden_units=[64,64,64,64,64,64,64,64,64,64,64,64,64,64,], dnn_dropout=0.5, device=device)\n",
    "\n",
    "#model = xDeepFM(fixlen_feature_columns, fixlen_feature_columns, dnn_use_bn=True, dnn_dropout=0.5, device='cuda:0')\n",
    "\n",
    "model.compile(\"adam\", \"binary_crossentropy\",\n",
    "              metrics=[\"binary_crossentropy\", \"auc\"], )\n",
    "\n",
    "model.fit(train_model_input, y.values, batch_size=2**15, epochs=50, verbose=2, validation_split=0.2)\n",
    "\n",
    "pred_ans = model.predict(test_model_input, 256)\n",
    "print(\"\")\n",
    "print(\"test LogLoss\", round(log_loss(y_val.values, pred_ans), 4))\n",
    "print(\"test AUC\", round(roc_auc_score(y_val.values, pred_ans), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc  ###计算roc和auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def acu_curve(y,prob):\n",
    "    fpr,tpr,threshold = roc_curve(y,prob) ###计算真正率和假正率\n",
    "    roc_auc = auc(fpr,tpr) ###计算auc的值\n",
    " \n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.3f)' % roc_auc) ###假正率为横坐标，真正率为纵坐标做曲线\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    " \n",
    "    plt.show()\n",
    "acu_curve(y_val.values, pred_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "papermill": {
     "duration": 2.149547,
     "end_time": "2020-10-30T18:59:55.393297",
     "exception": false,
     "start_time": "2020-10-30T18:59:53.243750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport riiideducation\\nenv = riiideducation.make_env()\\niter_test = env.iter_test()\\n\\nfor (test_df, sample_prediction_df) in iter_test:\\n    test_df[\\'task_container_id\\'] = test_df.task_container_id.mask(test_df.task_container_id > 9999, 9999)\\n    test_df = pd.merge(test_df, group3, left_on=[\\'task_container_id\\'], right_index= True, how=\"left\")\\n    test_df = pd.merge(test_df, question2, left_on = \\'content_id\\', right_on = \\'question_id\\', how = \\'left\\')\\n    test_df = pd.merge(test_df, results_u_final, on=[\\'user_id\\'],  how=\"left\")\\n    test_df = pd.merge(test_df, results_u2_final, on=[\\'user_id\\'],  how=\"left\")\\n    \\n    test_df = pd.merge(test_df, user_lecture_stats_part, on=[\\'user_id\\'], how=\"left\")\\n    test_df[\\'part_1\\'].fillna(0, inplace = True)\\n    test_df[\\'part_2\\'].fillna(0, inplace = True)\\n    test_df[\\'part_3\\'].fillna(0, inplace = True)\\n    test_df[\\'part_4\\'].fillna(0, inplace = True)\\n    test_df[\\'part_5\\'].fillna(0, inplace = True)\\n    test_df[\\'part_6\\'].fillna(0, inplace = True)\\n    test_df[\\'part_7\\'].fillna(0, inplace = True)\\n    test_df[\\'type_of_concept\\'].fillna(0, inplace = True)\\n    test_df[\\'type_of_intention\\'].fillna(0, inplace = True)\\n    test_df[\\'type_of_solving_question\\'].fillna(0, inplace = True)\\n    test_df[\\'type_of_starter\\'].fillna(0, inplace = True)\\n    test_df[\\'part_1_boolean\\'].fillna(0, inplace = True)\\n    test_df[\\'part_2_boolean\\'].fillna(0, inplace = True)\\n    test_df[\\'part_3_boolean\\'].fillna(0, inplace = True)\\n    test_df[\\'part_4_boolean\\'].fillna(0, inplace = True)\\n    test_df[\\'part_5_boolean\\'].fillna(0, inplace = True)\\n    test_df[\\'part_6_boolean\\'].fillna(0, inplace = True)\\n    test_df[\\'part_7_boolean\\'].fillna(0, inplace = True)\\n    test_df[\\'type_of_concept_boolean\\'].fillna(0, inplace = True)\\n    test_df[\\'type_of_intention_boolean\\'].fillna(0, inplace = True)\\n    test_df[\\'type_of_solving_question_boolean\\'].fillna(0, inplace = True)\\n    test_df[\\'type_of_starter_boolean\\'].fillna(0, inplace = True)\\n    \\n    test_df[\\'answered_correctly_user\\'].fillna(0.65,  inplace=True)\\n    test_df[\\'explanation_mean_user\\'].fillna(prior_mean_user,  inplace=True)\\n    test_df[\\'quest_pct\\'].fillna(content_mean,  inplace=True)\\n    test_df[\\'part\\'] = test_df.part - 1\\n\\n    test_df[\\'part\\'].fillna(4, inplace = True)\\n    test_df[\\'avg_questions_seen\\'].fillna(1, inplace = True)\\n    test_df[\\'prior_question_elapsed_time\\'].fillna(elapsed_mean, inplace = True)\\n    test_df[\\'prior_question_had_explanation\\'].fillna(False, inplace=True)\\n    test_df[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(test_df[\"prior_question_had_explanation\"])\\n    \\n    X = test_df[[\\'answered_correctly_user\\', \\'explanation_mean_user\\', \\'quest_pct\\', \\'avg_questions_seen\\',\\n                                                            \\'prior_question_elapsed_time\\',\\'prior_question_had_explanation_enc\\', \\'part\\',\\n                                                            \\'part_1\\', \\'part_2\\', \\'part_3\\', \\'part_4\\', \\'part_5\\', \\'part_6\\', \\'part_7\\',\\n                                                            \\'type_of_concept\\', \\'type_of_intention\\', \\'type_of_solving_question\\', \\'type_of_starter\\',\\n                                                            \\'part_1_boolean\\', \\'part_2_boolean\\', \\'part_3_boolean\\', \\'part_4_boolean\\', \\'part_5_boolean\\', \\'part_6_boolean\\', \\'part_7_boolean\\',\\n                                                            \\'type_of_concept_boolean\\', \\'type_of_intention_boolean\\', \\'type_of_solving_question_boolean\\', \\'type_of_starter_boolean\\']]\\n    for col in cat_columns + cont_columns:\\n        X[col] = scaler.fit_transoform(X[col].values.reshape(-1,1))\\n        \\n     fixlen_feature_columns = [SparseFeat(feat, X[feat].nunique())\\n                          for feat in cat_columns] + [DenseFeat(feat, 1, )\\n                                                      for feat in cont_columns]\\n    \\n    feature_names = get_feature_names(fixlen_feature_columns)\\n    \\n    test_model_input = {name: X[name] for name in feature_names}\\n\\n    test_df[\\'answered_correctly\\'] = model.predict(test_model_input, 256)\\n\\n    env.predict(test_df.loc[test_df[\\'content_type_id\\'] == 0, [\\'row_id\\', \\'answered_correctly\\']])\\n\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import riiideducation\n",
    "env = riiideducation.make_env()\n",
    "iter_test = env.iter_test()\n",
    "\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    test_df['task_container_id'] = test_df.task_container_id.mask(test_df.task_container_id > 9999, 9999)\n",
    "    test_df = pd.merge(test_df, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\n",
    "    test_df = pd.merge(test_df, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "    test_df = pd.merge(test_df, results_u_final, on=['user_id'],  how=\"left\")\n",
    "    test_df = pd.merge(test_df, results_u2_final, on=['user_id'],  how=\"left\")\n",
    "    \n",
    "    test_df = pd.merge(test_df, user_lecture_stats_part, on=['user_id'], how=\"left\")\n",
    "    test_df['part_1'].fillna(0, inplace = True)\n",
    "    test_df['part_2'].fillna(0, inplace = True)\n",
    "    test_df['part_3'].fillna(0, inplace = True)\n",
    "    test_df['part_4'].fillna(0, inplace = True)\n",
    "    test_df['part_5'].fillna(0, inplace = True)\n",
    "    test_df['part_6'].fillna(0, inplace = True)\n",
    "    test_df['part_7'].fillna(0, inplace = True)\n",
    "    test_df['type_of_concept'].fillna(0, inplace = True)\n",
    "    test_df['type_of_intention'].fillna(0, inplace = True)\n",
    "    test_df['type_of_solving_question'].fillna(0, inplace = True)\n",
    "    test_df['type_of_starter'].fillna(0, inplace = True)\n",
    "    test_df['part_1_boolean'].fillna(0, inplace = True)\n",
    "    test_df['part_2_boolean'].fillna(0, inplace = True)\n",
    "    test_df['part_3_boolean'].fillna(0, inplace = True)\n",
    "    test_df['part_4_boolean'].fillna(0, inplace = True)\n",
    "    test_df['part_5_boolean'].fillna(0, inplace = True)\n",
    "    test_df['part_6_boolean'].fillna(0, inplace = True)\n",
    "    test_df['part_7_boolean'].fillna(0, inplace = True)\n",
    "    test_df['type_of_concept_boolean'].fillna(0, inplace = True)\n",
    "    test_df['type_of_intention_boolean'].fillna(0, inplace = True)\n",
    "    test_df['type_of_solving_question_boolean'].fillna(0, inplace = True)\n",
    "    test_df['type_of_starter_boolean'].fillna(0, inplace = True)\n",
    "    \n",
    "    test_df['answered_correctly_user'].fillna(0.65,  inplace=True)\n",
    "    test_df['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\n",
    "    test_df['quest_pct'].fillna(content_mean,  inplace=True)\n",
    "    test_df['part'] = test_df.part - 1\n",
    "\n",
    "    test_df['part'].fillna(4, inplace = True)\n",
    "    test_df['avg_questions_seen'].fillna(1, inplace = True)\n",
    "    test_df['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\n",
    "    test_df['prior_question_had_explanation'].fillna(False, inplace=True)\n",
    "    test_df[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(test_df[\"prior_question_had_explanation\"])\n",
    "    \n",
    "    X = test_df[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n",
    "                                                            'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part',\n",
    "                                                            'part_1', 'part_2', 'part_3', 'part_4', 'part_5', 'part_6', 'part_7',\n",
    "                                                            'type_of_concept', 'type_of_intention', 'type_of_solving_question', 'type_of_starter',\n",
    "                                                            'part_1_boolean', 'part_2_boolean', 'part_3_boolean', 'part_4_boolean', 'part_5_boolean', 'part_6_boolean', 'part_7_boolean',\n",
    "                                                            'type_of_concept_boolean', 'type_of_intention_boolean', 'type_of_solving_question_boolean', 'type_of_starter_boolean']]\n",
    "    for col in cat_columns + cont_columns:\n",
    "        X[col] = scaler.fit_transoform(X[col].values.reshape(-1,1))\n",
    "        \n",
    "     fixlen_feature_columns = [SparseFeat(feat, X[feat].nunique())\n",
    "                          for feat in cat_columns] + [DenseFeat(feat, 1, )\n",
    "                                                      for feat in cont_columns]\n",
    "    \n",
    "    feature_names = get_feature_names(fixlen_feature_columns)\n",
    "    \n",
    "    test_model_input = {name: X[name] for name in feature_names}\n",
    "\n",
    "    test_df['answered_correctly'] = model.predict(test_model_input, 256)\n",
    "\n",
    "    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-15",
   "language": "python",
   "name": "torch-15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "duration": 2235.200117,
   "end_time": "2020-10-30T18:59:58.325254",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-30T18:22:43.125137",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
