{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049311,
     "end_time": "2020-10-30T18:22:55.213053",
     "exception": false,
     "start_time": "2020-10-30T18:22:55.163742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Reading Data and Importing Libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.953599,
     "end_time": "2020-10-30T18:22:56.217436",
     "exception": false,
     "start_time": "2020-10-30T18:22:55.263837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 265.168936,
     "end_time": "2020-10-30T18:27:21.467302",
     "exception": false,
     "start_time": "2020-10-30T18:22:56.298366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nrows = 200 * 10000\n",
    "\n",
    "train = pd.read_csv('riiid_data/train.csv',\n",
    "                   usecols=[1, 2, 3, 4, 5, 7, 8, 9],\n",
    "                    nrows = nrows,\n",
    "                   dtype={'timestamp': 'int64',\n",
    "                          'user_id': 'int32',\n",
    "                          'content_id': 'int16',\n",
    "                          'content_type_id': 'int8',\n",
    "                          'task_container_id': 'int16',\n",
    "                          'answered_correctly':'int8',\n",
    "                          'prior_question_elapsed_time': 'float32',\n",
    "                          'prior_question_had_explanation': 'boolean'}\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.063739,
     "end_time": "2020-10-30T18:27:21.579272",
     "exception": false,
     "start_time": "2020-10-30T18:27:21.515533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reading in question df\n",
    "questions_df = pd.read_csv('riiid_data/questions.csv',   \n",
    "                            nrows = nrows,\n",
    "                            usecols=[0, 3],\n",
    "                            dtype={'question_id': 'int16',\n",
    "                              'part': 'int8'}\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.059379,
     "end_time": "2020-10-30T18:27:21.686115",
     "exception": false,
     "start_time": "2020-10-30T18:27:21.626736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reading in lecture df\n",
    "lectures_df = pd.read_csv('riiid_data/lectures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.078297,
     "end_time": "2020-10-30T18:27:21.812238",
     "exception": false,
     "start_time": "2020-10-30T18:27:21.733941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lectures_df['type_of'] = lectures_df['type_of'].replace('solving question', 'solving_question')\n",
    "\n",
    "lectures_df = pd.get_dummies(lectures_df, columns=['part', 'type_of'])\n",
    "\n",
    "part_lectures_columns = [column for column in lectures_df.columns if column.startswith('part')]\n",
    "\n",
    "types_of_lectures_columns = [column for column in lectures_df.columns if column.startswith('type_of_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.06796,
     "end_time": "2020-10-30T18:27:21.927852",
     "exception": false,
     "start_time": "2020-10-30T18:27:21.859892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lecture_id</th>\n",
       "      <th>tag</th>\n",
       "      <th>part_1</th>\n",
       "      <th>part_2</th>\n",
       "      <th>part_3</th>\n",
       "      <th>part_4</th>\n",
       "      <th>part_5</th>\n",
       "      <th>part_6</th>\n",
       "      <th>part_7</th>\n",
       "      <th>type_of_concept</th>\n",
       "      <th>type_of_intention</th>\n",
       "      <th>type_of_solving_question</th>\n",
       "      <th>type_of_starter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>185</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>317</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lecture_id  tag  part_1  part_2  part_3  part_4  part_5  part_6  part_7  \\\n",
       "0          89  159       0       0       0       0       1       0       0   \n",
       "1         100   70       1       0       0       0       0       0       0   \n",
       "2         185   45       0       0       0       0       0       1       0   \n",
       "3         192   79       0       0       0       0       1       0       0   \n",
       "4         317  156       0       0       0       0       1       0       0   \n",
       "\n",
       "   type_of_concept  type_of_intention  type_of_solving_question  \\\n",
       "0                1                  0                         0   \n",
       "1                1                  0                         0   \n",
       "2                1                  0                         0   \n",
       "3                0                  0                         1   \n",
       "4                0                  0                         1   \n",
       "\n",
       "   type_of_starter  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lectures_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.831637,
     "end_time": "2020-10-30T18:27:22.808805",
     "exception": false,
     "start_time": "2020-10-30T18:27:21.977168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge lecture features to train dataset\n",
    "train_lectures = train[train.content_type_id == True].merge(lectures_df, left_on='content_id', right_on='lecture_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 0.079568,
     "end_time": "2020-10-30T18:27:22.937772",
     "exception": false,
     "start_time": "2020-10-30T18:27:22.858204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>content_type_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "      <th>lecture_id</th>\n",
       "      <th>tag</th>\n",
       "      <th>...</th>\n",
       "      <th>part_2</th>\n",
       "      <th>part_3</th>\n",
       "      <th>part_4</th>\n",
       "      <th>part_5</th>\n",
       "      <th>part_6</th>\n",
       "      <th>part_7</th>\n",
       "      <th>type_of_concept</th>\n",
       "      <th>type_of_intention</th>\n",
       "      <th>type_of_solving_question</th>\n",
       "      <th>type_of_starter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>653762</td>\n",
       "      <td>2746</td>\n",
       "      <td>6808</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>6808</td>\n",
       "      <td>129</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10183847</td>\n",
       "      <td>5382</td>\n",
       "      <td>16736</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>16736</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1424348597</td>\n",
       "      <td>5382</td>\n",
       "      <td>30207</td>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>30207</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1425557777</td>\n",
       "      <td>5382</td>\n",
       "      <td>18545</td>\n",
       "      <td>1</td>\n",
       "      <td>121</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>18545</td>\n",
       "      <td>58</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>405813029</td>\n",
       "      <td>8623</td>\n",
       "      <td>10540</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>10540</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    timestamp  user_id  content_id  content_type_id  task_container_id  \\\n",
       "0      653762     2746        6808                1                 14   \n",
       "1    10183847     5382       16736                1                 21   \n",
       "2  1424348597     5382       30207                1                104   \n",
       "3  1425557777     5382       18545                1                121   \n",
       "4   405813029     8623       10540                1                 59   \n",
       "\n",
       "   answered_correctly  prior_question_elapsed_time  \\\n",
       "0                  -1                          NaN   \n",
       "1                  -1                          NaN   \n",
       "2                  -1                          NaN   \n",
       "3                  -1                          NaN   \n",
       "4                  -1                          NaN   \n",
       "\n",
       "   prior_question_had_explanation  lecture_id  tag  ...  part_2  part_3  \\\n",
       "0                           False        6808  129  ...       1       0   \n",
       "1                           False       16736   40  ...       0       0   \n",
       "2                           False       30207   43  ...       0       0   \n",
       "3                           False       18545   58  ...       0       0   \n",
       "4                           False       10540   99  ...       0       0   \n",
       "\n",
       "   part_4  part_5  part_6  part_7  type_of_concept  type_of_intention  \\\n",
       "0       0       0       0       0                0                  1   \n",
       "1       0       0       0       0                1                  0   \n",
       "2       0       1       0       0                1                  0   \n",
       "3       0       1       0       0                1                  0   \n",
       "4       0       0       0       0                1                  0   \n",
       "\n",
       "   type_of_solving_question  type_of_starter  \n",
       "0                         0                0  \n",
       "1                         0                0  \n",
       "2                         0                0  \n",
       "3                         0                0  \n",
       "4                         0                0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lectures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 0.262435,
     "end_time": "2020-10-30T18:27:23.253304",
     "exception": false,
     "start_time": "2020-10-30T18:27:22.990869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# collect per user stats\n",
    "user_lecture_stats_part = train_lectures.groupby('user_id')[part_lectures_columns + types_of_lectures_columns].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 0.065649,
     "end_time": "2020-10-30T18:27:23.370185",
     "exception": false,
     "start_time": "2020-10-30T18:27:23.304536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>part_1</th>\n",
       "      <th>part_2</th>\n",
       "      <th>part_3</th>\n",
       "      <th>part_4</th>\n",
       "      <th>part_5</th>\n",
       "      <th>part_6</th>\n",
       "      <th>part_7</th>\n",
       "      <th>type_of_concept</th>\n",
       "      <th>type_of_intention</th>\n",
       "      <th>type_of_solving_question</th>\n",
       "      <th>type_of_starter</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5382</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8623</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12741</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13134</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         part_1  part_2  part_3  part_4  part_5  part_6  part_7  \\\n",
       "user_id                                                           \n",
       "2746          0       1       0       0       0       0       0   \n",
       "5382          1       0       0       0       2       0       0   \n",
       "8623          2       1       0       0       0       0       0   \n",
       "12741         0       0       0       3       0       1       2   \n",
       "13134         1       3       0       0       3       0       0   \n",
       "\n",
       "         type_of_concept  type_of_intention  type_of_solving_question  \\\n",
       "user_id                                                                 \n",
       "2746                   0                  1                         0   \n",
       "5382                   3                  0                         0   \n",
       "8623                   3                  0                         0   \n",
       "12741                  4                  0                         2   \n",
       "13134                  6                  1                         0   \n",
       "\n",
       "         type_of_starter  \n",
       "user_id                   \n",
       "2746                   0  \n",
       "5382                   0  \n",
       "8623                   0  \n",
       "12741                  0  \n",
       "13134                  0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_lecture_stats_part.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 0.073265,
     "end_time": "2020-10-30T18:27:23.494840",
     "exception": false,
     "start_time": "2020-10-30T18:27:23.421575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add boolean features\n",
    "for column in user_lecture_stats_part.columns:\n",
    "    bool_column = column + '_boolean'\n",
    "    user_lecture_stats_part[bool_column] = (user_lecture_stats_part[column] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "papermill": {
     "duration": 0.074029,
     "end_time": "2020-10-30T18:27:23.619515",
     "exception": false,
     "start_time": "2020-10-30T18:27:23.545486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>part_1</th>\n",
       "      <th>part_2</th>\n",
       "      <th>part_3</th>\n",
       "      <th>part_4</th>\n",
       "      <th>part_5</th>\n",
       "      <th>part_6</th>\n",
       "      <th>part_7</th>\n",
       "      <th>type_of_concept</th>\n",
       "      <th>type_of_intention</th>\n",
       "      <th>type_of_solving_question</th>\n",
       "      <th>...</th>\n",
       "      <th>part_2_boolean</th>\n",
       "      <th>part_3_boolean</th>\n",
       "      <th>part_4_boolean</th>\n",
       "      <th>part_5_boolean</th>\n",
       "      <th>part_6_boolean</th>\n",
       "      <th>part_7_boolean</th>\n",
       "      <th>type_of_concept_boolean</th>\n",
       "      <th>type_of_intention_boolean</th>\n",
       "      <th>type_of_solving_question_boolean</th>\n",
       "      <th>type_of_starter_boolean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5382</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8623</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12741</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13134</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         part_1  part_2  part_3  part_4  part_5  part_6  part_7  \\\n",
       "user_id                                                           \n",
       "2746          0       1       0       0       0       0       0   \n",
       "5382          1       0       0       0       2       0       0   \n",
       "8623          2       1       0       0       0       0       0   \n",
       "12741         0       0       0       3       0       1       2   \n",
       "13134         1       3       0       0       3       0       0   \n",
       "\n",
       "         type_of_concept  type_of_intention  type_of_solving_question  ...  \\\n",
       "user_id                                                                ...   \n",
       "2746                   0                  1                         0  ...   \n",
       "5382                   3                  0                         0  ...   \n",
       "8623                   3                  0                         0  ...   \n",
       "12741                  4                  0                         2  ...   \n",
       "13134                  6                  1                         0  ...   \n",
       "\n",
       "         part_2_boolean  part_3_boolean  part_4_boolean  part_5_boolean  \\\n",
       "user_id                                                                   \n",
       "2746                  1               0               0               0   \n",
       "5382                  0               0               0               1   \n",
       "8623                  1               0               0               0   \n",
       "12741                 0               0               1               0   \n",
       "13134                 1               0               0               1   \n",
       "\n",
       "         part_6_boolean  part_7_boolean  type_of_concept_boolean  \\\n",
       "user_id                                                            \n",
       "2746                  0               0                        0   \n",
       "5382                  0               0                        1   \n",
       "8623                  0               0                        1   \n",
       "12741                 1               1                        1   \n",
       "13134                 0               0                        1   \n",
       "\n",
       "         type_of_intention_boolean  type_of_solving_question_boolean  \\\n",
       "user_id                                                                \n",
       "2746                             1                                 0   \n",
       "5382                             0                                 0   \n",
       "8623                             0                                 0   \n",
       "12741                            0                                 1   \n",
       "13134                            1                                 0   \n",
       "\n",
       "         type_of_starter_boolean  \n",
       "user_id                           \n",
       "2746                           0  \n",
       "5382                           0  \n",
       "8623                           0  \n",
       "12741                          0  \n",
       "13134                          0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_lecture_stats_part.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 0.05761,
     "end_time": "2020-10-30T18:27:23.728836",
     "exception": false,
     "start_time": "2020-10-30T18:27:23.671226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#clearing memory\n",
    "del(train_lectures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.050874,
     "end_time": "2020-10-30T18:27:23.830884",
     "exception": false,
     "start_time": "2020-10-30T18:27:23.780010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Affirmatives (True) for content_type_id are only for those with a different type of content (lectures). These are not real questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 31.315981,
     "end_time": "2020-10-30T18:27:55.198084",
     "exception": false,
     "start_time": "2020-10-30T18:27:23.882103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#removing True or 1 for content_type_id\n",
    "\n",
    "train = train[train.content_type_id == False].sort_values('timestamp').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": 0.255707,
     "end_time": "2020-10-30T18:27:55.506963",
     "exception": false,
     "start_time": "2020-10-30T18:27:55.251256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>content_type_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1786449</th>\n",
       "      <td>24925609370</td>\n",
       "      <td>39919444</td>\n",
       "      <td>7979</td>\n",
       "      <td>0</td>\n",
       "      <td>9999</td>\n",
       "      <td>1</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847134</th>\n",
       "      <td>31352567848</td>\n",
       "      <td>40224694</td>\n",
       "      <td>5316</td>\n",
       "      <td>0</td>\n",
       "      <td>9999</td>\n",
       "      <td>1</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           timestamp   user_id  content_id  content_type_id  \\\n",
       "1786449  24925609370  39919444        7979                0   \n",
       "1847134  31352567848  40224694        5316                0   \n",
       "\n",
       "         task_container_id  answered_correctly  prior_question_elapsed_time  \\\n",
       "1786449               9999                   1                      19000.0   \n",
       "1847134               9999                   1                       8000.0   \n",
       "\n",
       "         prior_question_had_explanation  \n",
       "1786449                            True  \n",
       "1847134                            True  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[(train.task_container_id == 9999)].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "papermill": {
     "duration": 5.37707,
     "end_time": "2020-10-30T18:28:00.936721",
     "exception": false,
     "start_time": "2020-10-30T18:27:55.559651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[(train.content_type_id == False)].task_container_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "papermill": {
     "duration": 0.199683,
     "end_time": "2020-10-30T18:28:01.191474",
     "exception": false,
     "start_time": "2020-10-30T18:28:00.991791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#saving value to fillna\n",
    "elapsed_mean = train.prior_question_elapsed_time.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "papermill": {
     "duration": 75.482206,
     "end_time": "2020-10-30T18:29:16.727348",
     "exception": false,
     "start_time": "2020-10-30T18:28:01.245142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "group1 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['count'])\n",
    "group1.columns = ['avg_questions']\n",
    "group2 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['nunique'])\n",
    "group2.columns = ['avg_questions']\n",
    "group3 = group1 / group2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "papermill": {
     "duration": 0.067807,
     "end_time": "2020-10-30T18:29:16.871165",
     "exception": false,
     "start_time": "2020-10-30T18:29:16.803358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "group3['avg_questions_seen'] = group3.avg_questions.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "papermill": {
     "duration": 0.09889,
     "end_time": "2020-10-30T18:29:17.040119",
     "exception": false,
     "start_time": "2020-10-30T18:29:16.941229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0062272963155163"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group3.iloc[0].avg_questions_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "papermill": {
     "duration": 34.77285,
     "end_time": "2020-10-30T18:29:51.888845",
     "exception": false,
     "start_time": "2020-10-30T18:29:17.115995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_u_final = train.loc[train.content_type_id == False, ['user_id','answered_correctly']].groupby(['user_id']).agg(['mean','sum', 'count'])\n",
    "results_u_final.columns = ['answered_correctly_user', 'sum_correct', 'count_sum']\n",
    "\n",
    "results_c_final = train.loc[train.content_type_id == False, ['content_id','answered_correctly']].groupby(['content_id']).agg(['mean'])\n",
    "results_c_final.columns = [\"answered_correctly_content_mean\"]\n",
    "\n",
    "results_u2_final = train.loc[train.content_type_id == False, ['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\n",
    "results_u2_final.columns = ['explanation_mean_user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "papermill": {
     "duration": 0.117058,
     "end_time": "2020-10-30T18:29:52.088413",
     "exception": false,
     "start_time": "2020-10-30T18:29:51.971355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    7709.000000\n",
       "mean        0.565149\n",
       "std         0.360969\n",
       "min         0.000000\n",
       "25%         0.230769\n",
       "50%         0.650000\n",
       "75%         0.906061\n",
       "max         1.000000\n",
       "Name: explanation_mean_user, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_u2_final.explanation_mean_user.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "papermill": {
     "duration": 18.230328,
     "end_time": "2020-10-30T18:30:10.373990",
     "exception": false,
     "start_time": "2020-10-30T18:29:52.143662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "papermill": {
     "duration": 17.478391,
     "end_time": "2020-10-30T18:30:27.907861",
     "exception": false,
     "start_time": "2020-10-30T18:30:10.429470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_q_final = train.loc[train.content_type_id == False, ['question_id','answered_correctly']].groupby(['question_id']).agg(['mean'])\n",
    "results_q_final.columns = ['quest_pct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "papermill": {
     "duration": 15.787128,
     "end_time": "2020-10-30T18:30:43.757431",
     "exception": false,
     "start_time": "2020-10-30T18:30:27.970303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_q2_final = train.loc[train.content_type_id == False, ['question_id','part']].groupby(['question_id']).agg(['count'])\n",
    "results_q2_final.columns = ['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "papermill": {
     "duration": 0.104144,
     "end_time": "2020-10-30T18:30:43.920829",
     "exception": false,
     "start_time": "2020-10-30T18:30:43.816685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "question2 = pd.merge(questions_df, results_q_final, left_on = 'question_id', right_on = 'question_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "papermill": {
     "duration": 0.103089,
     "end_time": "2020-10-30T18:30:44.131450",
     "exception": false,
     "start_time": "2020-10-30T18:30:44.028361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "question2 = pd.merge(question2, results_q2_final, left_on = 'question_id', right_on = 'question_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "papermill": {
     "duration": 0.094251,
     "end_time": "2020-10-30T18:30:44.316358",
     "exception": false,
     "start_time": "2020-10-30T18:30:44.222107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "question2.quest_pct = round(question2.quest_pct,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "papermill": {
     "duration": 0.134043,
     "end_time": "2020-10-30T18:30:44.533042",
     "exception": false,
     "start_time": "2020-10-30T18:30:44.398999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>part</th>\n",
       "      <th>quest_pct</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.89474</td>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92667</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.55386</td>\n",
       "      <td>919.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.79452</td>\n",
       "      <td>438.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.64154</td>\n",
       "      <td>650.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_id  part  quest_pct  count\n",
       "0            0     1    0.89474  152.0\n",
       "1            1     1    0.92667  150.0\n",
       "2            2     1    0.55386  919.0\n",
       "3            3     1    0.79452  438.0\n",
       "4            4     1    0.64154  650.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>part</th>\n",
       "      <th>quest_pct</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13518</th>\n",
       "      <td>13518</td>\n",
       "      <td>5</td>\n",
       "      <td>0.76471</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13519</th>\n",
       "      <td>13519</td>\n",
       "      <td>5</td>\n",
       "      <td>0.47059</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13520</th>\n",
       "      <td>13520</td>\n",
       "      <td>5</td>\n",
       "      <td>0.70588</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13521</th>\n",
       "      <td>13521</td>\n",
       "      <td>5</td>\n",
       "      <td>0.66667</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13522</th>\n",
       "      <td>13522</td>\n",
       "      <td>5</td>\n",
       "      <td>0.82353</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       question_id  part  quest_pct  count\n",
       "13518        13518     5    0.76471   17.0\n",
       "13519        13519     5    0.47059   17.0\n",
       "13520        13520     5    0.70588   17.0\n",
       "13521        13521     5    0.66667   21.0\n",
       "13522        13522     5    0.82353   17.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(question2.head(), question2.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "papermill": {
     "duration": 0.096861,
     "end_time": "2020-10-30T18:30:44.704696",
     "exception": false,
     "start_time": "2020-10-30T18:30:44.607835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>content_type_id</th>\n",
       "      <th>task_container_id</th>\n",
       "      <th>answered_correctly</th>\n",
       "      <th>prior_question_elapsed_time</th>\n",
       "      <th>prior_question_had_explanation</th>\n",
       "      <th>question_id</th>\n",
       "      <th>part</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>5692</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5692</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>39068636</td>\n",
       "      <td>7900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>7900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2121898</td>\n",
       "      <td>5556</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5556</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>11460655</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>11459815</td>\n",
       "      <td>7900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>7900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp   user_id  content_id  content_type_id  task_container_id  \\\n",
       "0          0       115        5692                0                  1   \n",
       "1          0  39068636        7900                0                  0   \n",
       "2          0   2121898        5556                0                  0   \n",
       "3          0  11460655         128                0                  0   \n",
       "4          0  11459815        7900                0                  0   \n",
       "\n",
       "   answered_correctly  prior_question_elapsed_time  \\\n",
       "0                   1                          NaN   \n",
       "1                   1                          NaN   \n",
       "2                   1                          NaN   \n",
       "3                   1                          NaN   \n",
       "4                   1                          NaN   \n",
       "\n",
       "   prior_question_had_explanation  question_id  part  \n",
       "0                            <NA>         5692     5  \n",
       "1                            <NA>         7900     1  \n",
       "2                            <NA>         5556     5  \n",
       "3                            <NA>          128     1  \n",
       "4                            <NA>         7900     1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "papermill": {
     "duration": 0.092536,
     "end_time": "2020-10-30T18:30:44.870352",
     "exception": false,
     "start_time": "2020-10-30T18:30:44.777816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1961017"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.092857,
     "end_time": "2020-10-30T18:30:45.040685",
     "exception": false,
     "start_time": "2020-10-30T18:30:44.947828",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Exploration ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "papermill": {
     "duration": 0.067389,
     "end_time": "2020-10-30T18:30:45.172623",
     "exception": false,
     "start_time": "2020-10-30T18:30:45.105234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1961017"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "papermill": {
     "duration": 0.134113,
     "end_time": "2020-10-30T18:30:45.403968",
     "exception": false,
     "start_time": "2020-10-30T18:30:45.269855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6544859121568044"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.answered_correctly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "papermill": {
     "duration": 0.065947,
     "end_time": "2020-10-30T18:30:45.528816",
     "exception": false,
     "start_time": "2020-10-30T18:30:45.462869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prior_mean_user = results_u2_final.explanation_mean_user.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "papermill": {
     "duration": 0.197254,
     "end_time": "2020-10-30T18:30:45.785069",
     "exception": false,
     "start_time": "2020-10-30T18:30:45.587815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6812790097988654"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[(train.timestamp == 0)].answered_correctly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "papermill": {
     "duration": 4.394188,
     "end_time": "2020-10-30T18:30:50.238758",
     "exception": false,
     "start_time": "2020-10-30T18:30:45.844570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.654379522245107"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[(train.timestamp != 0)].answered_correctly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "papermill": {
     "duration": 0.863208,
     "end_time": "2020-10-30T18:30:51.162901",
     "exception": false,
     "start_time": "2020-10-30T18:30:50.299693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.drop(['timestamp', 'content_type_id', 'question_id', 'part'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "papermill": {
     "duration": 0.071352,
     "end_time": "2020-10-30T18:30:51.296928",
     "exception": false,
     "start_time": "2020-10-30T18:30:51.225576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1961017"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.059855,
     "end_time": "2020-10-30T18:30:51.419973",
     "exception": false,
     "start_time": "2020-10-30T18:30:51.360118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating Validation Set (Most Recent Answers by User) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "papermill": {
     "duration": 22.95362,
     "end_time": "2020-10-30T18:31:14.559520",
     "exception": false,
     "start_time": "2020-10-30T18:30:51.605900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1961017"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = train.groupby('user_id').tail(5)\n",
    "train = train[~train.index.isin(validation.index)]\n",
    "len(train) + len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "papermill": {
     "duration": 0.072255,
     "end_time": "2020-10-30T18:31:14.693765",
     "exception": false,
     "start_time": "2020-10-30T18:31:14.621510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5400544959128065"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.answered_correctly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "papermill": {
     "duration": 0.135666,
     "end_time": "2020-10-30T18:31:14.891218",
     "exception": false,
     "start_time": "2020-10-30T18:31:14.755552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6567796213436589"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.answered_correctly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "papermill": {
     "duration": 17.675045,
     "end_time": "2020-10-30T18:31:32.628820",
     "exception": false,
     "start_time": "2020-10-30T18:31:14.953775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_u_val = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean','sum', 'count'])\n",
    "results_u_val.columns = ['answered_correctly_user','sum_correct', 'count_sum']\n",
    "\n",
    "results_c_val = train[['content_id','answered_correctly']].groupby(['content_id']).agg(['mean'])\n",
    "results_c_val.columns = [\"answered_correctly_content_mean\"]\n",
    "\n",
    "results_u2_val = train[['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\n",
    "results_u2_val.columns = ['explanation_mean_user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.062246,
     "end_time": "2020-10-30T18:31:32.753560",
     "exception": false,
     "start_time": "2020-10-30T18:31:32.691314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Does it make sense to use last questions as validation? Why is the rate of correct answers so low?\n",
    "I am convinced there is a better way to match the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.062267,
     "end_time": "2020-10-30T18:31:32.878153",
     "exception": false,
     "start_time": "2020-10-30T18:31:32.815886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Extracting Training Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "papermill": {
     "duration": 24.129633,
     "end_time": "2020-10-30T18:31:57.069659",
     "exception": false,
     "start_time": "2020-10-30T18:31:32.940026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1961017"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train.groupby('user_id').tail(18)\n",
    "train = train[~train.index.isin(X.index)]\n",
    "len(X) + len(train) + len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "papermill": {
     "duration": 0.077582,
     "end_time": "2020-10-30T18:31:57.210279",
     "exception": false,
     "start_time": "2020-10-30T18:31:57.132697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5514405200868873"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.answered_correctly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "papermill": {
     "duration": 0.134154,
     "end_time": "2020-10-30T18:31:57.408393",
     "exception": false,
     "start_time": "2020-10-30T18:31:57.274239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6643792185571621"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.answered_correctly.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "papermill": {
     "duration": 16.135509,
     "end_time": "2020-10-30T18:32:13.609055",
     "exception": false,
     "start_time": "2020-10-30T18:31:57.473546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_u_X = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean','sum', 'count'])\n",
    "results_u_X.columns = ['answered_correctly_user','sum_correct', 'count_sum']\n",
    "\n",
    "results_c_X = train[['content_id','answered_correctly']].groupby(['content_id']).agg(['mean'])\n",
    "results_c_X.columns = [\"answered_correctly_content_mean\"]\n",
    "\n",
    "results_u2_X = train[['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\n",
    "results_u2_X.columns = ['explanation_mean_user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.066644,
     "end_time": "2020-10-30T18:32:13.743591",
     "exception": false,
     "start_time": "2020-10-30T18:32:13.676947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Merging Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "papermill": {
     "duration": 0.182485,
     "end_time": "2020-10-30T18:32:13.993486",
     "exception": false,
     "start_time": "2020-10-30T18:32:13.811001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#clearing memory\n",
    "del(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "papermill": {
     "duration": 4.357752,
     "end_time": "2020-10-30T18:32:18.419534",
     "exception": false,
     "start_time": "2020-10-30T18:32:14.061782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = pd.merge(X, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\n",
    "X = pd.merge(X, results_u_X, on=['user_id'], how=\"left\")\n",
    "X = pd.merge(X, results_u2_X, on=['user_id'], how=\"left\")\n",
    "X = pd.merge(X, results_c_X, on=['content_id'], how=\"left\")\n",
    "X = pd.merge(X, user_lecture_stats_part, on=['user_id'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "papermill": {
     "duration": 1.357739,
     "end_time": "2020-10-30T18:32:19.841886",
     "exception": false,
     "start_time": "2020-10-30T18:32:18.484147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation = pd.merge(validation, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\n",
    "validation = pd.merge(validation, results_u_val, on=['user_id'], how=\"left\")\n",
    "validation = pd.merge(validation, results_u2_val, on=['user_id'], how=\"left\")\n",
    "validation = pd.merge(validation, results_c_val, on=['content_id'], how=\"left\")\n",
    "validation = pd.merge(validation, user_lecture_stats_part, on=['user_id'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "papermill": {
     "duration": 2.891219,
     "end_time": "2020-10-30T18:32:22.798219",
     "exception": false,
     "start_time": "2020-10-30T18:32:19.907000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "\n",
    "X.prior_question_had_explanation.fillna(False, inplace = True)\n",
    "validation.prior_question_had_explanation.fillna(False, inplace = True)\n",
    "\n",
    "validation[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(validation[\"prior_question_had_explanation\"])\n",
    "X[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(X[\"prior_question_had_explanation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "papermill": {
     "duration": 0.117061,
     "end_time": "2020-10-30T18:32:23.019768",
     "exception": false,
     "start_time": "2020-10-30T18:32:22.902707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reading in question df\n",
    "#question2 = pd.read_csv('/kaggle/input/question2/question2.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "papermill": {
     "duration": 0.107573,
     "end_time": "2020-10-30T18:32:23.223490",
     "exception": false,
     "start_time": "2020-10-30T18:32:23.115917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7022135696126812"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_mean = question2.quest_pct.mean()\n",
    "\n",
    "question2.quest_pct.mean()\n",
    "#there are a lot of high percentage questions, should use median instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "papermill": {
     "duration": 0.133191,
     "end_time": "2020-10-30T18:32:23.450587",
     "exception": false,
     "start_time": "2020-10-30T18:32:23.317396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#filling questions with no info with a new value\n",
    "question2.quest_pct = question2.quest_pct.mask((question2['count'] < 3), .65)\n",
    "\n",
    "\n",
    "#filling very hard new questions with a more reasonable value\n",
    "question2.quest_pct = question2.quest_pct.mask((question2.quest_pct < .2) & (question2['count'] < 21), .2)\n",
    "\n",
    "#filling very easy new questions with a more reasonable value\n",
    "question2.quest_pct = question2.quest_pct.mask((question2.quest_pct > .95) & (question2['count'] < 21), .95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "papermill": {
     "duration": 2.658176,
     "end_time": "2020-10-30T18:32:26.206787",
     "exception": false,
     "start_time": "2020-10-30T18:32:23.548611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = pd.merge(X, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "validation = pd.merge(validation, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "X.part = X.part - 1\n",
    "validation.part = validation.part - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "papermill": {
     "duration": 0.099169,
     "end_time": "2020-10-30T18:32:26.372357",
     "exception": false,
     "start_time": "2020-10-30T18:32:26.273188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 129363 entries, 0 to 129362\n",
      "Data columns (total 40 columns):\n",
      " #   Column                              Non-Null Count   Dtype  \n",
      "---  ------                              --------------   -----  \n",
      " 0   user_id                             129363 non-null  int32  \n",
      " 1   content_id                          129363 non-null  int16  \n",
      " 2   task_container_id                   129363 non-null  int16  \n",
      " 3   answered_correctly                  129363 non-null  int8   \n",
      " 4   prior_question_elapsed_time         127893 non-null  float32\n",
      " 5   prior_question_had_explanation      129363 non-null  boolean\n",
      " 6   avg_questions                       129363 non-null  float64\n",
      " 7   avg_questions_seen                  129363 non-null  float64\n",
      " 8   answered_correctly_user             112194 non-null  float64\n",
      " 9   sum_correct                         112194 non-null  float64\n",
      " 10  count_sum                           112194 non-null  float64\n",
      " 11  explanation_mean_user               111600 non-null  float64\n",
      " 12  answered_correctly_content_mean     129355 non-null  float64\n",
      " 13  part_1                              52744 non-null   float64\n",
      " 14  part_2                              52744 non-null   float64\n",
      " 15  part_3                              52744 non-null   float64\n",
      " 16  part_4                              52744 non-null   float64\n",
      " 17  part_5                              52744 non-null   float64\n",
      " 18  part_6                              52744 non-null   float64\n",
      " 19  part_7                              52744 non-null   float64\n",
      " 20  type_of_concept                     52744 non-null   float64\n",
      " 21  type_of_intention                   52744 non-null   float64\n",
      " 22  type_of_solving_question            52744 non-null   float64\n",
      " 23  type_of_starter                     52744 non-null   float64\n",
      " 24  part_1_boolean                      52744 non-null   float64\n",
      " 25  part_2_boolean                      52744 non-null   float64\n",
      " 26  part_3_boolean                      52744 non-null   float64\n",
      " 27  part_4_boolean                      52744 non-null   float64\n",
      " 28  part_5_boolean                      52744 non-null   float64\n",
      " 29  part_6_boolean                      52744 non-null   float64\n",
      " 30  part_7_boolean                      52744 non-null   float64\n",
      " 31  type_of_concept_boolean             52744 non-null   float64\n",
      " 32  type_of_intention_boolean           52744 non-null   float64\n",
      " 33  type_of_solving_question_boolean    52744 non-null   float64\n",
      " 34  type_of_starter_boolean             52744 non-null   float64\n",
      " 35  prior_question_had_explanation_enc  129363 non-null  int64  \n",
      " 36  question_id                         129363 non-null  int16  \n",
      " 37  part                                129363 non-null  int8   \n",
      " 38  quest_pct                           129363 non-null  float64\n",
      " 39  count                               129363 non-null  float64\n",
      "dtypes: boolean(1), float32(1), float64(31), int16(3), int32(1), int64(1), int8(2)\n",
      "memory usage: 34.8 MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "papermill": {
     "duration": 2.030403,
     "end_time": "2020-10-30T18:32:28.475400",
     "exception": false,
     "start_time": "2020-10-30T18:32:26.444997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = X['answered_correctly']\n",
    "X = X.drop(['answered_correctly'], axis=1)\n",
    "X.head()\n",
    "\n",
    "y_val = validation['answered_correctly']\n",
    "X_val = validation.drop(['answered_correctly'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "papermill": {
     "duration": 0.702908,
     "end_time": "2020-10-30T18:32:29.246464",
     "exception": false,
     "start_time": "2020-10-30T18:32:28.543556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "features_map = ['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n",
    "       'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part',\n",
    "       'part_1', 'part_2', 'part_3', 'part_4', 'part_5', 'part_6', 'part_7',\n",
    "       'type_of_concept', 'type_of_intention', 'type_of_solving_question', 'type_of_starter',\n",
    "       'part_1_boolean', 'part_2_boolean', 'part_3_boolean', 'part_4_boolean', 'part_5_boolean', 'part_6_boolean', 'part_7_boolean',\n",
    "       'type_of_concept_boolean', 'type_of_intention_boolean', 'type_of_solving_question_boolean', 'type_of_starter_boolean']\n",
    "'''\n",
    "\n",
    "features_map = ['answered_correctly_user', 'explanation_mean_user', 'quest_pct','answered_correctly_content_mean','count_sum',\n",
    "                'prior_question_had_explanation_enc','type_of_solving_question','type_of_solving_question_boolean'\n",
    "                , 'type_of_starter', 'type_of_starter_boolean', 'type_of_concept', 'type_of_concept_boolean']\n",
    "\n",
    "\n",
    "X = X[features_map]\n",
    "X_val = X_val[features_map]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "papermill": {
     "duration": 0.628645,
     "end_time": "2020-10-30T18:32:29.943345",
     "exception": false,
     "start_time": "2020-10-30T18:32:29.314700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Filling with 0.5 for simplicity; there could likely be a better value\n",
    "if 'answered_correctly_user' in features_map:\n",
    "    X['answered_correctly_user'].fillna(0.65,  inplace=True)\n",
    "\n",
    "if 'answered_correctly_content_mean' in features_map:\n",
    "    X['answered_correctly_content_mean'].fillna(0.5,  inplace=True)\n",
    "\n",
    "if 'sum_correct' in features_map:\n",
    "    X['sum_correct'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'count_sum' in features_map:\n",
    "    X['count_sum'].fillna(0, inplace = True)\n",
    "\n",
    "    \n",
    "if 'explanation_mean_user' in features_map:\n",
    "    X['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\n",
    "\n",
    "if 'quest_pct' in features_map:\n",
    "    X['quest_pct'].fillna(content_mean, inplace=True)\n",
    "\n",
    "if 'part' in features_map:\n",
    "    X['part'].fillna(4, inplace = True)\n",
    "\n",
    "if 'avg_questions_seen' in features_map:\n",
    "    X['avg_questions_seen'].fillna(1, inplace = True)\n",
    "\n",
    "if 'prior_question_elapsed_time' in features_map:\n",
    "    X['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\n",
    "\n",
    "if 'prior_question_had_explanation_enc' in features_map:\n",
    "    X['prior_question_had_explanation_enc'].fillna(0, inplace = True)\n",
    "\n",
    "if 'part_1' in features_map:\n",
    "    X['part_1'].fillna(0, inplace = True)\n",
    "\n",
    "if 'part_2' in features_map:\n",
    "    X['part_2'].fillna(0, inplace = True)\n",
    "\n",
    "if 'part_3' in features_map:\n",
    "    X['part_3'].fillna(0, inplace = True)\n",
    "\n",
    "if 'part_4' in features_map:\n",
    "    X['part_4'].fillna(0, inplace = True)\n",
    "\n",
    "if 'part_5' in features_map:\n",
    "    X['part_5'].fillna(0, inplace = True)\n",
    "\n",
    "if 'part_6' in features_map:\n",
    "    X['part_6'].fillna(0, inplace = True)\n",
    "\n",
    "if 'part_7' in features_map:\n",
    "    X['part_7'].fillna(0, inplace = True)\n",
    "\n",
    "if 'type_of_concept' in features_map:\n",
    "    X['type_of_concept'].fillna(0, inplace = True)\n",
    "\n",
    "if 'type_of_intention' in features_map:\n",
    "    X['type_of_intention'].fillna(0, inplace = True)\n",
    "\n",
    "if 'type_of_solving_question' in features_map:\n",
    "    X['type_of_solving_question'].fillna(0, inplace = True)\n",
    "\n",
    "if 'type_of_starter' in features_map:\n",
    "    X['type_of_starter'].fillna(0, inplace = True)\n",
    "\n",
    "if 'part_1_boolean' in features_map:\n",
    "    X['part_1_boolean'].fillna(0, inplace = True)\n",
    "\n",
    "if 'part_2_boolean' in features_map:\n",
    "    X['part_2_boolean'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'part_3_boolean' in features_map: \n",
    "    X['part_3_boolean'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'part_4_boolean' in features_map: \n",
    "    X['part_4_boolean'].fillna(0, inplace = True)\n",
    "\n",
    "if 'part_5_boolean' in features_map: \n",
    "    X['part_5_boolean'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'part_6_boolean' in features_map: \n",
    "    X['part_6_boolean'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'part_7_boolean' in features_map: \n",
    "    X['part_7_boolean'].fillna(0, inplace = True)\n",
    "\n",
    "if 'type_of_concept_boolean' in features_map: \n",
    "    X['type_of_concept_boolean'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'type_of_intention_boolean' in features_map: \n",
    "    X['type_of_intention_boolean'].fillna(0, inplace = True)\n",
    "\n",
    "if 'type_of_solving_question_boolean' in features_map: \n",
    "    X['type_of_solving_question_boolean'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'type_of_starter_boolean' in features_map: \n",
    "    X['type_of_starter_boolean'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "papermill": {
     "duration": 0.25411,
     "end_time": "2020-10-30T18:32:30.266506",
     "exception": false,
     "start_time": "2020-10-30T18:32:30.012396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'answered_correctly_user' in features_map: \n",
    "    X_val['answered_correctly_user'].fillna(0.65,  inplace=True)\n",
    "    \n",
    "if 'answered_correctly_content_mean' in features_map:\n",
    "    X_val['answered_correctly_content_mean'].fillna(0.5,  inplace=True)\n",
    "    \n",
    "if 'sum_correct' in features_map:\n",
    "    X_val['sum_correct'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'count_sum' in features_map:\n",
    "    X_val['count_sum'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'explanation_mean_user' in features_map: \n",
    "    X_val['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\n",
    "    \n",
    "if 'quest_pct' in features_map: \n",
    "    X_val['quest_pct'].fillna(content_mean,  inplace=True)\n",
    "\n",
    "if 'part' in features_map: \n",
    "    X_val['part'].fillna(4, inplace = True)\n",
    "    \n",
    "if 'avg_questions_seen' in features_map: \n",
    "    X_val['avg_questions_seen'].fillna(1, inplace = True)\n",
    "    \n",
    "if 'prior_question_elapsed_time' in features_map: \n",
    "    X_val['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\n",
    "    \n",
    "if 'prior_question_had_explanation_enc' in features_map: \n",
    "    X_val['prior_question_had_explanation_enc'].fillna(0, inplace = True)\n",
    "\n",
    "if 'part_1' in features_map: \n",
    "    X_val['part_1'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'part_2' in features_map: \n",
    "    X_val['part_2'].fillna(0, inplace = True)\n",
    "\n",
    "if 'part_3' in features_map: \n",
    "    X_val['part_3'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'part_4' in features_map: \n",
    "    X_val['part_4'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'part_5' in features_map: \n",
    "    X_val['part_5'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'part_6' in features_map: \n",
    "    X_val['part_6'].fillna(0, inplace = True)\n",
    "\n",
    "if 'part_7' in features_map: \n",
    "    X_val['part_7'].fillna(0, inplace = True)\n",
    "\n",
    "if 'type_of_concept' in features_map: \n",
    "    X_val['type_of_concept'].fillna(0, inplace = True)\n",
    "\n",
    "if 'type_of_intention' in features_map: \n",
    "    X_val['type_of_intention'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'type_of_solving_question' in features_map: \n",
    "    X_val['type_of_solving_question'].fillna(0, inplace = True)\n",
    "\n",
    "if 'type_of_starter' in features_map: \n",
    "    X_val['type_of_starter'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'part_1_boolean' in features_map: \n",
    "    X_val['part_1_boolean'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'part_2_boolean' in features_map: \n",
    "    X_val['part_2_boolean'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'part_3_boolean' in features_map: \n",
    "    X_val['part_3_boolean'].fillna(0, inplace = True)\n",
    "\n",
    "if 'part_4_boolean' in features_map: \n",
    "    X_val['part_4_boolean'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'part_5_boolean' in features_map: \n",
    "    X_val['part_5_boolean'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'part_6_boolean' in features_map: \n",
    "    X_val['part_6_boolean'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'part_7_boolean' in features_map: \n",
    "    X_val['part_7_boolean'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'type_of_concept_boolean' in features_map: \n",
    "    X_val['type_of_concept_boolean'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'type_of_intention_boolean' in features_map: \n",
    "    X_val['type_of_intention_boolean'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'type_of_solving_question_boolean' in features_map: \n",
    "    X_val['type_of_solving_question_boolean'].fillna(0, inplace = True)\n",
    "    \n",
    "if 'type_of_starter_boolean' in features_map: \n",
    "    X_val['type_of_starter_boolean'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.067624,
     "end_time": "2020-10-30T18:32:30.402330",
     "exception": false,
     "start_time": "2020-10-30T18:32:30.334706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Modeling ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "papermill": {
     "duration": 3.577685,
     "end_time": "2020-10-30T18:32:34.047978",
     "exception": false,
     "start_time": "2020-10-30T18:32:30.470293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_train = X.reshape(X.shape[0], X.shape[1])\n",
    "X_test = X_val.reshape(X_val.shape[0], X_val.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129363, 12)\n",
      "cuda:0\n",
      "Train on 103490 samples, validate on 25872 samples, 102 steps per epoch\n",
      "Epoch 1/100\n",
      "2s - loss:  0.6709 - binary_crossentropy:  0.6705 - auc:  0.6893 - val_binary_crossentropy:  0.6583 - val_auc:  0.6611\n",
      "Epoch 2/100\n",
      "2s - loss:  0.6354 - binary_crossentropy:  0.6354 - auc:  0.6971 - val_binary_crossentropy:  0.6201 - val_auc:  0.6705\n",
      "Epoch 3/100\n",
      "2s - loss:  0.6283 - binary_crossentropy:  0.6281 - auc:  0.7003 - val_binary_crossentropy:  0.6170 - val_auc:  0.6800\n",
      "Epoch 4/100\n",
      "2s - loss:  0.6286 - binary_crossentropy:  0.6280 - auc:  0.7001 - val_binary_crossentropy:  0.6568 - val_auc:  0.6659\n",
      "Epoch 5/100\n",
      "2s - loss:  0.6283 - binary_crossentropy:  0.6286 - auc:  0.6994 - val_binary_crossentropy:  0.6297 - val_auc:  0.6743\n",
      "Epoch 6/100\n",
      "2s - loss:  0.6288 - binary_crossentropy:  0.6288 - auc:  0.6999 - val_binary_crossentropy:  0.6206 - val_auc:  0.6638\n",
      "Epoch 7/100\n",
      "2s - loss:  0.6282 - binary_crossentropy:  0.6286 - auc:  0.6995 - val_binary_crossentropy:  0.6194 - val_auc:  0.6685\n",
      "Epoch 8/100\n",
      "2s - loss:  0.6284 - binary_crossentropy:  0.6286 - auc:  0.6994 - val_binary_crossentropy:  0.6464 - val_auc:  0.6336\n",
      "Epoch 9/100\n",
      "2s - loss:  0.6283 - binary_crossentropy:  0.6272 - auc:  0.7016 - val_binary_crossentropy:  0.6201 - val_auc:  0.6733\n",
      "Epoch 10/100\n",
      "2s - loss:  0.6279 - binary_crossentropy:  0.6275 - auc:  0.7018 - val_binary_crossentropy:  0.6203 - val_auc:  0.6776\n",
      "Epoch 11/100\n",
      "2s - loss:  0.6275 - binary_crossentropy:  0.6274 - auc:  0.7013 - val_binary_crossentropy:  0.6173 - val_auc:  0.6747\n",
      "Epoch 12/100\n",
      "2s - loss:  0.6290 - binary_crossentropy:  0.6284 - auc:  0.7006 - val_binary_crossentropy:  0.6221 - val_auc:  0.6737\n",
      "Epoch 13/100\n",
      "2s - loss:  0.6277 - binary_crossentropy:  0.6281 - auc:  0.7005 - val_binary_crossentropy:  0.6357 - val_auc:  0.6686\n",
      "Epoch 14/100\n",
      "2s - loss:  0.6276 - binary_crossentropy:  0.6277 - auc:  0.7012 - val_binary_crossentropy:  0.6241 - val_auc:  0.6782\n",
      "Epoch 15/100\n",
      "2s - loss:  0.6279 - binary_crossentropy:  0.6278 - auc:  0.7009 - val_binary_crossentropy:  0.6187 - val_auc:  0.6782\n",
      "Epoch 16/100\n",
      "2s - loss:  0.6274 - binary_crossentropy:  0.6271 - auc:  0.7018 - val_binary_crossentropy:  0.6309 - val_auc:  0.6758\n",
      "Epoch 17/100\n",
      "2s - loss:  0.6274 - binary_crossentropy:  0.6273 - auc:  0.7013 - val_binary_crossentropy:  0.6228 - val_auc:  0.6782\n",
      "Epoch 18/100\n",
      "2s - loss:  0.6269 - binary_crossentropy:  0.6265 - auc:  0.7020 - val_binary_crossentropy:  0.6189 - val_auc:  0.6715\n",
      "Epoch 19/100\n",
      "2s - loss:  0.6271 - binary_crossentropy:  0.6276 - auc:  0.7005 - val_binary_crossentropy:  0.6185 - val_auc:  0.6799\n",
      "Epoch 20/100\n",
      "2s - loss:  0.6274 - binary_crossentropy:  0.6276 - auc:  0.7016 - val_binary_crossentropy:  0.6174 - val_auc:  0.6805\n",
      "Epoch 21/100\n",
      "2s - loss:  0.6268 - binary_crossentropy:  0.6265 - auc:  0.7023 - val_binary_crossentropy:  0.6207 - val_auc:  0.6782\n",
      "Epoch 22/100\n",
      "2s - loss:  0.6274 - binary_crossentropy:  0.6269 - auc:  0.7020 - val_binary_crossentropy:  0.6186 - val_auc:  0.6756\n",
      "Epoch 23/100\n",
      "2s - loss:  0.6280 - binary_crossentropy:  0.6281 - auc:  0.7004 - val_binary_crossentropy:  0.6258 - val_auc:  0.6564\n",
      "Epoch 24/100\n",
      "2s - loss:  0.6297 - binary_crossentropy:  0.6299 - auc:  0.6993 - val_binary_crossentropy:  0.6316 - val_auc:  0.6718\n",
      "Epoch 25/100\n",
      "2s - loss:  0.6317 - binary_crossentropy:  0.6316 - auc:  0.6980 - val_binary_crossentropy:  0.6182 - val_auc:  0.6774\n",
      "Epoch 26/100\n",
      "2s - loss:  0.6281 - binary_crossentropy:  0.6281 - auc:  0.7012 - val_binary_crossentropy:  0.6176 - val_auc:  0.6759\n",
      "Epoch 27/100\n",
      "2s - loss:  0.6272 - binary_crossentropy:  0.6274 - auc:  0.7012 - val_binary_crossentropy:  0.6180 - val_auc:  0.6793\n",
      "Epoch 28/100\n",
      "2s - loss:  0.6276 - binary_crossentropy:  0.6279 - auc:  0.7010 - val_binary_crossentropy:  0.6234 - val_auc:  0.6804\n",
      "Epoch 29/100\n",
      "2s - loss:  0.6272 - binary_crossentropy:  0.6268 - auc:  0.7028 - val_binary_crossentropy:  0.6234 - val_auc:  0.6785\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-02db55f8d53b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"auc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-61-02db55f8d53b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, validation_split, shuffle, callbacks)\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0mloss_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                         \u001b[0mtotal_loss_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/torch-15/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/torch-15/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from sklearn.metrics import *\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization,Dropout,Dense,Flatten,Conv1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import itertools    \n",
    "\n",
    "try:\n",
    "    from tensorflow.python.keras.callbacks import CallbackList\n",
    "except ImportError:\n",
    "    from tensorflow.python.keras._impl.keras.callbacks import CallbackList\n",
    "    \n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, device='cpu'):\n",
    "        super(BaseModel, self).__init__()\n",
    "        \n",
    "        # device\n",
    "        self.device = device \n",
    "        self.regularization_weight = []\n",
    "        self.to(device)\n",
    "    \n",
    "    def compile(self, optimizer,\n",
    "                loss=None,\n",
    "                metrics=None,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        :param optimizer: String (name of optimizer) or optimizer instance. See [optimizers](https://pytorch.org/docs/stable/optim.html).\n",
    "        :param loss: String (name of objective function) or objective function. See [losses](https://pytorch.org/docs/stable/nn.functional.html#loss-functions).\n",
    "        :param metrics: List of metrics to be evaluated by the model during training and testing. Typically you will use `metrics=['accuracy']`.\n",
    "        \"\"\"\n",
    "        self.metrics_names = [\"loss\"]\n",
    "        self.optim = self._get_optim(optimizer)\n",
    "        self.loss_func = self._get_loss_func(loss)\n",
    "        self.metrics = self._get_metrics(metrics)\n",
    "\n",
    "    def _get_optim(self, optimizer):\n",
    "        if isinstance(optimizer, str):\n",
    "            if optimizer == \"sgd\":\n",
    "                optim = torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "            elif optimizer == \"adam\":\n",
    "                optim = torch.optim.Adam(self.parameters())  # 0.001\n",
    "            elif optimizer == \"adagrad\":\n",
    "                optim = torch.optim.Adagrad(self.parameters())  # 0.01\n",
    "            elif optimizer == \"rmsprop\":\n",
    "                optim = torch.optim.RMSprop(self.parameters())\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            optim = optimizer\n",
    "        return optim\n",
    "\n",
    "    def _get_loss_func(self, loss):\n",
    "        if isinstance(loss, str):\n",
    "            if loss == \"binary_crossentropy\":\n",
    "                loss_func = F.binary_cross_entropy\n",
    "            elif loss == \"mse\":\n",
    "                loss_func = F.mse_loss\n",
    "            elif loss == \"mae\":\n",
    "                loss_func = F.l1_loss\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            loss_func = loss\n",
    "        return loss_func\n",
    "\n",
    "    def _log_loss(self, y_true, y_pred, eps=1e-7, normalize=True, sample_weight=None, labels=None):\n",
    "        # change eps to improve calculation accuracy\n",
    "        return log_loss(y_true,\n",
    "                        y_pred,\n",
    "                        eps,\n",
    "                        normalize,\n",
    "                        sample_weight,\n",
    "                        labels)\n",
    "\n",
    "    def _get_metrics(self, metrics, set_eps=False):\n",
    "        metrics_ = {}\n",
    "        if metrics:\n",
    "            for metric in metrics:\n",
    "                if metric == \"binary_crossentropy\" or metric == \"logloss\":\n",
    "                    if set_eps:\n",
    "                        metrics_[metric] = self._log_loss\n",
    "                    else:\n",
    "                        metrics_[metric] = log_loss\n",
    "                if metric == \"auc\":\n",
    "                    metrics_[metric] = roc_auc_score\n",
    "                if metric == \"mse\":\n",
    "                    metrics_[metric] = mean_squared_error\n",
    "                if metric == \"accuracy\" or metric == \"acc\":\n",
    "                    metrics_[metric] = lambda y_true, y_pred: accuracy_score(\n",
    "                        y_true, np.where(y_pred > 0.5, 1, 0))\n",
    "                self.metrics_names.append(metric)\n",
    "        return metrics_\n",
    "    \n",
    "    def fit(self, x = None, y = None, batch_size=None, epochs=1, verbose=2,  validation_split=0.1, shuffle=True, callbacks=None):\n",
    "        \n",
    "        if validation_split and 0. < validation_split < 1.:\n",
    "            do_validation = True\n",
    "            \n",
    "            split_at = int(x.shape[0] * (1. - validation_split))\n",
    "                \n",
    "            x_, val_x = x[0:split_at], x[split_at: -1]\n",
    "            \n",
    "            y_, val_y = y[0:split_at], y[split_at: -1]\n",
    "            \n",
    "            x = x_\n",
    "            y = y_\n",
    "        else:\n",
    "            do_validation = False\n",
    "            val_x = []\n",
    "            val_y = []\n",
    "        \n",
    "        train_tensor_data = Data.TensorDataset(torch.from_numpy(x), torch.from_numpy(y))\n",
    "        \n",
    "        if batch_size is None:\n",
    "            batch_size = 256\n",
    "            \n",
    "        train_loader = DataLoader(\n",
    "            dataset=train_tensor_data, shuffle=shuffle, batch_size=batch_size)\n",
    "        \n",
    "        print(self.device, end=\"\\n\")\n",
    "        model = self.train()\n",
    "        loss_func = self.loss_func\n",
    "        optim = self.optim\n",
    "        \n",
    "        sample_num = len(train_tensor_data)\n",
    "        steps_per_epoch = (sample_num - 1) // batch_size + 1\n",
    "\n",
    "        callbacks = CallbackList(callbacks)\n",
    "        callbacks.set_model(self)\n",
    "        callbacks.on_train_begin()\n",
    "        self.stop_training = False  # used for early stopping\n",
    "\n",
    "        # Train\n",
    "        print(\"Train on {0} samples, validate on {1} samples, {2} steps per epoch\".format(\n",
    "            len(train_tensor_data), len(val_y), steps_per_epoch))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            callbacks.on_epoch_begin(epoch)\n",
    "            epoch_logs = {}\n",
    "            start_time = time.time()\n",
    "            loss_epoch = 0\n",
    "            total_loss_epoch = 0\n",
    "            train_result = {}\n",
    "            try:\n",
    "                with tqdm(enumerate(train_loader), disable = verbose != 1) as t:\n",
    "                    for index, (x_train, y_train) in t:\n",
    "                        x = x_train.to(self.device).float()\n",
    "                        y = y_train.to(self.device).float()\n",
    "\n",
    "                        y_pred = model(x).squeeze()\n",
    "                        #---------------------------\n",
    "                        #print(y_pred.shape)\n",
    "                        \n",
    "                        optim.zero_grad()\n",
    "                        loss = loss_func(y_pred, y.squeeze(), reduction='sum')\n",
    "                        \n",
    "                        #reg_loss = self.get_regularization_loss()\n",
    "\n",
    "                        #total_loss = loss + reg_loss + self.aux_loss\n",
    "                        total_loss = loss\n",
    "                        \n",
    "                        loss_epoch += loss.item()\n",
    "                        total_loss_epoch += total_loss.item()\n",
    "                        total_loss.backward(retain_graph=True)\n",
    "                        \n",
    "                        optim.step()\n",
    "\n",
    "                        if verbose > 0:\n",
    "                            for name, metric_fun in self.metrics.items():\n",
    "                                if name not in train_result:\n",
    "                                    train_result[name] = []\n",
    "                                train_result[name].append(metric_fun(\n",
    "                                    y.cpu().data.numpy(), y_pred.cpu().data.numpy().astype(\"float64\")))\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                t.close()\n",
    "                raise\n",
    "            t.close()\n",
    "\n",
    "            # Add epoch_logs\n",
    "            epoch_logs[\"loss\"] = total_loss_epoch / sample_num\n",
    "            for name, result in train_result.items():\n",
    "                epoch_logs[name] = np.sum(result) / steps_per_epoch\n",
    "\n",
    "            if do_validation:\n",
    "                eval_result = self.evaluate(val_x, val_y, batch_size)\n",
    "                for name, result in eval_result.items():\n",
    "                    epoch_logs[\"val_\" + name] = result\n",
    "                    \n",
    "            # verbose\n",
    "            if verbose > 0:\n",
    "                epoch_time = int(time.time() - start_time)\n",
    "                print('Epoch {0}/{1}'.format(epoch + 1, epochs))\n",
    "\n",
    "                eval_str = \"{0}s - loss: {1: .4f}\".format(\n",
    "                    epoch_time, epoch_logs[\"loss\"])\n",
    "\n",
    "                for name in self.metrics:\n",
    "                    eval_str += \" - \" + name + \\\n",
    "                                \": {0: .4f}\".format(epoch_logs[name])\n",
    "\n",
    "                if do_validation:\n",
    "                    for name in self.metrics:\n",
    "                        eval_str += \" - \" + \"val_\" + name + \\\n",
    "                                    \": {0: .4f}\".format(epoch_logs[\"val_\" + name])\n",
    "                print(eval_str)\n",
    "            callbacks.on_epoch_end(epoch, epoch_logs)\n",
    "            if self.stop_training:\n",
    "                break\n",
    "\n",
    "        callbacks.on_train_end()\n",
    "        \n",
    "    def evaluate(self, x, y, batch_size=256):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: Numpy array of test data (if the model has a single input), or list of Numpy arrays (if the model has multiple inputs).\n",
    "        :param y: Numpy array of target (label) data (if the model has a single output), or list of Numpy arrays (if the model has multiple outputs).\n",
    "        :param batch_size: Integer or `None`. Number of samples per evaluation step. If unspecified, `batch_size` will default to 256.\n",
    "        :return: Dict contains metric names and metric values.\n",
    "        \"\"\"\n",
    "        pred_ans = self.predict(x, batch_size)\n",
    "        eval_result = {}\n",
    "        for name, metric_fun in self.metrics.items():\n",
    "            eval_result[name] = metric_fun(y, pred_ans)\n",
    "        return eval_result\n",
    "\n",
    "    def predict(self, x, batch_size=256):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs).\n",
    "        :param batch_size: Integer. If unspecified, it will default to 256.\n",
    "        :return: Numpy array(s) of predictions.\n",
    "        \"\"\"\n",
    "        model = self.eval()\n",
    "        tensor_data = Data.TensorDataset(torch.tensor(x))\n",
    "        test_loader = DataLoader(\n",
    "            dataset=tensor_data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "        pred_ans = []\n",
    "        with torch.no_grad():\n",
    "            for index, x_test in enumerate(test_loader):\n",
    "                x = x_test[0].to(self.device).float()\n",
    "\n",
    "                y_pred = model(x).cpu().data.numpy()  # .squeeze()\n",
    "                pred_ans.append(y_pred)\n",
    "\n",
    "        return np.concatenate(pred_ans).astype(\"float64\")\n",
    "    \n",
    "    def add_regularization_weight(self, weight_list, weight_decay, p=2):\n",
    "        self.regularization_weight.append((list(weight_list), weight_decay, p))\n",
    "\n",
    "    def get_regularization_loss(self, ):\n",
    "        total_reg_loss = torch.zeros((1,), device=self.device)\n",
    "        for weight_list, weight_decay, p in self.regularization_weight:\n",
    "            weight_reg_loss = torch.zeros((1,), device=self.device)\n",
    "            for w in weight_list:\n",
    "                if isinstance(w, tuple):\n",
    "                    l2_reg = torch.norm(w[1], p=p, )\n",
    "                else:\n",
    "                    l2_reg = torch.norm(w, p=p, )\n",
    "                weight_reg_loss = weight_reg_loss + l2_reg\n",
    "            reg_loss = weight_decay * weight_reg_loss\n",
    "            total_reg_loss += reg_loss\n",
    "        return total_reg_loss\n",
    "\n",
    "    \n",
    "def activation_layer(act_name, hidden_size=None, dice_dim=2):\n",
    "    \"\"\"Construct activation layers\n",
    "\n",
    "    Args:\n",
    "        act_name: str or nn.Module, name of activation function\n",
    "        hidden_size: int, used for Dice activation\n",
    "        dice_dim: int, used for Dice activation\n",
    "    Return:\n",
    "        act_layer: activation layer\n",
    "    \"\"\"\n",
    "    if isinstance(act_name, str):\n",
    "        if act_name.lower() == 'sigmoid':\n",
    "            act_layer = nn.Sigmoid()\n",
    "        elif act_name.lower() == 'linear':\n",
    "            act_layer = Identity()\n",
    "        elif act_name.lower() == 'relu':\n",
    "            act_layer = nn.ReLU(inplace=True)\n",
    "        elif act_name.lower() == 'dice':\n",
    "            assert dice_dim\n",
    "            act_layer = Dice(hidden_size, dice_dim)\n",
    "        elif act_name.lower() == 'prelu':\n",
    "            act_layer = nn.PReLU()\n",
    "    elif issubclass(act_name, nn.Module):\n",
    "        act_layer = act_name()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return act_layer\n",
    "\n",
    "    \n",
    "    \n",
    "class PredictionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "      Arguments\n",
    "         - **task**: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n",
    "         - **use_bias**: bool.Whether add bias term or not.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, task='binary', use_bias=True, **kwargs):\n",
    "        if task not in [\"binary\", \"multiclass\", \"regression\"]:\n",
    "            raise ValueError(\"task must be binary,multiclass or regression\")\n",
    "\n",
    "        super(PredictionLayer, self).__init__()\n",
    "        self.use_bias = use_bias\n",
    "        self.task = task\n",
    "        if self.use_bias:\n",
    "            self.bias = nn.Parameter(torch.zeros((1,)))\n",
    "        self.to('cuda:0')\n",
    "    def forward(self, X):\n",
    "        output = X\n",
    "        if self.use_bias:\n",
    "            output += self.bias\n",
    "        if self.task == \"binary\":\n",
    "            output = torch.sigmoid(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "    def __init__(self, maxlen = 32, vocab_size = 50000, embed_dim = 32, device = 'cpu'):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = nn.Embedding(num_embeddings  = vocab_size, embedding_dim  = embed_dim)   \n",
    "        nn.init.normal_(self.token_emb.weight, mean=0, std=0.0001)\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    # 这里默认输入的是tensor\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.token_emb(x)\n",
    "    \n",
    "    \n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, device = 'cpu'):\n",
    "        super(Linear, self).__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(vocab_size  = 50000, embed_dim  = 32)    \n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        linear_sparse_logit = torch.sum(\n",
    "                torch.flatten(x,start_dim=1), dim=-1, keepdim=True)\n",
    "        \n",
    "        return linear_sparse_logit\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    \"\"\"The Multi Layer Percetron\n",
    "\n",
    "      Input shape\n",
    "        - nD tensor with shape: ``(batch_size, ..., input_dim)``. The most common situation would be a 2D input with shape ``(batch_size, input_dim)``.\n",
    "\n",
    "      Output shape\n",
    "        - nD tensor with shape: ``(batch_size, ..., hidden_size[-1])``. For instance, for a 2D input with shape ``(batch_size, input_dim)``, the output would have shape ``(batch_size, hidden_size[-1])``.\n",
    "\n",
    "      Arguments\n",
    "        - **inputs_dim**: input feature dimension.\n",
    "\n",
    "        - **hidden_units**:list of positive integer, the layer number and units in each layer.\n",
    "\n",
    "        - **activation**: Activation function to use.\n",
    "\n",
    "        - **l2_reg**: float between 0 and 1. L2 regularizer strength applied to the kernel weights matrix.\n",
    "\n",
    "        - **dropout_rate**: float in [0,1). Fraction of the units to dropout.\n",
    "\n",
    "        - **use_bn**: bool. Whether use BatchNormalization before activation or not.\n",
    "\n",
    "        - **seed**: A Python integer to use as random seed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputs_dim, hidden_units, activation='relu', l2_reg=0, dropout_rate=0.5, use_bn=True,\n",
    "                 init_std=0.0001, dice_dim=3, seed=1024, device='cpu'):\n",
    "        super(DNN, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.seed = seed\n",
    "        self.l2_reg = l2_reg\n",
    "        self.use_bn = use_bn\n",
    "        if len(hidden_units) == 0:\n",
    "            raise ValueError(\"hidden_units is empty!!\")\n",
    "        hidden_units = [inputs_dim] + list(hidden_units)\n",
    "\n",
    "        self.linears = nn.ModuleList(\n",
    "            [nn.Linear(hidden_units[i], hidden_units[i + 1]) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        if self.use_bn:\n",
    "            self.bn = nn.ModuleList(\n",
    "                [nn.BatchNorm1d(hidden_units[i + 1]) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        self.activation_layers = nn.ModuleList(\n",
    "            [activation_layer(activation, hidden_units[i + 1], dice_dim) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        for name, tensor in self.linears.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(tensor, mean=0, std=init_std)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        deep_input = inputs\n",
    "\n",
    "        for i in range(len(self.linears)):\n",
    "\n",
    "            fc = self.linears[i](deep_input)\n",
    "\n",
    "            if self.use_bn:\n",
    "                fc = self.bn[i](fc)\n",
    "\n",
    "            fc = self.activation_layers[i](fc)\n",
    "\n",
    "            fc = self.dropout(fc)\n",
    "            deep_input = fc\n",
    "        return deep_input\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class FM(nn.Module):\n",
    "    \"\"\"Factorization Machine models pairwise (order-2) feature interactions\n",
    "     without linear term and bias.\n",
    "      Input shape\n",
    "        - 3D tensor with shape: ``(batch_size,field_size,embedding_size)``.\n",
    "      Output shape\n",
    "        - 2D tensor with shape: ``(batch_size, 1)``.\n",
    "      References\n",
    "        - [Factorization Machines](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FM, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        fm_input = inputs\n",
    "\n",
    "        square_of_sum = torch.pow(torch.sum(fm_input, dim=1, keepdim=True), 2)\n",
    "        sum_of_square = torch.sum(fm_input * fm_input, dim=1, keepdim=True)\n",
    "        cross_term = square_of_sum - sum_of_square\n",
    "        cross_term = 0.5 * torch.sum(cross_term, dim=2, keepdim=False)\n",
    "\n",
    "        return cross_term\n",
    "\n",
    "\n",
    "class CIN(nn.Module):\n",
    "    \"\"\"Compressed Interaction Network used in xDeepFM.\n",
    "      Input shape\n",
    "        - 3D tensor with shape: ``(batch_size,field_size,embedding_size)``.\n",
    "      Output shape\n",
    "        - 2D tensor with shape: ``(batch_size, featuremap_num)`` ``featuremap_num =  sum(self.layer_size[:-1]) // 2 + self.layer_size[-1]`` if ``split_half=True``,else  ``sum(layer_size)`` .\n",
    "      Arguments\n",
    "        - **filed_size** : Positive integer, number of feature groups.\n",
    "        - **layer_size** : list of int.Feature maps in each layer.\n",
    "        - **activation** : activation function name used on feature maps.\n",
    "        - **split_half** : bool.if set to False, half of the feature maps in each hidden will connect to output unit.\n",
    "        - **seed** : A Python integer to use as random seed.\n",
    "      References\n",
    "        - [Lian J, Zhou X, Zhang F, et al. xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems[J]. arXiv preprint arXiv:1803.05170, 2018.] (https://arxiv.org/pdf/1803.05170.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field_size, layer_size=(128, 128), activation='relu', split_half=True, l2_reg=1e-5, seed=1024,\n",
    "                 device='cpu'):\n",
    "        super(CIN, self).__init__()\n",
    "        if len(layer_size) == 0:\n",
    "            raise ValueError(\n",
    "                \"layer_size must be a list(tuple) of length greater than 1\")\n",
    "\n",
    "        self.layer_size = layer_size\n",
    "        self.field_nums = [field_size]\n",
    "        self.split_half = split_half\n",
    "        self.activation = activation_layer(activation)\n",
    "        self.l2_reg = l2_reg\n",
    "        self.seed = seed\n",
    "\n",
    "        self.conv1ds = nn.ModuleList()\n",
    "        for i, size in enumerate(self.layer_size):\n",
    "            self.conv1ds.append(\n",
    "                nn.Conv1d(self.field_nums[-1] * self.field_nums[0], size, 1))\n",
    "\n",
    "            if self.split_half:\n",
    "                if i != len(self.layer_size) - 1 and size % 2 > 0:\n",
    "                    raise ValueError(\n",
    "                        \"layer_size must be even number except for the last layer when split_half=True\")\n",
    "\n",
    "                self.field_nums.append(size // 2)\n",
    "            else:\n",
    "                self.field_nums.append(size)\n",
    "\n",
    "        #         for tensor in self.conv1ds:\n",
    "        #             nn.init.normal_(tensor.weight, mean=0, std=init_std)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if len(inputs.shape) != 3:\n",
    "            raise ValueError(\n",
    "                \"Unexpected inputs dimensions %d, expect to be 3 dimensions\" % (len(inputs.shape)))\n",
    "        batch_size = inputs.shape[0]\n",
    "        dim = inputs.shape[-1]\n",
    "        hidden_nn_layers = [inputs]\n",
    "        final_result = []\n",
    "\n",
    "        for i, size in enumerate(self.layer_size):\n",
    "            # x^(k-1) * x^0\n",
    "            x = torch.einsum(\n",
    "                'bhd,bmd->bhmd', hidden_nn_layers[-1], hidden_nn_layers[0])\n",
    "            # x.shape = (batch_size , hi * m, dim)\n",
    "            x = x.reshape(\n",
    "                batch_size, hidden_nn_layers[-1].shape[1] * hidden_nn_layers[0].shape[1], dim)\n",
    "            # x.shape = (batch_size , hi, dim)\n",
    "            x = self.conv1ds[i](x)\n",
    "\n",
    "            if self.activation is None or self.activation == 'linear':\n",
    "                curr_out = x\n",
    "            else:\n",
    "                curr_out = self.activation(x)\n",
    "\n",
    "            if self.split_half:\n",
    "                if i != len(self.layer_size) - 1:\n",
    "                    next_hidden, direct_connect = torch.split(\n",
    "                        curr_out, 2 * [size // 2], 1)\n",
    "                else:\n",
    "                    direct_connect = curr_out\n",
    "                    next_hidden = 0\n",
    "            else:\n",
    "                direct_connect = curr_out\n",
    "                next_hidden = curr_out\n",
    "\n",
    "            final_result.append(direct_connect)\n",
    "            hidden_nn_layers.append(next_hidden)\n",
    "\n",
    "        result = torch.cat(final_result, dim=1)\n",
    "        result = torch.sum(result, -1)\n",
    "\n",
    "        return result\n",
    "\n",
    "    \n",
    "class InteractingLayer(nn.Module):\n",
    "    \"\"\"A Layer used in AutoInt that model the correlations between different feature fields by multi-head self-attention mechanism.\n",
    "      Input shape\n",
    "            - A 3D tensor with shape: ``(batch_size,field_size,embedding_size)``.\n",
    "      Output shape\n",
    "            - 3D tensor with shape:``(batch_size,field_size,att_embedding_size * head_num)``.\n",
    "      Arguments\n",
    "            - **in_features** : Positive integer, dimensionality of input features.\n",
    "            - **att_embedding_size**: int.The embedding size in multi-head self-attention network.\n",
    "            - **head_num**: int.The head number in multi-head  self-attention network.\n",
    "            - **use_res**: bool.Whether or not use standard residual connections before output.\n",
    "            - **seed**: A Python integer to use as random seed.\n",
    "      References\n",
    "            - [Song W, Shi C, Xiao Z, et al. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks[J]. arXiv preprint arXiv:1810.11921, 2018.](https://arxiv.org/abs/1810.11921)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, att_embedding_size=8, head_num=2, use_res=True, seed=1024, device='cpu'):\n",
    "        super(InteractingLayer, self).__init__()\n",
    "        if head_num <= 0:\n",
    "            raise ValueError('head_num must be a int > 0')\n",
    "        self.att_embedding_size = att_embedding_size\n",
    "        self.head_num = head_num\n",
    "        self.use_res = use_res\n",
    "        self.seed = seed\n",
    "\n",
    "        embedding_size = in_features\n",
    "\n",
    "        self.W_Query = nn.Parameter(torch.Tensor(\n",
    "            embedding_size, self.att_embedding_size * self.head_num))\n",
    "\n",
    "        self.W_key = nn.Parameter(torch.Tensor(\n",
    "            embedding_size, self.att_embedding_size * self.head_num))\n",
    "\n",
    "        self.W_Value = nn.Parameter(torch.Tensor(\n",
    "            embedding_size, self.att_embedding_size * self.head_num))\n",
    "\n",
    "        if self.use_res:\n",
    "            self.W_Res = nn.Parameter(torch.Tensor(\n",
    "                embedding_size, self.att_embedding_size * self.head_num))\n",
    "        for tensor in self.parameters():\n",
    "            nn.init.normal_(tensor, mean=0.0, std=0.05)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        if len(inputs.shape) != 3:\n",
    "            raise ValueError(\n",
    "                \"Unexpected inputs dimensions %d, expect to be 3 dimensions\" % (len(inputs.shape)))\n",
    "\n",
    "        querys = torch.tensordot(inputs, self.W_Query,\n",
    "                                 dims=([-1], [0]))  # None F D*head_num\n",
    "        keys = torch.tensordot(inputs, self.W_key, dims=([-1], [0]))\n",
    "        values = torch.tensordot(inputs, self.W_Value, dims=([-1], [0]))\n",
    "\n",
    "        # head_num None F D\n",
    "\n",
    "        querys = torch.stack(torch.split(\n",
    "            querys, self.att_embedding_size, dim=2))\n",
    "        keys = torch.stack(torch.split(keys, self.att_embedding_size, dim=2))\n",
    "        values = torch.stack(torch.split(\n",
    "            values, self.att_embedding_size, dim=2))\n",
    "        inner_product = torch.einsum(\n",
    "            'bnik,bnjk->bnij', querys, keys)  # head_num None F F\n",
    "\n",
    "        self.normalized_att_scores = F.softmax(\n",
    "            inner_product, dim=-1)  # head_num None F F\n",
    "        result = torch.matmul(self.normalized_att_scores,\n",
    "                              values)  # head_num None F D\n",
    "\n",
    "        result = torch.cat(torch.split(result, 1, ), dim=-1)\n",
    "        result = torch.squeeze(result, dim=0)  # None F D*head_num\n",
    "        if self.use_res:\n",
    "            result += torch.tensordot(inputs, self.W_Res, dims=([-1], [0]))\n",
    "        result = F.relu(result)\n",
    "\n",
    "        return result\n",
    "\n",
    "    \n",
    "class Model(BaseModel):\n",
    "    def __init__(self, field_size,device = 'cuda:0'):\n",
    "        super(Model, self).__init__(device = device)\n",
    "        \n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen = 32, vocab_size = 50000, embed_dim = 32, device = device)\n",
    "    \n",
    "        \n",
    "        self.fm_linear = Linear()\n",
    "        \n",
    "        # ----- attention\n",
    "        att_embedding_size = 8\n",
    "        att_head_num = 4\n",
    "        att_layer_num = 10\n",
    "        att_res = True\n",
    "        self.atten_layers = nn.ModuleList([InteractingLayer(32 if i == 0 else att_embedding_size * att_head_num,\n",
    "                              att_embedding_size, att_head_num, att_res, device=device) for i in range(att_layer_num)])\n",
    "        self.atten_linear = nn.Linear(field_size * att_embedding_size * att_head_num, 1, bias = False).to(device)\n",
    "        \n",
    "        \n",
    "        # -----fm \n",
    "        self.fm = FM()\n",
    "        \n",
    "        # -----dnn\n",
    "        dnn_hidden_units = [256, 256,256, 256,256, 256]\n",
    "        l2_reg = 0.0001\n",
    "        self.dnn = DNN(field_size * 32, dnn_hidden_units,\n",
    "                           activation='relu', l2_reg=0.0001, dropout_rate=0.1, use_bn=True,\n",
    "                           init_std=0.0001, device=device)\n",
    "        self.dnn_linear = nn.Linear(dnn_hidden_units[-1], 1, bias=False).to(device)\n",
    "        self.add_regularization_weight(\n",
    "                filter(lambda x: 'weight' in x[0] and 'bn' not in x[0], self.dnn.named_parameters()), l2_reg)\n",
    "        self.add_regularization_weight(self.dnn_linear.weight, l2_reg)\n",
    "        \n",
    "        # ----cin\n",
    "        cin_layer_size=(256, 128)\n",
    "        cin_split_half=True\n",
    "        cin_activation='relu'\n",
    "        l2_reg_linear=0.00001,\n",
    "        l2_reg_embedding=0.00001\n",
    "        l2_reg_dnn=0.00001\n",
    "        l2_reg_cin=0.00001\n",
    "        init_std=0.0001\n",
    "        seed=1024\n",
    "        dnn_dropout=0.1\n",
    "        \n",
    "        \n",
    "        field_num = field_size\n",
    "        if cin_split_half == True:\n",
    "            self.featuremap_num = sum(\n",
    "                cin_layer_size[:-1]) // 2 + cin_layer_size[-1]\n",
    "        else:\n",
    "            self.featuremap_num = sum(cin_layer_size)\n",
    "        self.cin = CIN(field_num, cin_layer_size,\n",
    "                       cin_activation, cin_split_half, l2_reg_cin, seed, device=device)\n",
    "        self.cin_linear = nn.Linear(self.featuremap_num, 1, bias=False).to(device)\n",
    "        self.add_regularization_weight(\n",
    "            filter(lambda x: 'weight' in x[0], self.cin.named_parameters()), l2_reg_cin)\n",
    "        \n",
    "        self.out = PredictionLayer()\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        logit = self.fm_linear(x)\n",
    "        \n",
    "        fm_input = self.embedding_layer(x.long())\n",
    "        #print(fm_input.shape)\n",
    "        \n",
    "        #--- attention\n",
    "        \n",
    "        att_input = fm_input\n",
    "        for layer in self.atten_layers:\n",
    "            att_input = layer(att_input)\n",
    "            #print(att_input.shape)\n",
    "        logit += self.atten_linear(torch.flatten(att_input, start_dim = 1))\n",
    "\n",
    "        \n",
    "        #---cin\n",
    "        '''\n",
    "        cin_output = self.cin(fm_input)\n",
    "        cin_logit = self.cin_linear(cin_output)\n",
    "        logit += cin_logit\n",
    "        '''\n",
    "        #----fm\n",
    "        #logit = self.fm(fm_input)\n",
    "        \n",
    "        #----dnn\n",
    "        dnn_output = self.dnn(torch.flatten(fm_input, start_dim = 1))\n",
    "        dnn_logit = self.dnn_linear(dnn_output)\n",
    "        logit += dnn_logit\n",
    "\n",
    "        y_pred = self.out(logit)\n",
    "        \n",
    "        return y_pred\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "#maxlen = 32 \n",
    "#X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "#X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print(X_train.shape)\n",
    "\n",
    "# Embedding size for each token\n",
    "embed_dim = 32 \n",
    "\n",
    "# Number of attention heads\n",
    "num_heads = 8  \n",
    "\n",
    "# Hidden layer size in feed forward network inside transformer\n",
    "ff_dim = 64  \n",
    "\n",
    "vocab_size = 50000\n",
    "\n",
    "model = Model(field_size = len(features_map))\n",
    "model.compile('adam', 'binary_crossentropy',metrics=[\"binary_crossentropy\", \"auc\"],)\n",
    "model.fit(X_train, y.values, batch_size=2**10, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 179.584828,
     "end_time": "2020-10-30T18:59:48.530782",
     "exception": false,
     "start_time": "2020-10-30T18:56:48.945954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc  ###计算roc和auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def acu_curve(y,prob):\n",
    "    fpr,tpr,threshold = roc_curve(y,prob) ###计算真正率和假正率\n",
    "    roc_auc = auc(fpr,tpr) ###计算auc的值\n",
    " \n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.3f)' % roc_auc) ###假正率为横坐标，真正率为纵坐标做曲线\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    " \n",
    "    plt.show()\n",
    "    \n",
    "y_pred = model.predict(X_test)\n",
    "y_true = np.array(y_val)\n",
    "#roc_auc_score(y_true, y_pred)\n",
    "acu_curve(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.856903,
     "end_time": "2020-10-30T18:59:50.239874",
     "exception": false,
     "start_time": "2020-10-30T18:59:49.382971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Making Predictions for New Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.149547,
     "end_time": "2020-10-30T18:59:55.393297",
     "exception": false,
     "start_time": "2020-10-30T18:59:53.243750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import riiideducation\n",
    "import pandas as pd\n",
    "env = riiideducation.make_env()\n",
    "iter_test = env.iter_test()\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    test_df['task_container_id'] = test_df.task_container_id.mask(test_df.task_container_id > 9999, 9999)\n",
    "    test_df = pd.merge(test_df, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\n",
    "    test_df = pd.merge(test_df, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\n",
    "    test_df = pd.merge(test_df, results_u_final, on=['user_id'],  how=\"left\")\n",
    "    test_df = pd.merge(test_df, results_u2_final, on=['user_id'],  how=\"left\")\n",
    "    test_df = pd.merge(test_df, results_c_final, on=['contend_id'],  how=\"left\")\n",
    "    test_df = pd.merge(test_df, user_lecture_stats_part, on=['user_id'], how=\"left\")\n",
    "    \n",
    "    test_df['answered_correctly_content_mean'].fillna(0.5,  inplace=True)\n",
    "    test_df['sum_correct'].fillna(0, inplace = True)\n",
    "    test_df['count_sum'].fillna(0, inplace = True)\n",
    "    \n",
    "    test_df['part_1'].fillna(0, inplace = True)\n",
    "    test_df['part_2'].fillna(0, inplace = True)\n",
    "    test_df['part_3'].fillna(0, inplace = True)\n",
    "    test_df['part_4'].fillna(0, inplace = True)\n",
    "    test_df['part_5'].fillna(0, inplace = True)\n",
    "    test_df['part_6'].fillna(0, inplace = True)\n",
    "    test_df['part_7'].fillna(0, inplace = True)\n",
    "    test_df['type_of_concept'].fillna(0, inplace = True)\n",
    "    test_df['type_of_intention'].fillna(0, inplace = True)\n",
    "    test_df['type_of_solving_question'].fillna(0, inplace = True)\n",
    "    test_df['type_of_starter'].fillna(0, inplace = True)\n",
    "    test_df['part_1_boolean'].fillna(0, inplace = True)\n",
    "    test_df['part_2_boolean'].fillna(0, inplace = True)\n",
    "    test_df['part_3_boolean'].fillna(0, inplace = True)\n",
    "    test_df['part_4_boolean'].fillna(0, inplace = True)\n",
    "    test_df['part_5_boolean'].fillna(0, inplace = True)\n",
    "    test_df['part_6_boolean'].fillna(0, inplace = True)\n",
    "    test_df['part_7_boolean'].fillna(0, inplace = True)\n",
    "    test_df['type_of_concept_boolean'].fillna(0, inplace = True)\n",
    "    test_df['type_of_intention_boolean'].fillna(0, inplace = True)\n",
    "    test_df['type_of_solving_question_boolean'].fillna(0, inplace = True)\n",
    "    test_df['type_of_starter_boolean'].fillna(0, inplace = True)\n",
    "    \n",
    "    test_df['answered_correctly_user'].fillna(0.65,  inplace=True)\n",
    "    test_df['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\n",
    "    test_df['quest_pct'].fillna(content_mean,  inplace=True)\n",
    "    test_df['part'] = test_df.part - 1\n",
    "\n",
    "    test_df['part'].fillna(4, inplace = True)\n",
    "    test_df['avg_questions_seen'].fillna(1, inplace = True)\n",
    "    test_df['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\n",
    "    test_df['prior_question_had_explanation'].fillna(False, inplace=True)\n",
    "    test_df[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(test_df[\"prior_question_had_explanation\"])\n",
    "    X = test_df[features_map]\n",
    "    X=scaler.transform(X)\n",
    "    X = X.reshape(X.shape[0], X.shape[1])\n",
    "    #X = keras.preprocessing.sequence.pad_sequences(X, maxlen=32)\n",
    "    test_df['answered_correctly'] =  model.predict(X)\n",
    "\n",
    "    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-15",
   "language": "python",
   "name": "torch-15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "duration": 2235.200117,
   "end_time": "2020-10-30T18:59:58.325254",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-30T18:22:43.125137",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
